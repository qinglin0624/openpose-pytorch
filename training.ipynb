{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015659,
     "end_time": "2021-03-07T17:14:27.944829",
     "exception": false,
     "start_time": "2021-03-07T17:14:27.929170",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train\n",
    "- unfreeze vgg after 2000 iterations\n",
    "- decrease lr by half if it does not give a new lowest loss in 2000 iterations\n",
    "- In this notebook, the model is trained for 28200 iterations from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.018572,
     "end_time": "2021-03-07T17:14:29.977982",
     "exception": false,
     "start_time": "2021-03-07T17:14:27.959410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "from torchvision.models import vgg19\n",
    "import cv2\n",
    "import torch.nn.functional as Func\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.022717,
     "end_time": "2021-03-07T17:14:30.015868",
     "exception": false,
     "start_time": "2021-03-07T17:14:29.993151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_heat_ps(size=15, sigma2=25):\n",
    "    m = np.empty((size,size))\n",
    "    c = size//2\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            m[i,j] = (i-c)**2 + (j-c)**2\n",
    "    m = np.exp(-m/sigma2)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015192,
     "end_time": "2021-03-07T17:14:30.046158",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.030966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.047706,
     "end_time": "2021-03-07T17:14:30.108859",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.061153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx_to_keypoint_type = {0: 'nose', 1: 'left_eye', 2: 'right_eye', 3: 'left_ear', 4: 'right_ear', 5: 'left_shoulder', 6: 'right_shoulder', 7: 'left_elbow', 8: 'right_elbow', 9: 'left_wrist', 10: 'right_wrist', 11: 'left_hip', 12: 'right_hip', 13: 'left_knee', 14: 'right_knee', 15: 'left_ankle', 16: 'right_ankle'}\n",
    "\n",
    "keypoint_type_to_idx = {'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4, 'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8, 'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12, 'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16}\n",
    "\n",
    "part_pairs = [['left_ankle', 'left_knee'], ['left_knee', 'left_hip'], ['right_ankle', 'right_knee'], ['right_knee', 'right_hip'], ['left_hip', 'right_hip'], ['left_shoulder', 'left_hip'], ['right_shoulder', 'right_hip'], ['left_shoulder', 'right_shoulder'], ['left_shoulder', 'left_elbow'], ['right_shoulder', 'right_elbow'], ['left_elbow', 'left_wrist'], ['right_elbow', 'right_wrist'], ['left_eye', 'right_eye'], ['nose', 'left_eye'], ['nose', 'right_eye'], ['left_eye', 'left_ear'], ['right_eye', 'right_ear'], ['left_ear', 'left_shoulder'], ['right_ear', 'right_shoulder'], ['left_shoulder', 'left_wrist'], ['right_shoulder', 'right_wrist'], ['left_hip', 'left_ankle'], ['right_hip', 'right_ankle']]\n",
    "\n",
    "keypoint_labels = ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder', 'left_elbow', 'right_elbow', 'left_wrist', 'right_wrist', 'left_hip', 'right_hip', 'left_knee', 'right_knee', 'left_ankle', 'right_ankle']\n",
    "\n",
    "KEYPOINT_ORDER = np.arange(0,17)\n",
    "\n",
    "SMALLER_HEATMAP_GROUP = np.arange(0,5)#['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear']\n",
    "\n",
    "SKELETON = np.array([[15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7], [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5,9], [6,10], [11,15], [12,16]])\n",
    "INFERENCE_SKELETON = np.array([[15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7], [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4], [3, 5], [4, 6]])\n",
    "\n",
    "GAUSSIAN_15X15 = np.load('util_data/gaussian_15X15_sigma_7.npy')\n",
    "# GAUSSIAN_15X15 = get_heat_ps(size=15, sigma2=49)\n",
    "GAUSSIAN_9X9 = np.load('util_data/gaussian_9X9_sigma_3.npy')\n",
    "# GAUSSIAN_9X9 = get_heat_ps(size=9, sigma2=9)\n",
    "GAUSSIAN_5X5 = np.load('util_data/gaussian_5X5_sigma_3.npy')\n",
    "# GAUSSIAN_5X5 = get_heat_ps(size=5, sigma2=9)\n",
    "\n",
    "heatmap_ps_map = {'ps_vals':GAUSSIAN_15X15,'ps':15, 'ps_small_vals':GAUSSIAN_9X9, 'ps_small':9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015965,
     "end_time": "2021-03-07T17:14:30.142162",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.126197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.136374,
     "end_time": "2021-03-07T17:14:30.293786",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.157412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Denorm(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        return tensor.mul(self.std).add(self.mean)\n",
    "    \n",
    "class RandomCrop(object):\n",
    "    def __init__(self, size=368, p=1):\n",
    "        self.sz = size\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        keypoints = sample['keypoints']\n",
    "        H, W = image.height, image.width\n",
    "        \n",
    "        if (W>self.sz and H>self.sz and np.random.random()>(1-self.p)):\n",
    "            x = np.random.randint(W - self.sz)\n",
    "            y = np.random.randint(H - self.sz)\n",
    "            croped_img = image.crop(box=(x,y, x+self.sz, y+self.sz))\n",
    "            \n",
    "            keypoints[keypoints[:,:,0]<x] = np.array([0,0,0])\n",
    "            keypoints[keypoints[:,:,1]<y] = np.array([0,0,0])\n",
    "            keypoints[keypoints[:,:,0]>x+self.sz] = np.array([0,0,0])\n",
    "            keypoints[keypoints[:,:,1]>y+self.sz] = np.array([0,0,0])\n",
    "            keypoints[:,:,:2][keypoints[:,:,2]>0] = keypoints[:,:,:2][keypoints[:,:,2]>0] - np.array([[x, y]])\n",
    "            \n",
    "            return { 'image' : croped_img, 'keypoints':keypoints }\n",
    "        else: return sample\n",
    "\n",
    "class ResizeImgAndKeypoints(object):\n",
    "    def __init__(self, size=368):\n",
    "        self.size = size\n",
    "        self.paf_sz = int(size*46/368)\n",
    "        self.Resize = transforms.Resize((self.paf_sz, self.paf_sz))\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        keypoints = sample['keypoints'].copy().astype(float) #2x17x3\n",
    "        IM_H, IM_W = image.height, image.width\n",
    "        if(IM_H > IM_W):\n",
    "            w = int(self.size*IM_W/IM_H)\n",
    "            h = self.size\n",
    "            pad_val = int((self.size-w)/2)\n",
    "            pad = (self.size-w-pad_val,0,pad_val ,0)\n",
    "            keypoints[:,:,0] = keypoints[:,:,0]*(w/IM_W)\n",
    "            keypoints[:,:,0][keypoints[:,:,2]>0] += self.size-w-pad_val\n",
    "            keypoints[:,:,1] = keypoints[:,:,1]*(self.size/IM_H)\n",
    "        \n",
    "        else:\n",
    "            h = int(self.size*IM_H/IM_W)\n",
    "            w = self.size\n",
    "            pad_val = int((self.size-h)/2)\n",
    "            pad = (0,self.size-h-pad_val,0,pad_val)\n",
    "            keypoints[:,:,0] = keypoints[:,:,0]*(self.size/IM_W)\n",
    "            keypoints[:,:,1] = keypoints[:,:,1]*(h/IM_H)\n",
    "            keypoints[:,:,1][keypoints[:,:,2]>0] += self.size-h-pad_val\n",
    "        \n",
    "        resized_img = ImageOps.expand(image.resize((w,h),resample=Image.BILINEAR), pad)\n",
    "        return { 'image' : resized_img , 'image_stg_input': self.Resize(resized_img),'keypoints' : keypoints }\n",
    "\n",
    "class FlipHR(object):\n",
    "    def __init__(self, p=0.25):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        keypoints = sample['keypoints']\n",
    "        \n",
    "        if np.random.random() > (1-self.p):\n",
    "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            w, h = image.size\n",
    "            keypoints[:, :, 0][keypoints[:, :, 2]>0] = w - keypoints[:, :, 0][keypoints[:, :, 2]>0]\n",
    "            copy = keypoints.copy()\n",
    "            keypoints[:,1,:], keypoints[:,2,:] = copy[:,2,:], copy[:,1,:]\n",
    "            keypoints[:,3,:], keypoints[:,4,:] = copy[:,4,:], copy[:,3,:]\n",
    "            keypoints[:,5,:], keypoints[:,6,:] = copy[:,6,:], copy[:,5,:]\n",
    "            keypoints[:,7,:], keypoints[:,8,:] = copy[:,8,:], copy[:,7,:]\n",
    "            keypoints[:,9,:], keypoints[:,10,:] = copy[:,10,:], copy[:,9,:]\n",
    "            keypoints[:,11,:], keypoints[:,12,:] = copy[:,12,:], copy[:,11,:]\n",
    "            keypoints[:,13,:], keypoints[:,14,:] = copy[:,14,:], copy[:,13,:]\n",
    "            keypoints[:,15,:], keypoints[:,16,:] = copy[:,16,:], copy[:,15,:]\n",
    "        \n",
    "            return { 'image' : image, 'image_stg_input': ImageOps.mirror(sample['image_stg_input']) ,'keypoints' : keypoints }\n",
    "        else: return sample\n",
    "\n",
    "class FlipUD(object):\n",
    "    def __init__(self, p=0.2):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        keypoints = sample['keypoints']\n",
    "        \n",
    "        if np.random.random() > (1-self.p):\n",
    "            image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            w, h = image.size\n",
    "            keypoints[:, :, 1][keypoints[:, :, 2]>0] = h - keypoints[:, :, 1][keypoints[:, :, 2]>0]\n",
    "            copy = keypoints.copy()\n",
    "            keypoints[:,1,:], keypoints[:,2,:] = copy[:,2,:], copy[:,1,:]\n",
    "            keypoints[:,3,:], keypoints[:,4,:] = copy[:,4,:], copy[:,3,:]\n",
    "            keypoints[:,5,:], keypoints[:,6,:] = copy[:,6,:], copy[:,5,:]\n",
    "            keypoints[:,7,:], keypoints[:,8,:] = copy[:,8,:], copy[:,7,:]\n",
    "            keypoints[:,9,:], keypoints[:,10,:] = copy[:,10,:], copy[:,9,:]\n",
    "            keypoints[:,11,:], keypoints[:,12,:] = copy[:,12,:], copy[:,11,:]\n",
    "            keypoints[:,13,:], keypoints[:,14,:] = copy[:,14,:], copy[:,13,:]\n",
    "            keypoints[:,15,:], keypoints[:,16,:] = copy[:,16,:], copy[:,15,:]\n",
    "        \n",
    "            return { 'image' : image, 'image_stg_input': ImageOps.flip(sample['image_stg_input']) ,'keypoints' : keypoints }\n",
    "        else: return sample\n",
    "        \n",
    "class ColorJitter(object):\n",
    "    def __init__(self, brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1):\n",
    "        self.tfm = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = self.tfm(sample['image'])\n",
    "        return { 'image' : image, 'image_stg_input': sample['image_stg_input'], 'keypoints': sample['keypoints'] }\n",
    "\n",
    "class RandomGrayscale(object):\n",
    "    def __init__(self, p=0.33):\n",
    "        self.tfm = transforms.RandomGrayscale(p=p)\n",
    "        self.gs = transforms.Grayscale(num_output_channels=3)\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image = self.tfm(sample['image'])\n",
    "        if(len(image.getbands())<3):\n",
    "            image = self.gs(image)\n",
    "            sample['image_stg_input'] = self.gs(sample['image_stg_input'])\n",
    "        return { 'image' : image, 'image_stg_input' : sample['image_stg_input'], 'keypoints': sample['keypoints'] }\n",
    "\n",
    "class RandomRotateImgAndKeypoints(object):\n",
    "    def __init__(self, deg=30, p=0.9):\n",
    "        self.deg = deg\n",
    "        self.p = p\n",
    "    \n",
    "    def __rotate__(self, origin, keypoints, deg, sz):\n",
    "        ox, oy = origin\n",
    "        theta = np.math.radians(-deg) #-deg since we measure y,x from top left and not w/2,h/2\n",
    "        X = keypoints[:,:,0][keypoints[:,:,2]>0]\n",
    "        Y = keypoints[:,:,1][keypoints[:,:,2]>0]\n",
    "        \n",
    "        keypoints[:,:,0][keypoints[:,:,2]>0] = ox + (np.math.cos(theta)*(X - ox) - np.math.sin(theta)*(Y - oy)) \n",
    "        keypoints[:,:,1][keypoints[:,:,2]>0] = oy + (np.math.sin(theta)*(X - ox) + np.math.cos(theta)*(Y - oy)) \n",
    "        \n",
    "        inds = np.logical_or(np.any((keypoints[:,:,:2]<0), axis=2), np.any((keypoints[:,:,:2]>sz), axis=2))\n",
    "        keypoints[inds,:] = np.array([0,0,0])\n",
    "        return keypoints\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        if(np.random.random()>(1-self.p)):\n",
    "            image = sample['image']\n",
    "            keypoints = sample['keypoints'].copy()\n",
    "            rand_deg = np.random.randint(-1*self.deg, self.deg+1)\n",
    "            image = image.rotate(rand_deg)\n",
    "            w, h = image.size\n",
    "            res = self.__rotate__((w/2, h/2), keypoints, rand_deg, h)\n",
    "            return { 'image' : image, 'image_stg_input' : sample['image_stg_input'].rotate(rand_deg) ,'keypoints' : res }\n",
    "        else:\n",
    "            return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __init__(self):\n",
    "        self.ToTensor = transforms.ToTensor()\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        return { 'image' : self.ToTensor(sample['image']),\n",
    "                 'image_stg_input' : self.ToTensor(sample['image_stg_input']),\n",
    "                 'pafs' : torch.tensor(sample['pafs'], dtype=torch.float),\n",
    "                 'PAF_BINARY_IND' : torch.tensor(sample['PAF_BINARY_IND'], dtype=torch.uint8),\n",
    "                 'heatmaps' : torch.tensor(sample['heatmaps'], dtype=torch.float),\n",
    "                 'HM_BINARY_IND' : torch.tensor(sample['HM_BINARY_IND'], dtype=torch.uint8),\n",
    "                }\n",
    "\n",
    "class NormalizeImg(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.normalize = transforms.Normalize(mean, std)\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        sample['image'] = self.normalize(sample['image'])\n",
    "        return sample\n",
    "\n",
    "class UnNormalizeImgBatch(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean.reshape((1,3,1,1))\n",
    "        self.std = std.reshape((1,3,1,1))\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        return (batch*self.std) + self.mean\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size=368):\n",
    "        self.size = size\n",
    "    \n",
    "    def __call__(self, im):\n",
    "        if(im.height > im.width):\n",
    "            w = int(self.size*im.width/im.height)\n",
    "            h = self.size\n",
    "            pad_val = int((self.size-w)/2)\n",
    "            pad = (self.size-w-pad_val,0,pad_val,0)\n",
    "        else:\n",
    "            h = int(self.size*im.height/im.width)\n",
    "            w = self.size\n",
    "            pad_val = int((self.size-h)/2)\n",
    "            pad = (0,self.size-h-pad_val,0,pad_val)\n",
    "        return ImageOps.expand(im.resize((w,h),resample=Image.BILINEAR), pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014989,
     "end_time": "2021-03-07T17:14:30.324077",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.309088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.035652,
     "end_time": "2021-03-07T17:14:30.374931",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.339279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CALCULATE_PAF_MASK(fliped_img, joint_pair, keypoints, limb_width):\n",
    "    j1_idx, j2_idx = joint_pair[0], joint_pair[1]\n",
    "    mask = np.zeros((46, 46))               #in x,y order\n",
    "    col, row = np.ogrid[:46, :46]\n",
    "    \n",
    "    paf_p_x = np.zeros((len(keypoints), 46, 46))\n",
    "    paf_p_y = np.zeros((len(keypoints), 46, 46))\n",
    "    NON_ZERO_VEC_COUNT = np.zeros((2, 46, 46))\n",
    "    PAF_IND = False\n",
    "    final_paf_map = np.zeros((2, 46, 46))\n",
    "    \n",
    "    for i, item in enumerate(keypoints):\n",
    "        j1, j2 =  item[j1_idx][:2], item[j2_idx][:2]\n",
    "        keypoints_detected = item[j1_idx][2] and item[j2_idx][2]\n",
    "        PAF_IND = PAF_IND or keypoints_detected>0\n",
    "        if(keypoints_detected):\n",
    "            limb_length = np.linalg.norm(j2 - j1)\n",
    "            if(limb_length > 1e-8):\n",
    "                v = (j2 - j1) / limb_length\n",
    "                v_perp = np.array([v[1], -v[0]])\n",
    "                center_point = (j1 + j2)/2\n",
    "                \n",
    "                cond1 = np.abs(np.dot(v, np.array([col, row]) - center_point))<=limb_length/2\n",
    "                cond2 = np.abs(np.dot(v_perp, np.array([col, row]) - j1))<=limb_width\n",
    "                mask = np.logical_and(cond1, cond2)\n",
    "                \n",
    "                paf_p_x[i], paf_p_y[i] = mask*v[0], mask*v[1]\n",
    "                if(v[0]):\n",
    "                    NON_ZERO_VEC_COUNT[0][mask] +=1\n",
    "                if(v[1]):\n",
    "                    NON_ZERO_VEC_COUNT[1][mask] +=1\n",
    "\n",
    "    NON_ZERO_VEC_COUNT[NON_ZERO_VEC_COUNT==0] = 1\n",
    "    final_paf_map[0], final_paf_map[1] = (paf_p_x.sum(axis=0)/NON_ZERO_VEC_COUNT[0]), (paf_p_y.sum(axis=0)/NON_ZERO_VEC_COUNT[1])\n",
    "    return final_paf_map, PAF_IND\n",
    "\n",
    "\n",
    "\n",
    "def GET_PAF_MAP(img_stg_input, keypoints, limb_width, part_pairs=SKELETON):\n",
    "    img_stg_input = np.array(img_stg_input)\n",
    "    downsample_ratio = 0.125\n",
    "\n",
    "    pafs = np.zeros((len(part_pairs)*2, 46, 46))\n",
    "    PAF_BINARY_IND = np.zeros(len(part_pairs)*2)\n",
    "    fliped_img = img_stg_input.transpose((1,0,2))\n",
    "    kps_copy = keypoints.copy()\n",
    "    kps_copy[:,:,:2][kps_copy[:,:,2]>0] = kps_copy[:,:,:2][kps_copy[:,:,2]>0]*downsample_ratio\n",
    "    \n",
    "    for i, joint_pair in enumerate(part_pairs):\n",
    "        mask, PAF_IS_LABELED = CALCULATE_PAF_MASK(fliped_img, joint_pair, kps_copy, downsample_ratio*limb_width)\n",
    "\n",
    "        PAF_BINARY_IND[2*i], PAF_BINARY_IND[(2*i)+1]  = int(PAF_IS_LABELED), int(PAF_IS_LABELED)\n",
    "        mask = mask.transpose((0,2,1))\n",
    "        pafs[2*i], pafs[(2*i) +1] = mask[0], mask[1]   #x component, y component of v\n",
    "    return pafs, PAF_BINARY_IND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.031608,
     "end_time": "2021-03-07T17:14:30.423164",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.391556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CALCULATE_HEATMAP(fliped_img, kp_id, keypoints, pad, hm_sz):\n",
    "    H,W = fliped_img.shape[:2]\n",
    "    ps = 15\n",
    "    g_vals = heatmap_ps_map['ps_vals'].copy()\n",
    "    if(kp_id in SMALLER_HEATMAP_GROUP):\n",
    "        ps = int(heatmap_ps_map['ps_small'])\n",
    "        g_vals = heatmap_ps_map['ps_small_vals'].copy()\n",
    "    \n",
    "    ps_hf = ps//2\n",
    "    points = keypoints[:,kp_id, :2][keypoints[:,kp_id,2]>0]\n",
    "    points = np.rint(points).astype(int)\n",
    "    KEYPOINT_EXISTS = (len(points)>0)\n",
    "    mask = np.zeros((H, W))\n",
    "    \n",
    "    for (x,y) in points:\n",
    "        mask[x-ps_hf : x+ps_hf+1, y-ps_hf : y+ps_hf+1] = g_vals\n",
    "    \n",
    "    mask = mask[pad:-pad, pad:-pad]\n",
    "    return mask, KEYPOINT_EXISTS\n",
    "\n",
    "\n",
    "\n",
    "def GET_HEAT_MAP(img, keypoints, sigma):\n",
    "    img = np.array(img)\n",
    "    h,w = img.shape[:2]\n",
    "    pad = 8\n",
    "    kp_ids = np.arange(0,17)\n",
    "    img = np.pad(img, pad_width=[(pad,pad),(pad,pad),(0,0)], mode='constant', constant_values=0)\n",
    "    heatmaps = np.zeros((len(kp_ids)+1, h, w))\n",
    "    HM_BINARY_IND = np.zeros(len(kp_ids)+1)\n",
    "    fliped_img = img.transpose((1,0,2)) # why flip ?\n",
    "    kps_copy = keypoints.copy()\n",
    "    \n",
    "    kps_copy[:,:,:2][kps_copy[:,:,2]>0] = kps_copy[:,:,:2][kps_copy[:,:,2]>0]+pad\n",
    "    \n",
    "    for i, kp_id in enumerate(kp_ids):\n",
    "        mask, HM_IS_LABELED = CALCULATE_HEATMAP(fliped_img, kp_id, kps_copy, pad, h)\n",
    "        HM_BINARY_IND[i] = int(HM_IS_LABELED)\n",
    "        mask = mask.transpose()\n",
    "        heatmaps[i] = mask\n",
    "        \n",
    "    heatmaps[len(kp_ids)] = np.ones((h,w)) - np.sum(heatmaps, axis=0)\n",
    "    \n",
    "    HM_BINARY_IND[len(kp_ids)] = 1\n",
    "    \n",
    "    return heatmaps, HM_BINARY_IND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.032759,
     "end_time": "2021-03-07T17:14:30.471776",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.439017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class COCO_Person_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file, tfms, tensor_tfms, devide=None):\n",
    "        super(COCO_Person_Dataset, self).__init__()\n",
    "        \n",
    "        self.devide = devide\n",
    "        json_file = json.load(open(file))\n",
    "        self.images = []\n",
    "        self.keypoints = []\n",
    "        for f in json_file:\n",
    "            self.images.append(f[0]) # image path\n",
    "            key_list = f[1]\n",
    "            keypoints = []\n",
    "            for k in key_list:\n",
    "                keypoints.append(list(zip(k[::3],k[1::3], k[2::3])))\n",
    "            self.keypoints.append(np.array(keypoints))\n",
    "#         self.keypoints = np.array(self.keypoints)\n",
    "            \n",
    "        self.len = len(self.images)        \n",
    "        self.tfms = tfms\n",
    "        self.tensor_tfms = tensor_tfms\n",
    "        self.get_heatmap_masks = GET_HEAT_MAP     \n",
    "        self.get_paf_masks = GET_PAF_MAP\n",
    "        \n",
    "        self.limb_width = 5        \n",
    "        self.sigma = 7\n",
    "            \n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.images[index].split('/coco2014/')[1]).copy()\n",
    "        if(image.mode!='RGB'):\n",
    "#             print('gray value image: '+self.images[index])\n",
    "            image = image.convert('RGB')\n",
    "        keypoints = self.keypoints[index].copy()\n",
    "        \n",
    "        if self.tfms:\n",
    "            tfmd_sample = self.tfms({\"image\":image, \"keypoints\":keypoints})\n",
    "            image, image_stg_input, keypoints = tfmd_sample[\"image\"], tfmd_sample[\"image_stg_input\"], tfmd_sample[\"keypoints\"]\n",
    "        \n",
    "        heatmaps, HM_BINARY_IND = self.get_heatmap_masks(image, keypoints, self.sigma)\n",
    "        pafs, PAF_BINARY_IND = self.get_paf_masks(image_stg_input, keypoints, self.limb_width) \n",
    "            \n",
    "        if self.tensor_tfms:\n",
    "            res = self.tensor_tfms({\"image\":image, \"image_stg_input\": image_stg_input, \"pafs\":pafs, \"PAF_BINARY_IND\":PAF_BINARY_IND, \"heatmaps\":heatmaps, \"HM_BINARY_IND\":HM_BINARY_IND})\n",
    "            image = res[\"image\"]\n",
    "            image_stg_input = res[\"image_stg_input\"]\n",
    "            pafs = res[\"pafs\"]\n",
    "            PAF_BINARY_IND = res[\"PAF_BINARY_IND\"]\n",
    "            heatmaps = res[\"heatmaps\"]\n",
    "            HM_BINARY_IND = res[\"HM_BINARY_IND\"]\n",
    "        return (image, image_stg_input, pafs, PAF_BINARY_IND, heatmaps, HM_BINARY_IND)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.devide is not None:\n",
    "            return self.len//self.devide\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022915,
     "end_time": "2021-03-07T17:14:30.513075",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.490160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model\n",
    "10658963 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.026497,
     "end_time": "2021-03-07T17:14:30.559206",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.532709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(F, self).__init__()\n",
    "        self.vgg = vgg19(pretrained=False).features[:23]\n",
    "        self.conv_4_3_and_4_4 = nn.Sequential(\n",
    "                                              nn.Conv2d(512, 256, 3, 1, 1),\n",
    "                                              nn.BatchNorm2d(256),\n",
    "                                              nn.ReLU(inplace=True),\n",
    "                                              nn.Conv2d(256, 125, 3, 1, 1),\n",
    "                                              nn.BatchNorm2d(125),\n",
    "                                              nn.ReLU(inplace=True)\n",
    "                                              )\n",
    "        \n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_4_3_and_4_4(self.vgg(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.028162,
     "end_time": "2021-03-07T17:14:30.604261",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.576099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conv_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=128):\n",
    "        #64, 32, 32\n",
    "        super(Conv_Block, self).__init__()\n",
    "        self.C1 = nn.Sequential(\n",
    "                                nn.Conv2d(in_channels, 64, 3, 1, 1),\n",
    "                                nn.BatchNorm2d(64),\n",
    "                                nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.C2 = nn.Sequential(\n",
    "                                nn.Conv2d(64, 32, 3, 1, 1),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.C3 = nn.Sequential(\n",
    "                                nn.Conv2d(32, 32, 3, 1, 1),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.ReLU(inplace=True)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        c1_out = self.C1(x)\n",
    "        c2_out = self.C2(c1_out)\n",
    "        c3_out = self.C3(c2_out)\n",
    "        return torch.cat((c1_out, c2_out, c3_out), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.022893,
     "end_time": "2021-03-07T17:14:30.644550",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.621657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_stage_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "                         Conv_Block(in_channels),\n",
    "                         Conv_Block(128),\n",
    "                         Conv_Block(128),\n",
    "                         Conv_Block(128),\n",
    "                         Conv_Block(128),\n",
    "                         nn.Conv2d(128,128,1,1,0),\n",
    "                         nn.BatchNorm2d(128),\n",
    "                         nn.ReLU(inplace=True),\n",
    "                         nn.Conv2d(128,out_channels,1,1,0)\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.025788,
     "end_time": "2021-03-07T17:14:30.686045",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.660257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PAF_Stages(nn.Module):\n",
    "    # 46 = 39+8 ?\n",
    "    def __init__(self, in_channels=128, paf_out_channels=38+8):\n",
    "        super(PAF_Stages, self).__init__()\n",
    "        self.Stage1 = get_stage_block(in_channels, paf_out_channels)\n",
    "        self.Stage2 = get_stage_block(in_channels+paf_out_channels, paf_out_channels)\n",
    "        self.Stage3 = get_stage_block(in_channels+paf_out_channels, paf_out_channels)\n",
    "        self.Stage4 = get_stage_block(in_channels+paf_out_channels, paf_out_channels)\n",
    "\n",
    "    def forward(self, img_stg_input, F):\n",
    "        res = []\n",
    "        o1 = self.Stage1(torch.cat((img_stg_input.clone(), F.clone()), dim=1))\n",
    "        res.append(o1)\n",
    "        o2 = self.Stage2(torch.cat((img_stg_input.clone(), F.clone(), o1), dim=1))\n",
    "        res.append(o2)\n",
    "        o3 = self.Stage3(torch.cat((img_stg_input.clone(), F.clone(), o2), dim=1))\n",
    "        res.append(o3)\n",
    "        o4 = self.Stage4(torch.cat((img_stg_input.clone(), F.clone(), o3), dim=1))\n",
    "        res.append(o4)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.024316,
     "end_time": "2021-03-07T17:14:30.726142",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.701826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Heatmap_Stages(nn.Module):\n",
    "    def __init__(self, in_channels=128+38+8, hm_out_channels=18):\n",
    "        super(Heatmap_Stages, self).__init__()\n",
    "        self.Stage1 = get_stage_block(in_channels, hm_out_channels)\n",
    "        self.Stage2 = get_stage_block(in_channels+hm_out_channels, hm_out_channels)\n",
    "    \n",
    "    def forward(self, img_stg_input, F, L):\n",
    "        res = []\n",
    "        o1 = self.Stage1(torch.cat((img_stg_input.clone(), F.clone(), L.clone()), dim=1))\n",
    "        res.append(o1)\n",
    "        o2 = self.Stage2(torch.cat((img_stg_input.clone(), F.clone(), L.clone(), o1), dim=1))\n",
    "        res.append(o2)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.023649,
     "end_time": "2021-03-07T17:14:30.765599",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.741950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.F = F()\n",
    "        self.PAF_Stages = PAF_Stages()\n",
    "        self.Heatmap_Stages = Heatmap_Stages()\n",
    "    \n",
    "    def forward(self, img, img_stg_input):\n",
    "        image_features = self.F(img)\n",
    "        pafs_op = self.PAF_Stages(img_stg_input, image_features)\n",
    "        heatmaps_op = self.Heatmap_Stages(img_stg_input, image_features, pafs_op[3])\n",
    "        return pafs_op, heatmaps_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.021806,
     "end_time": "2021-03-07T17:14:30.803582",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.781776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# # model.load_state_dict(torch.load('model-wts-368.ckpt', map_location='cpu'))\n",
    "# paf, hm = model(torch.randn(4, 3, 368,368),torch.randn((4,3,46,46)))\n",
    "# print(len(paf))\n",
    "# print(paf[0].shape)\n",
    "# print(len(hm))\n",
    "# print(hm[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015785,
     "end_time": "2021-03-07T17:14:30.835519",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.819734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 0.026816,
     "end_time": "2021-03-07T17:14:30.879630",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.852814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def paf_and_heatmap_loss(pred_pafs_stages, pafs_gt, paf_inds, pred_hms_stages, hms_gt, hm_inds, IM_SIZE):\n",
    "    total_paf_loss, total_hm_loss ,mean_paf_loss,mean_hm_loss= 0, 0,0,0\n",
    "   \n",
    "    for paf_stg in pred_pafs_stages:\n",
    "            #scaled_pafs = Func.interpolate(paf_stg, IM_SIZE, mode=\"bilinear\", align_corners=True).to(device)\n",
    "#         print(paf_stg.shape)\n",
    "#         print(paf_stg[paf_inds].shape)\n",
    "#         print(pafs_gt[paf_inds].shape)\n",
    "#         print(paf_inds)\n",
    "#         print(paf_inds.shape)\n",
    "        \n",
    "        stg_paf_loss = torch.square(torch.norm(paf_stg[paf_inds] - pafs_gt[paf_inds],dim=(1,2))).sum()\n",
    "        paf_mean = torch.square(torch.norm(paf_stg[paf_inds] - pafs_gt[paf_inds],dim=(1,2))).mean().item()\n",
    "        \n",
    "#         print(stg_paf_loss.requires_grad)\n",
    "        total_paf_loss += stg_paf_loss\n",
    "        mean_paf_loss += paf_mean\n",
    "    \n",
    "    for hm_stg in pred_hms_stages:\n",
    "        scaled_hms = Func.interpolate(hm_stg, IM_SIZE, mode=\"bilinear\", align_corners=True).to(device)\n",
    "        stg_hm_loss = torch.square(torch.norm(scaled_hms[hm_inds] - hms_gt[hm_inds],dim=(1,2))).sum()\n",
    "        hm_mean = torch.square(torch.norm(scaled_hms[hm_inds] - hms_gt[hm_inds],dim=(1,2))).mean().item()\n",
    "        total_hm_loss += stg_hm_loss\n",
    "        mean_hm_loss += hm_mean \n",
    "   \n",
    "    return total_paf_loss + total_hm_loss, mean_paf_loss+mean_hm_loss, mean_paf_loss, mean_hm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016729,
     "end_time": "2021-03-07T17:14:30.912442",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.895713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.037943,
     "end_time": "2021-03-07T17:14:30.966797",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.928854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pkmodel(model, optimizer, train_loader, val_loader, scheduler=None):\n",
    "    LR = 0.001\n",
    "    train_step = int(train_size/BATCH_SIZE/500)\n",
    "#     train_step=1\n",
    "    \n",
    "    train_begin = time.time()\n",
    "    min_loss = float(\"inf\")\n",
    "    train_min_loss = float(\"inf\")\n",
    "    change_lr_count = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())   \n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(EPOCH):\n",
    "        print('Epoch {}/{}'.format(epoch, EPOCH - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()              \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        paf_running_loss =0.0\n",
    "        hm_running_loss =0.0\n",
    "        \n",
    "            \n",
    "        for itera,(imgs, imgs_stg_input, pafs, paf_inds, hms, hm_inds) in enumerate(train_loader):\n",
    "            \n",
    "            if step == 2000:\n",
    "#             if step == 2:\n",
    "                for param in model.F.vgg.parameters():\n",
    "                    param.requires_grad = True\n",
    "                optimizer.add_param_group({'params' :  model.F.vgg.parameters(), 'lr' : LR / 4}) \n",
    "                print('VGG activated')\n",
    "            \n",
    "            optimizer.zero_grad()          \n",
    "            \n",
    "            imgs, imgs_stg_input, pafs, paf_inds, hms, hm_inds = imgs.to(device), imgs_stg_input.to(device), pafs.to(device), paf_inds.to(device), hms.to(device), hm_inds.to(device) \n",
    "            pred_pafs, pred_hms = model(imgs, imgs_stg_input)\n",
    "            loss, mean, paf_mean, hm_mean = paf_and_heatmap_loss(pred_pafs, pafs, paf_inds, pred_hms, hms, hm_inds, IM_SIZE)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if itera%train_step==0:\n",
    "#                 torch.save(model.state_dict(),'output/'+TRIAL+'_itera.pth')\n",
    "                print(\"Iteration: {:6}    step: {:8}     combined loss: {:.5f}     paf loss {:.5f}     hm loss {:.5f}\".format(itera,step,mean, paf_mean, hm_mean))\n",
    "                iter_stats['train'].append([mean,paf_mean, hm_mean])\n",
    "#             print(loss.item())\n",
    "            \n",
    "            running_loss += mean\n",
    "            paf_running_loss += paf_mean\n",
    "            hm_running_loss += hm_mean\n",
    "#             print(running_loss)\n",
    "            \n",
    "            step+=1\n",
    "            \n",
    "            if mean<train_min_loss:\n",
    "                train_min_loss = mean\n",
    "                change_lr_count = 0\n",
    "            else:\n",
    "                change_lr_count +=1\n",
    "                \n",
    "            if change_lr_count >=2000 and change_lr_count!=0:\n",
    "                \n",
    "                for params in optimizer.param_groups:\n",
    "                    params['lr'] /= 2\n",
    "                print(f'learning rate change: {LR} --> {LR/2}')\n",
    "                LR/=2\n",
    "                change_lr_count = 0\n",
    "            \n",
    "        epoch_loss = running_loss / itera\n",
    "        paf_epoch_loss = paf_running_loss / itera\n",
    "        hm_epoch_loss = hm_running_loss / itera\n",
    "        \n",
    "        epoch_stats['train'].append([epoch_loss,paf_epoch_loss,hm_epoch_loss])\n",
    "        \n",
    "        \n",
    "        print('Train Loss: {:.4f}    PAF Loss:  {:.4f}    HM Loss:  {:.4f}    Acc: NA'.format(epoch_loss,paf_epoch_loss,hm_epoch_loss))\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            running_loss = 0.0\n",
    "            paf_running_loss =0.0\n",
    "            hm_running_loss =0.0\n",
    "\n",
    "            for itera,(imgs, imgs_stg_input, pafs, paf_inds, hms, hm_inds) in enumerate(valid_loader):\n",
    "                \n",
    "                imgs, imgs_stg_input, pafs, paf_inds, hms, hm_inds = imgs.to(device), imgs_stg_input.to(device), pafs.to(device), paf_inds.to(device), hms.to(device), hm_inds.to(device) \n",
    "                pred_pafs, pred_hms = model(imgs, imgs_stg_input)\n",
    "                loss,mean, paf_mean,hm_mean = paf_and_heatmap_loss(pred_pafs, pafs, paf_inds, pred_hms, hms, hm_inds, IM_SIZE)\n",
    "\n",
    "                running_loss += mean\n",
    "                paf_running_loss += paf_mean\n",
    "                hm_running_loss += hm_mean\n",
    "\n",
    "            epoch_loss = running_loss / itera\n",
    "            paf_epoch_loss = paf_running_loss / itera\n",
    "            hm_epoch_loss = hm_running_loss / itera\n",
    "        \n",
    "            epoch_stats['valid'].append([epoch_loss,paf_epoch_loss,hm_epoch_loss])          \n",
    "            print('Val Loss: {:.4f}    PAF Loss:  {:.4f}    HM Loss:  {:.4f}     Acc: NA'.format(epoch_loss,paf_epoch_loss,hm_epoch_loss))\n",
    "        \n",
    "        \n",
    "            if epoch_loss < min_loss:\n",
    "                min_loss = epoch_loss\n",
    "                torch.save(model.state_dict(),'output/'+TRIAL+'_best.pth')\n",
    "                \n",
    "        json.dump(epoch_stats, open(f'output/{TRIAL}_{EPOCH}epochs_e.json','w'))\n",
    "        json.dump(iter_stats, open(f'output/{TRIAL}_{EPOCH}epochs_i.json','w'))\n",
    "        torch.save(model.state_dict(),f'output/{TRIAL}_{epoch}.pth')\n",
    "\n",
    "    train_length = time.time() - train_begin\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(train_length // 60, train_length % 60))\n",
    "    print('Min train loss: {:4f}'.format(min_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 0.031494,
     "end_time": "2021-03-07T17:14:31.015015",
     "exception": false,
     "start_time": "2021-03-07T17:14:30.983521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IM_SIZE = 368\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "EPOCH = 10\n",
    "TRIAL = 'scratch_10e'\n",
    "\n",
    "mean, std = torch.tensor([0.485, 0.456, 0.406]), torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    RandomCrop(size=368, p=0.5),\n",
    "    ResizeImgAndKeypoints(size=IM_SIZE),\n",
    "    ColorJitter(),\n",
    "    RandomGrayscale(),\n",
    "    FlipHR(p=0.5),\n",
    "    #FlipUD(),\n",
    "    #RandomRotateImgAndKeypoints(deg=30, p=1)\n",
    "])\n",
    "\n",
    "valid_tfms = transforms.Compose([\n",
    "    ResizeImgAndKeypoints(size=IM_SIZE),\n",
    "])\n",
    "\n",
    "tensor_tfms = transforms.Compose([\n",
    "    ToTensor(),\n",
    "    NormalizeImg(mean, std)\n",
    "])\n",
    "\n",
    "# UnNormalize = UnNormalizeImgBatch(mean, std)\n",
    "# To_Pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 20.778314,
     "end_time": "2021-03-07T17:14:51.809998",
     "exception": false,
     "start_time": "2021-03-07T17:14:31.031684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45174\n",
      "10817\n"
     ]
    }
   ],
   "source": [
    "train_set = COCO_Person_Dataset('util_data/train_pk.json',train_tfms,tensor_tfms,devide=None)\n",
    "valid_set = COCO_Person_Dataset('util_data/val_pk.json',valid_tfms,tensor_tfms,devide=2)\n",
    "train_size = len(train_set)\n",
    "val_size = len(valid_set)\n",
    "print(train_size)\n",
    "print(val_size)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, BATCH_SIZE)\n",
    "\n",
    "model = Net()\n",
    "# model.load_state_dict(torch.load('util data/model-wts-368.ckpt'))\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam([\n",
    "                {'params' : model.F.conv_4_3_and_4_4.parameters() , 'lr' : LR / 4},\n",
    "                {'params' : model.PAF_Stages.parameters() , 'lr' : LR},\n",
    "                {'params' : model.Heatmap_Stages.parameters() , 'lr' : LR},\n",
    "            ])\n",
    "\n",
    "iter_stats = {'train':[]}\n",
    "epoch_stats = {'train':[],'valid':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-03-07T17:14:51.827924",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Iteration:      0    step:        0     combined loss: 51133.03952     paf loss 1516.06100     hm loss 49616.97852\n",
      "Iteration:      5    step:        5     combined loss: 17773.79720     paf loss 772.30989     hm loss 17001.48730\n",
      "Iteration:     10    step:       10     combined loss: 7506.73367     paf loss 605.43875     hm loss 6901.29492\n",
      "Iteration:     15    step:       15     combined loss: 7079.09101     paf loss 313.70014     hm loss 6765.39087\n",
      "Iteration:     20    step:       20     combined loss: 5538.42130     paf loss 169.34123     hm loss 5369.08008\n",
      "Iteration:     25    step:       25     combined loss: 4179.02111     paf loss 157.77160     hm loss 4021.24951\n",
      "Iteration:     30    step:       30     combined loss: 5229.23663     paf loss 81.04962     hm loss 5148.18701\n",
      "Iteration:     35    step:       35     combined loss: 5731.15662     paf loss 100.60071     hm loss 5630.55591\n",
      "Iteration:     40    step:       40     combined loss: 5035.27223     paf loss 79.58912     hm loss 4955.68311\n",
      "Iteration:     45    step:       45     combined loss: 4474.73439     paf loss 72.54250     hm loss 4402.19189\n",
      "Iteration:     50    step:       50     combined loss: 4186.57923     paf loss 57.01453     hm loss 4129.56470\n",
      "Iteration:     55    step:       55     combined loss: 3901.87354     paf loss 43.84120     hm loss 3858.03235\n",
      "Iteration:     60    step:       60     combined loss: 2971.69493     paf loss 48.33861     hm loss 2923.35632\n",
      "Iteration:     65    step:       65     combined loss: 4414.08053     paf loss 41.60787     hm loss 4372.47266\n",
      "Iteration:     70    step:       70     combined loss: 5813.87895     paf loss 46.43217     hm loss 5767.44678\n",
      "Iteration:     75    step:       75     combined loss: 4790.58018     paf loss 29.49497     hm loss 4761.08521\n",
      "Iteration:     80    step:       80     combined loss: 4350.66010     paf loss 40.43647     hm loss 4310.22363\n",
      "Iteration:     85    step:       85     combined loss: 5835.55242     paf loss 31.67400     hm loss 5803.87842\n",
      "Iteration:     90    step:       90     combined loss: 4538.95391     paf loss 44.74663     hm loss 4494.20728\n",
      "Iteration:     95    step:       95     combined loss: 4053.04354     paf loss 32.11214     hm loss 4020.93140\n",
      "Iteration:    100    step:      100     combined loss: 4443.62437     paf loss 29.57188     hm loss 4414.05249\n",
      "Iteration:    105    step:      105     combined loss: 4711.09387     paf loss 28.26818     hm loss 4682.82568\n",
      "Iteration:    110    step:      110     combined loss: 9312.76756     paf loss 25.10838     hm loss 9287.65918\n",
      "Iteration:    115    step:      115     combined loss: 4549.22475     paf loss 27.62123     hm loss 4521.60352\n",
      "Iteration:    120    step:      120     combined loss: 3513.08924     paf loss 34.34193     hm loss 3478.74731\n",
      "Iteration:    125    step:      125     combined loss: 4921.57847     paf loss 31.04575     hm loss 4890.53271\n",
      "Iteration:    130    step:      130     combined loss: 4273.71863     paf loss 25.17615     hm loss 4248.54248\n",
      "Iteration:    135    step:      135     combined loss: 4721.65819     paf loss 30.49657     hm loss 4691.16162\n",
      "Iteration:    140    step:      140     combined loss: 5187.43224     paf loss 23.53356     hm loss 5163.89868\n",
      "Iteration:    145    step:      145     combined loss: 3375.81520     paf loss 24.14699     hm loss 3351.66821\n",
      "Iteration:    150    step:      150     combined loss: 4843.84074     paf loss 25.61858     hm loss 4818.22217\n",
      "Iteration:    155    step:      155     combined loss: 9421.25119     paf loss 35.80295     hm loss 9385.44824\n",
      "Iteration:    160    step:      160     combined loss: 4804.61253     paf loss 31.46433     hm loss 4773.14819\n",
      "Iteration:    165    step:      165     combined loss: 3281.01993     paf loss 23.36185     hm loss 3257.65808\n",
      "Iteration:    170    step:      170     combined loss: 3406.03095     paf loss 24.12091     hm loss 3381.91003\n",
      "Iteration:    175    step:      175     combined loss: 4685.07942     paf loss 27.73274     hm loss 4657.34668\n",
      "Iteration:    180    step:      180     combined loss: 3350.54407     paf loss 22.81103     hm loss 3327.73303\n",
      "Iteration:    185    step:      185     combined loss: 4079.16352     paf loss 23.95637     hm loss 4055.20715\n",
      "Iteration:    190    step:      190     combined loss: 5152.54464     paf loss 21.56954     hm loss 5130.97510\n",
      "Iteration:    195    step:      195     combined loss: 5338.04062     paf loss 29.98496     hm loss 5308.05566\n",
      "Iteration:    200    step:      200     combined loss: 4407.36704     paf loss 23.17514     hm loss 4384.19189\n",
      "Iteration:    205    step:      205     combined loss: 3289.98475     paf loss 24.11525     hm loss 3265.86951\n",
      "Iteration:    210    step:      210     combined loss: 3192.50777     paf loss 21.44295     hm loss 3171.06482\n",
      "Iteration:    215    step:      215     combined loss: 4372.66280     paf loss 19.92207     hm loss 4352.74072\n",
      "Iteration:    220    step:      220     combined loss: 3440.61851     paf loss 21.32151     hm loss 3419.29700\n",
      "Iteration:    225    step:      225     combined loss: 2885.58895     paf loss 18.33797     hm loss 2867.25098\n",
      "Iteration:    230    step:      230     combined loss: 3864.71873     paf loss 26.56187     hm loss 3838.15686\n",
      "Iteration:    235    step:      235     combined loss: 4171.59400     paf loss 25.70752     hm loss 4145.88647\n",
      "Iteration:    240    step:      240     combined loss: 4250.37713     paf loss 22.93011     hm loss 4227.44702\n",
      "Iteration:    245    step:      245     combined loss: 6605.81523     paf loss 29.91240     hm loss 6575.90283\n",
      "Iteration:    250    step:      250     combined loss: 3998.41439     paf loss 21.03414     hm loss 3977.38025\n",
      "Iteration:    255    step:      255     combined loss: 4388.20533     paf loss 26.24829     hm loss 4361.95703\n",
      "Iteration:    260    step:      260     combined loss: 3013.53867     paf loss 22.27708     hm loss 2991.26160\n",
      "Iteration:    265    step:      265     combined loss: 5612.99816     paf loss 26.40612     hm loss 5586.59204\n",
      "Iteration:    270    step:      270     combined loss: 5487.81590     paf loss 23.46117     hm loss 5464.35474\n",
      "Iteration:    275    step:      275     combined loss: 3738.34336     paf loss 25.83164     hm loss 3712.51172\n",
      "Iteration:    280    step:      280     combined loss: 4585.78247     paf loss 19.48999     hm loss 4566.29248\n",
      "Iteration:    285    step:      285     combined loss: 3048.51589     paf loss 24.82192     hm loss 3023.69397\n",
      "Iteration:    290    step:      290     combined loss: 2775.12388     paf loss 17.65110     hm loss 2757.47278\n",
      "Iteration:    295    step:      295     combined loss: 3441.02597     paf loss 21.60141     hm loss 3419.42456\n",
      "Iteration:    300    step:      300     combined loss: 4630.87186     paf loss 16.76444     hm loss 4614.10742\n",
      "Iteration:    305    step:      305     combined loss: 3091.44014     paf loss 22.41317     hm loss 3069.02698\n",
      "Iteration:    310    step:      310     combined loss: 4203.96106     paf loss 19.25647     hm loss 4184.70459\n",
      "Iteration:    315    step:      315     combined loss: 5156.28024     paf loss 24.56954     hm loss 5131.71069\n",
      "Iteration:    320    step:      320     combined loss: 5339.48766     paf loss 20.28429     hm loss 5319.20337\n",
      "Iteration:    325    step:      325     combined loss: 3828.68952     paf loss 25.21894     hm loss 3803.47058\n",
      "Iteration:    330    step:      330     combined loss: 7055.91009     paf loss 23.95672     hm loss 7031.95337\n",
      "Iteration:    335    step:      335     combined loss: 7254.96930     paf loss 30.70050     hm loss 7224.26880\n",
      "Iteration:    340    step:      340     combined loss: 4162.12214     paf loss 22.98176     hm loss 4139.14038\n",
      "Iteration:    345    step:      345     combined loss: 2377.26528     paf loss 25.17116     hm loss 2352.09412\n",
      "Iteration:    350    step:      350     combined loss: 3741.55518     paf loss 20.27808     hm loss 3721.27710\n",
      "Iteration:    355    step:      355     combined loss: 5289.68053     paf loss 24.38146     hm loss 5265.29907\n",
      "Iteration:    360    step:      360     combined loss: 6351.61052     paf loss 31.50359     hm loss 6320.10693\n",
      "Iteration:    365    step:      365     combined loss: 7122.85796     paf loss 25.92681     hm loss 7096.93115\n",
      "Iteration:    370    step:      370     combined loss: 6209.14171     paf loss 24.00743     hm loss 6185.13428\n",
      "Iteration:    375    step:      375     combined loss: 4832.43463     paf loss 19.30255     hm loss 4813.13208\n",
      "Iteration:    380    step:      380     combined loss: 5544.90099     paf loss 24.82506     hm loss 5520.07593\n",
      "Iteration:    385    step:      385     combined loss: 4295.89868     paf loss 20.39941     hm loss 4275.49927\n",
      "Iteration:    390    step:      390     combined loss: 5791.13650     paf loss 27.47512     hm loss 5763.66138\n",
      "Iteration:    395    step:      395     combined loss: 3607.96475     paf loss 18.96915     hm loss 3588.99561\n",
      "Iteration:    400    step:      400     combined loss: 5438.86872     paf loss 21.89631     hm loss 5416.97241\n",
      "Iteration:    405    step:      405     combined loss: 3131.96609     paf loss 18.56008     hm loss 3113.40601\n",
      "Iteration:    410    step:      410     combined loss: 4081.71888     paf loss 16.62952     hm loss 4065.08936\n",
      "Iteration:    415    step:      415     combined loss: 5231.99572     paf loss 18.94616     hm loss 5213.04956\n",
      "Iteration:    420    step:      420     combined loss: 2923.50527     paf loss 21.60195     hm loss 2901.90332\n",
      "Iteration:    425    step:      425     combined loss: 3578.06498     paf loss 21.35343     hm loss 3556.71155\n",
      "Iteration:    430    step:      430     combined loss: 6429.13055     paf loss 25.40985     hm loss 6403.72070\n",
      "Iteration:    435    step:      435     combined loss: 3753.43846     paf loss 20.47398     hm loss 3732.96448\n",
      "Iteration:    440    step:      440     combined loss: 4087.99684     paf loss 19.69032     hm loss 4068.30652\n",
      "Iteration:    445    step:      445     combined loss: 3198.19049     paf loss 15.96051     hm loss 3182.22998\n",
      "Iteration:    450    step:      450     combined loss: 5157.57417     paf loss 19.97627     hm loss 5137.59790\n",
      "Iteration:    455    step:      455     combined loss: 4043.17599     paf loss 23.91464     hm loss 4019.26135\n",
      "Iteration:    460    step:      460     combined loss: 6529.00900     paf loss 22.88571     hm loss 6506.12329\n",
      "Iteration:    465    step:      465     combined loss: 4452.25779     paf loss 21.51047     hm loss 4430.74731\n",
      "Iteration:    470    step:      470     combined loss: 4561.20285     paf loss 25.28293     hm loss 4535.91992\n",
      "Iteration:    475    step:      475     combined loss: 5047.57749     paf loss 24.11045     hm loss 5023.46704\n",
      "Iteration:    480    step:      480     combined loss: 4673.39890     paf loss 24.44846     hm loss 4648.95044\n",
      "Iteration:    485    step:      485     combined loss: 5213.84843     paf loss 20.25736     hm loss 5193.59106\n",
      "Iteration:    490    step:      490     combined loss: 5774.10933     paf loss 24.13325     hm loss 5749.97607\n",
      "Iteration:    495    step:      495     combined loss: 2600.31213     paf loss 18.15819     hm loss 2582.15393\n",
      "Iteration:    500    step:      500     combined loss: 4314.09956     paf loss 15.26094     hm loss 4298.83862\n",
      "Iteration:    505    step:      505     combined loss: 4652.40477     paf loss 22.60399     hm loss 4629.80078\n",
      "Iteration:    510    step:      510     combined loss: 5893.39913     paf loss 29.69210     hm loss 5863.70703\n",
      "Iteration:    515    step:      515     combined loss: 4110.42545     paf loss 18.13236     hm loss 4092.29309\n",
      "Iteration:    520    step:      520     combined loss: 4222.92936     paf loss 26.16813     hm loss 4196.76123\n",
      "Iteration:    525    step:      525     combined loss: 3920.79299     paf loss 22.37063     hm loss 3898.42236\n",
      "Iteration:    530    step:      530     combined loss: 3600.06569     paf loss 17.13881     hm loss 3582.92688\n",
      "Iteration:    535    step:      535     combined loss: 3086.21574     paf loss 22.09831     hm loss 3064.11743\n",
      "Iteration:    540    step:      540     combined loss: 4452.14218     paf loss 24.01962     hm loss 4428.12256\n",
      "Iteration:    545    step:      545     combined loss: 3301.47376     paf loss 19.74805     hm loss 3281.72571\n",
      "Iteration:    550    step:      550     combined loss: 4071.77950     paf loss 24.60384     hm loss 4047.17566\n",
      "Iteration:    555    step:      555     combined loss: 5508.10981     paf loss 24.04194     hm loss 5484.06787\n",
      "Iteration:    560    step:      560     combined loss: 3715.22830     paf loss 18.32827     hm loss 3696.90002\n",
      "Iteration:    565    step:      565     combined loss: 7304.92872     paf loss 29.13697     hm loss 7275.79175\n",
      "Iteration:    570    step:      570     combined loss: 4240.46786     paf loss 24.42001     hm loss 4216.04785\n",
      "Iteration:    575    step:      575     combined loss: 4448.61875     paf loss 23.20054     hm loss 4425.41821\n",
      "Iteration:    580    step:      580     combined loss: 4318.83633     paf loss 18.02359     hm loss 4300.81274\n",
      "Iteration:    585    step:      585     combined loss: 4893.20881     paf loss 22.25740     hm loss 4870.95142\n",
      "Iteration:    590    step:      590     combined loss: 6945.76590     paf loss 25.10599     hm loss 6920.65991\n",
      "Iteration:    595    step:      595     combined loss: 5095.02677     paf loss 27.75504     hm loss 5067.27173\n",
      "Iteration:    600    step:      600     combined loss: 2554.06000     paf loss 23.21686     hm loss 2530.84314\n",
      "Iteration:    605    step:      605     combined loss: 5770.03754     paf loss 19.87446     hm loss 5750.16309\n",
      "Iteration:    610    step:      610     combined loss: 4220.59977     paf loss 24.61515     hm loss 4195.98462\n",
      "Iteration:    615    step:      615     combined loss: 6709.80409     paf loss 25.44251     hm loss 6684.36157\n",
      "Iteration:    620    step:      620     combined loss: 5252.38131     paf loss 21.11618     hm loss 5231.26514\n",
      "Iteration:    625    step:      625     combined loss: 8405.98693     paf loss 29.03137     hm loss 8376.95557\n",
      "Iteration:    630    step:      630     combined loss: 2744.62735     paf loss 17.92740     hm loss 2726.69995\n",
      "Iteration:    635    step:      635     combined loss: 3258.08436     paf loss 19.41077     hm loss 3238.67358\n",
      "Iteration:    640    step:      640     combined loss: 4514.99864     paf loss 16.30601     hm loss 4498.69263\n",
      "Iteration:    645    step:      645     combined loss: 3383.32542     paf loss 19.41173     hm loss 3363.91370\n",
      "Iteration:    650    step:      650     combined loss: 5961.51045     paf loss 25.60371     hm loss 5935.90674\n",
      "Iteration:    655    step:      655     combined loss: 2823.12474     paf loss 17.19761     hm loss 2805.92712\n",
      "Iteration:    660    step:      660     combined loss: 3747.89509     paf loss 19.50263     hm loss 3728.39246\n",
      "Iteration:    665    step:      665     combined loss: 5275.69001     paf loss 20.61823     hm loss 5255.07178\n",
      "Iteration:    670    step:      670     combined loss: 3364.83452     paf loss 24.20842     hm loss 3340.62610\n",
      "Iteration:    675    step:      675     combined loss: 5348.48494     paf loss 27.78450     hm loss 5320.70044\n",
      "Iteration:    680    step:      680     combined loss: 2990.71585     paf loss 23.14517     hm loss 2967.57068\n",
      "Iteration:    685    step:      685     combined loss: 5038.52663     paf loss 25.36086     hm loss 5013.16577\n",
      "Iteration:    690    step:      690     combined loss: 4989.10346     paf loss 20.54462     hm loss 4968.55884\n",
      "Iteration:    695    step:      695     combined loss: 4862.54605     paf loss 18.84097     hm loss 4843.70508\n",
      "Iteration:    700    step:      700     combined loss: 4795.75019     paf loss 25.81684     hm loss 4769.93335\n",
      "Iteration:    705    step:      705     combined loss: 4050.30984     paf loss 20.22647     hm loss 4030.08337\n",
      "Iteration:    710    step:      710     combined loss: 4373.22424     paf loss 18.62048     hm loss 4354.60376\n",
      "Iteration:    715    step:      715     combined loss: 2763.35251     paf loss 18.33871     hm loss 2745.01379\n",
      "Iteration:    720    step:      720     combined loss: 2584.79793     paf loss 21.63265     hm loss 2563.16528\n",
      "Iteration:    725    step:      725     combined loss: 4327.00528     paf loss 26.21109     hm loss 4300.79419\n",
      "Iteration:    730    step:      730     combined loss: 5829.65369     paf loss 27.14368     hm loss 5802.51001\n",
      "Iteration:    735    step:      735     combined loss: 5282.56624     paf loss 25.01741     hm loss 5257.54883\n",
      "Iteration:    740    step:      740     combined loss: 3991.43284     paf loss 18.65367     hm loss 3972.77917\n",
      "Iteration:    745    step:      745     combined loss: 4108.76165     paf loss 13.41851     hm loss 4095.34314\n",
      "Iteration:    750    step:      750     combined loss: 4286.28455     paf loss 19.03943     hm loss 4267.24512\n",
      "Iteration:    755    step:      755     combined loss: 5174.38936     paf loss 19.78926     hm loss 5154.60010\n",
      "Iteration:    760    step:      760     combined loss: 7066.02565     paf loss 26.30348     hm loss 7039.72217\n",
      "Iteration:    765    step:      765     combined loss: 5885.63452     paf loss 23.88428     hm loss 5861.75024\n",
      "Iteration:    770    step:      770     combined loss: 4775.54081     paf loss 18.97856     hm loss 4756.56226\n",
      "Iteration:    775    step:      775     combined loss: 4636.23657     paf loss 21.25952     hm loss 4614.97705\n",
      "Iteration:    780    step:      780     combined loss: 4051.58441     paf loss 17.18682     hm loss 4034.39758\n",
      "Iteration:    785    step:      785     combined loss: 4610.64471     paf loss 27.89959     hm loss 4582.74512\n",
      "Iteration:    790    step:      790     combined loss: 3395.80763     paf loss 24.84156     hm loss 3370.96606\n",
      "Iteration:    795    step:      795     combined loss: 3320.32483     paf loss 23.62561     hm loss 3296.69922\n",
      "Iteration:    800    step:      800     combined loss: 3828.88994     paf loss 24.32719     hm loss 3804.56274\n",
      "Iteration:    805    step:      805     combined loss: 5892.68575     paf loss 29.14278     hm loss 5863.54297\n",
      "Iteration:    810    step:      810     combined loss: 3617.61649     paf loss 16.97452     hm loss 3600.64197\n",
      "Iteration:    815    step:      815     combined loss: 8082.23850     paf loss 28.96922     hm loss 8053.26929\n",
      "Iteration:    820    step:      820     combined loss: 4104.88770     paf loss 20.44104     hm loss 4084.44666\n",
      "Iteration:    825    step:      825     combined loss: 3307.28551     paf loss 21.32371     hm loss 3285.96179\n",
      "Iteration:    830    step:      830     combined loss: 4115.90307     paf loss 18.10119     hm loss 4097.80188\n",
      "Iteration:    835    step:      835     combined loss: 6285.97312     paf loss 23.79539     hm loss 6262.17773\n",
      "Iteration:    840    step:      840     combined loss: 3795.42003     paf loss 16.41430     hm loss 3779.00574\n",
      "Iteration:    845    step:      845     combined loss: 4236.45076     paf loss 22.96736     hm loss 4213.48340\n",
      "Iteration:    850    step:      850     combined loss: 2985.59456     paf loss 16.98702     hm loss 2968.60754\n",
      "Iteration:    855    step:      855     combined loss: 3489.96215     paf loss 22.88818     hm loss 3467.07397\n",
      "Iteration:    860    step:      860     combined loss: 3744.50821     paf loss 21.96598     hm loss 3722.54224\n",
      "Iteration:    865    step:      865     combined loss: 3545.24388     paf loss 18.25169     hm loss 3526.99219\n",
      "Iteration:    870    step:      870     combined loss: 4541.12161     paf loss 24.76345     hm loss 4516.35815\n",
      "Iteration:    875    step:      875     combined loss: 4447.97734     paf loss 16.70488     hm loss 4431.27246\n",
      "Iteration:    880    step:      880     combined loss: 5938.31039     paf loss 29.60385     hm loss 5908.70654\n",
      "Iteration:    885    step:      885     combined loss: 4582.25854     paf loss 21.52612     hm loss 4560.73242\n",
      "Iteration:    890    step:      890     combined loss: 3280.30930     paf loss 23.68442     hm loss 3256.62488\n",
      "Iteration:    895    step:      895     combined loss: 5240.86845     paf loss 24.31010     hm loss 5216.55835\n",
      "Iteration:    900    step:      900     combined loss: 3539.28464     paf loss 24.65427     hm loss 3514.63037\n",
      "Iteration:    905    step:      905     combined loss: 3918.02361     paf loss 31.70147     hm loss 3886.32214\n",
      "Iteration:    910    step:      910     combined loss: 3783.02784     paf loss 16.01172     hm loss 3767.01611\n",
      "Iteration:    915    step:      915     combined loss: 4290.61207     paf loss 23.98976     hm loss 4266.62231\n",
      "Iteration:    920    step:      920     combined loss: 5941.62086     paf loss 29.32130     hm loss 5912.29956\n",
      "Iteration:    925    step:      925     combined loss: 5155.87469     paf loss 21.24163     hm loss 5134.63306\n",
      "Iteration:    930    step:      930     combined loss: 3643.39136     paf loss 23.95069     hm loss 3619.44067\n",
      "Iteration:    935    step:      935     combined loss: 3567.75221     paf loss 17.48195     hm loss 3550.27026\n",
      "Iteration:    940    step:      940     combined loss: 4310.31461     paf loss 23.31632     hm loss 4286.99829\n",
      "Iteration:    945    step:      945     combined loss: 5168.05973     paf loss 19.53214     hm loss 5148.52759\n",
      "Iteration:    950    step:      950     combined loss: 3706.39646     paf loss 18.01169     hm loss 3688.38477\n",
      "Iteration:    955    step:      955     combined loss: 4590.86369     paf loss 30.16740     hm loss 4560.69629\n",
      "Iteration:    960    step:      960     combined loss: 3534.80733     paf loss 18.14851     hm loss 3516.65881\n",
      "Iteration:    965    step:      965     combined loss: 3992.89719     paf loss 20.21139     hm loss 3972.68579\n",
      "Iteration:    970    step:      970     combined loss: 4256.78430     paf loss 21.73181     hm loss 4235.05249\n",
      "Iteration:    975    step:      975     combined loss: 2623.08868     paf loss 12.48150     hm loss 2610.60718\n",
      "Iteration:    980    step:      980     combined loss: 3431.44525     paf loss 18.80353     hm loss 3412.64172\n",
      "Iteration:    985    step:      985     combined loss: 3647.92000     paf loss 18.89876     hm loss 3629.02124\n",
      "Iteration:    990    step:      990     combined loss: 4583.28926     paf loss 16.29634     hm loss 4566.99292\n",
      "Iteration:    995    step:      995     combined loss: 2791.94980     paf loss 20.52475     hm loss 2771.42505\n",
      "Iteration:   1000    step:     1000     combined loss: 6017.80838     paf loss 24.84622     hm loss 5992.96216\n",
      "Iteration:   1005    step:     1005     combined loss: 4663.00111     paf loss 17.49232     hm loss 4645.50879\n",
      "Iteration:   1010    step:     1010     combined loss: 3792.02619     paf loss 18.29938     hm loss 3773.72681\n",
      "Iteration:   1015    step:     1015     combined loss: 4591.96225     paf loss 22.89438     hm loss 4569.06787\n",
      "Iteration:   1020    step:     1020     combined loss: 4604.74014     paf loss 20.45474     hm loss 4584.28540\n",
      "Iteration:   1025    step:     1025     combined loss: 3677.86883     paf loss 25.46441     hm loss 3652.40442\n",
      "Iteration:   1030    step:     1030     combined loss: 4304.50471     paf loss 19.20856     hm loss 4285.29614\n",
      "Iteration:   1035    step:     1035     combined loss: 4131.06839     paf loss 17.38723     hm loss 4113.68115\n",
      "Iteration:   1040    step:     1040     combined loss: 4045.28562     paf loss 16.56833     hm loss 4028.71729\n",
      "Iteration:   1045    step:     1045     combined loss: 4806.82442     paf loss 21.97432     hm loss 4784.85010\n",
      "Iteration:   1050    step:     1050     combined loss: 4920.94284     paf loss 16.76462     hm loss 4904.17822\n",
      "Iteration:   1055    step:     1055     combined loss: 2671.42139     paf loss 16.98402     hm loss 2654.43738\n",
      "Iteration:   1060    step:     1060     combined loss: 4545.02826     paf loss 21.67768     hm loss 4523.35059\n",
      "Iteration:   1065    step:     1065     combined loss: 4715.06215     paf loss 22.41005     hm loss 4692.65210\n",
      "Iteration:   1070    step:     1070     combined loss: 4139.69282     paf loss 19.86591     hm loss 4119.82690\n",
      "Iteration:   1075    step:     1075     combined loss: 7547.30942     paf loss 25.68051     hm loss 7521.62891\n",
      "Iteration:   1080    step:     1080     combined loss: 3160.06454     paf loss 20.21322     hm loss 3139.85132\n",
      "Iteration:   1085    step:     1085     combined loss: 3760.80992     paf loss 20.64098     hm loss 3740.16895\n",
      "Iteration:   1090    step:     1090     combined loss: 2830.59775     paf loss 16.93381     hm loss 2813.66394\n",
      "Iteration:   1095    step:     1095     combined loss: 3903.32024     paf loss 24.73442     hm loss 3878.58582\n",
      "Iteration:   1100    step:     1100     combined loss: 2808.74570     paf loss 16.00229     hm loss 2792.74341\n",
      "Iteration:   1105    step:     1105     combined loss: 4574.09387     paf loss 15.67786     hm loss 4558.41602\n",
      "Iteration:   1110    step:     1110     combined loss: 4156.34809     paf loss 21.50532     hm loss 4134.84277\n",
      "Iteration:   1115    step:     1115     combined loss: 4479.67903     paf loss 22.70247     hm loss 4456.97656\n",
      "Iteration:   1120    step:     1120     combined loss: 4636.39922     paf loss 27.34746     hm loss 4609.05176\n",
      "Iteration:   1125    step:     1125     combined loss: 3528.81978     paf loss 20.86507     hm loss 3507.95471\n",
      "Iteration:   1130    step:     1130     combined loss: 4607.72390     paf loss 23.50173     hm loss 4584.22217\n",
      "Iteration:   1135    step:     1135     combined loss: 3580.50777     paf loss 15.92525     hm loss 3564.58252\n",
      "Iteration:   1140    step:     1140     combined loss: 2538.90073     paf loss 13.46164     hm loss 2525.43909\n",
      "Iteration:   1145    step:     1145     combined loss: 2975.12247     paf loss 20.23355     hm loss 2954.88892\n",
      "Iteration:   1150    step:     1150     combined loss: 5104.45213     paf loss 19.49705     hm loss 5084.95508\n",
      "Iteration:   1155    step:     1155     combined loss: 5283.02473     paf loss 27.20271     hm loss 5255.82202\n",
      "Iteration:   1160    step:     1160     combined loss: 4882.49163     paf loss 22.96551     hm loss 4859.52612\n",
      "Iteration:   1165    step:     1165     combined loss: 4123.21534     paf loss 26.50306     hm loss 4096.71228\n",
      "Iteration:   1170    step:     1170     combined loss: 4643.00755     paf loss 19.93065     hm loss 4623.07690\n",
      "Iteration:   1175    step:     1175     combined loss: 7617.09278     paf loss 22.45216     hm loss 7594.64062\n",
      "Iteration:   1180    step:     1180     combined loss: 3766.25766     paf loss 14.03854     hm loss 3752.21912\n",
      "Iteration:   1185    step:     1185     combined loss: 3135.38178     paf loss 21.74433     hm loss 3113.63745\n",
      "Iteration:   1190    step:     1190     combined loss: 4138.34515     paf loss 25.51922     hm loss 4112.82593\n",
      "Iteration:   1195    step:     1195     combined loss: 3299.04370     paf loss 18.82751     hm loss 3280.21619\n",
      "Iteration:   1200    step:     1200     combined loss: 3950.94368     paf loss 22.35738     hm loss 3928.58630\n",
      "Iteration:   1205    step:     1205     combined loss: 3397.42326     paf loss 16.47062     hm loss 3380.95264\n",
      "Iteration:   1210    step:     1210     combined loss: 2911.76508     paf loss 16.98676     hm loss 2894.77832\n",
      "Iteration:   1215    step:     1215     combined loss: 5179.67418     paf loss 26.43688     hm loss 5153.23730\n",
      "Iteration:   1220    step:     1220     combined loss: 4615.71300     paf loss 23.91588     hm loss 4591.79712\n",
      "Iteration:   1225    step:     1225     combined loss: 4254.88000     paf loss 19.39245     hm loss 4235.48755\n",
      "Iteration:   1230    step:     1230     combined loss: 6485.28574     paf loss 29.83750     hm loss 6455.44824\n",
      "Iteration:   1235    step:     1235     combined loss: 5887.87660     paf loss 20.81923     hm loss 5867.05737\n",
      "Iteration:   1240    step:     1240     combined loss: 5113.37793     paf loss 25.92480     hm loss 5087.45312\n",
      "Iteration:   1245    step:     1245     combined loss: 2194.22832     paf loss 17.54961     hm loss 2176.67871\n",
      "Iteration:   1250    step:     1250     combined loss: 6720.45789     paf loss 30.51038     hm loss 6689.94751\n",
      "Iteration:   1255    step:     1255     combined loss: 4896.20447     paf loss 15.59143     hm loss 4880.61304\n",
      "Iteration:   1260    step:     1260     combined loss: 3696.02212     paf loss 16.44302     hm loss 3679.57910\n",
      "Iteration:   1265    step:     1265     combined loss: 5988.20048     paf loss 26.98930     hm loss 5961.21118\n",
      "Iteration:   1270    step:     1270     combined loss: 3924.84791     paf loss 21.69227     hm loss 3903.15564\n",
      "Iteration:   1275    step:     1275     combined loss: 4043.63126     paf loss 25.40812     hm loss 4018.22314\n",
      "Iteration:   1280    step:     1280     combined loss: 4735.37969     paf loss 15.22808     hm loss 4720.15161\n",
      "Iteration:   1285    step:     1285     combined loss: 3099.27369     paf loss 18.10560     hm loss 3081.16809\n",
      "Iteration:   1290    step:     1290     combined loss: 3250.01200     paf loss 18.10441     hm loss 3231.90759\n",
      "Iteration:   1295    step:     1295     combined loss: 6170.53303     paf loss 22.27985     hm loss 6148.25317\n",
      "Iteration:   1300    step:     1300     combined loss: 4193.31598     paf loss 16.35920     hm loss 4176.95679\n",
      "Iteration:   1305    step:     1305     combined loss: 3299.00214     paf loss 18.62787     hm loss 3280.37427\n",
      "Iteration:   1310    step:     1310     combined loss: 3680.54837     paf loss 20.74759     hm loss 3659.80078\n",
      "Iteration:   1315    step:     1315     combined loss: 4646.15822     paf loss 25.25661     hm loss 4620.90161\n",
      "Iteration:   1320    step:     1320     combined loss: 4279.35010     paf loss 20.73755     hm loss 4258.61255\n",
      "Iteration:   1325    step:     1325     combined loss: 2798.65535     paf loss 23.68990     hm loss 2774.96545\n",
      "Iteration:   1330    step:     1330     combined loss: 7609.89829     paf loss 23.83652     hm loss 7586.06177\n",
      "Iteration:   1335    step:     1335     combined loss: 3658.42788     paf loss 20.07486     hm loss 3638.35303\n",
      "Iteration:   1340    step:     1340     combined loss: 6651.58068     paf loss 21.87072     hm loss 6629.70996\n",
      "Iteration:   1345    step:     1345     combined loss: 6067.73734     paf loss 25.98660     hm loss 6041.75073\n",
      "Iteration:   1350    step:     1350     combined loss: 3164.39934     paf loss 21.62334     hm loss 3142.77600\n",
      "Iteration:   1355    step:     1355     combined loss: 3663.75512     paf loss 18.80859     hm loss 3644.94653\n",
      "Iteration:   1360    step:     1360     combined loss: 2815.16596     paf loss 15.51642     hm loss 2799.64954\n",
      "Iteration:   1365    step:     1365     combined loss: 3900.84990     paf loss 24.04374     hm loss 3876.80615\n",
      "Iteration:   1370    step:     1370     combined loss: 2946.22840     paf loss 12.67981     hm loss 2933.54858\n",
      "Iteration:   1375    step:     1375     combined loss: 4787.82230     paf loss 20.11478     hm loss 4767.70752\n",
      "Iteration:   1380    step:     1380     combined loss: 3183.31778     paf loss 17.81265     hm loss 3165.50513\n",
      "Iteration:   1385    step:     1385     combined loss: 2719.62534     paf loss 17.71262     hm loss 2701.91272\n",
      "Iteration:   1390    step:     1390     combined loss: 3316.42154     paf loss 18.32096     hm loss 3298.10059\n",
      "Iteration:   1395    step:     1395     combined loss: 3948.39659     paf loss 19.72288     hm loss 3928.67371\n",
      "Iteration:   1400    step:     1400     combined loss: 4309.30558     paf loss 22.21085     hm loss 4287.09473\n",
      "Iteration:   1405    step:     1405     combined loss: 5426.09472     paf loss 27.02417     hm loss 5399.07056\n",
      "Iteration:   1410    step:     1410     combined loss: 6033.89751     paf loss 24.11455     hm loss 6009.78296\n",
      "Iteration:   1415    step:     1415     combined loss: 3630.95759     paf loss 20.78803     hm loss 3610.16956\n",
      "Iteration:   1420    step:     1420     combined loss: 2621.69593     paf loss 16.71888     hm loss 2604.97705\n",
      "Iteration:   1425    step:     1425     combined loss: 3911.13821     paf loss 17.42886     hm loss 3893.70935\n",
      "Iteration:   1430    step:     1430     combined loss: 4495.10724     paf loss 21.27375     hm loss 4473.83350\n",
      "Iteration:   1435    step:     1435     combined loss: 2809.74848     paf loss 12.47028     hm loss 2797.27820\n",
      "Iteration:   1440    step:     1440     combined loss: 7160.06421     paf loss 20.87256     hm loss 7139.19165\n",
      "Iteration:   1445    step:     1445     combined loss: 4084.10414     paf loss 19.86476     hm loss 4064.23938\n",
      "Iteration:   1450    step:     1450     combined loss: 2865.95295     paf loss 19.69282     hm loss 2846.26013\n",
      "Iteration:   1455    step:     1455     combined loss: 3998.46653     paf loss 19.13071     hm loss 3979.33582\n",
      "Iteration:   1460    step:     1460     combined loss: 5331.32563     paf loss 16.61615     hm loss 5314.70947\n",
      "Iteration:   1465    step:     1465     combined loss: 3394.23220     paf loss 20.03469     hm loss 3374.19751\n",
      "Iteration:   1470    step:     1470     combined loss: 2302.38732     paf loss 16.32274     hm loss 2286.06458\n",
      "Iteration:   1475    step:     1475     combined loss: 6386.94402     paf loss 21.73064     hm loss 6365.21338\n",
      "Iteration:   1480    step:     1480     combined loss: 5539.49271     paf loss 19.67679     hm loss 5519.81592\n",
      "Iteration:   1485    step:     1485     combined loss: 3648.05012     paf loss 18.45966     hm loss 3629.59045\n",
      "Iteration:   1490    step:     1490     combined loss: 3990.55773     paf loss 25.88146     hm loss 3964.67627\n",
      "Iteration:   1495    step:     1495     combined loss: 4451.29103     paf loss 20.54176     hm loss 4430.74927\n",
      "Iteration:   1500    step:     1500     combined loss: 3443.80682     paf loss 26.64471     hm loss 3417.16211\n",
      "Iteration:   1505    step:     1505     combined loss: 5501.77727     paf loss 22.13762     hm loss 5479.63965\n",
      "Iteration:   1510    step:     1510     combined loss: 3808.24952     paf loss 20.04334     hm loss 3788.20618\n",
      "Iteration:   1515    step:     1515     combined loss: 3267.55547     paf loss 13.86504     hm loss 3253.69043\n",
      "Iteration:   1520    step:     1520     combined loss: 3175.20438     paf loss 18.92569     hm loss 3156.27869\n",
      "Iteration:   1525    step:     1525     combined loss: 5392.78166     paf loss 23.66520     hm loss 5369.11646\n",
      "Iteration:   1530    step:     1530     combined loss: 3730.66919     paf loss 21.33252     hm loss 3709.33667\n",
      "Iteration:   1535    step:     1535     combined loss: 2999.29890     paf loss 20.82893     hm loss 2978.46997\n",
      "Iteration:   1540    step:     1540     combined loss: 4010.75186     paf loss 24.41593     hm loss 3986.33594\n",
      "Iteration:   1545    step:     1545     combined loss: 4850.86123     paf loss 15.49405     hm loss 4835.36719\n",
      "Iteration:   1550    step:     1550     combined loss: 4077.05959     paf loss 20.04849     hm loss 4057.01111\n",
      "Iteration:   1555    step:     1555     combined loss: 2864.62434     paf loss 20.50276     hm loss 2844.12158\n",
      "Iteration:   1560    step:     1560     combined loss: 7400.70169     paf loss 21.72831     hm loss 7378.97339\n",
      "Iteration:   1565    step:     1565     combined loss: 4868.42529     paf loss 18.41065     hm loss 4850.01465\n",
      "Iteration:   1570    step:     1570     combined loss: 5408.94858     paf loss 23.19101     hm loss 5385.75757\n",
      "Iteration:   1575    step:     1575     combined loss: 2715.31313     paf loss 15.94301     hm loss 2699.37012\n",
      "Iteration:   1580    step:     1580     combined loss: 3392.92166     paf loss 18.37064     hm loss 3374.55103\n",
      "Iteration:   1585    step:     1585     combined loss: 5527.02727     paf loss 19.47893     hm loss 5507.54834\n",
      "Iteration:   1590    step:     1590     combined loss: 3460.18547     paf loss 15.96134     hm loss 3444.22412\n",
      "Iteration:   1595    step:     1595     combined loss: 3344.05274     paf loss 18.57887     hm loss 3325.47388\n",
      "Iteration:   1600    step:     1600     combined loss: 5491.18196     paf loss 21.78743     hm loss 5469.39453\n",
      "Iteration:   1605    step:     1605     combined loss: 5093.11384     paf loss 23.07844     hm loss 5070.03540\n",
      "Iteration:   1610    step:     1610     combined loss: 4229.34749     paf loss 29.57380     hm loss 4199.77368\n",
      "Iteration:   1615    step:     1615     combined loss: 4133.70677     paf loss 23.91820     hm loss 4109.78857\n",
      "Iteration:   1620    step:     1620     combined loss: 3582.51099     paf loss 15.40173     hm loss 3567.10925\n",
      "Iteration:   1625    step:     1625     combined loss: 5145.05804     paf loss 29.52264     hm loss 5115.53540\n",
      "Iteration:   1630    step:     1630     combined loss: 4780.50382     paf loss 21.63810     hm loss 4758.86572\n",
      "Iteration:   1635    step:     1635     combined loss: 8788.79159     paf loss 18.24911     hm loss 8770.54248\n",
      "Iteration:   1640    step:     1640     combined loss: 6137.97273     paf loss 20.11384     hm loss 6117.85889\n",
      "Iteration:   1645    step:     1645     combined loss: 4467.26219     paf loss 18.53440     hm loss 4448.72778\n",
      "Iteration:   1650    step:     1650     combined loss: 4317.33918     paf loss 14.07063     hm loss 4303.26855\n",
      "Iteration:   1655    step:     1655     combined loss: 4888.58200     paf loss 18.64792     hm loss 4869.93408\n",
      "Iteration:   1660    step:     1660     combined loss: 4395.34376     paf loss 22.23902     hm loss 4373.10474\n",
      "Iteration:   1665    step:     1665     combined loss: 2889.96568     paf loss 13.16050     hm loss 2876.80518\n",
      "Iteration:   1670    step:     1670     combined loss: 3730.07354     paf loss 21.30975     hm loss 3708.76379\n",
      "Iteration:   1675    step:     1675     combined loss: 5963.32394     paf loss 18.65352     hm loss 5944.67041\n",
      "Iteration:   1680    step:     1680     combined loss: 2810.84985     paf loss 15.52905     hm loss 2795.32080\n",
      "Iteration:   1685    step:     1685     combined loss: 2390.46906     paf loss 16.73200     hm loss 2373.73706\n",
      "Iteration:   1690    step:     1690     combined loss: 5251.77320     paf loss 22.12183     hm loss 5229.65137\n",
      "Iteration:   1695    step:     1695     combined loss: 6303.32654     paf loss 25.87146     hm loss 6277.45508\n",
      "Iteration:   1700    step:     1700     combined loss: 3739.96774     paf loss 13.49313     hm loss 3726.47461\n",
      "Iteration:   1705    step:     1705     combined loss: 4002.40559     paf loss 19.58771     hm loss 3982.81787\n",
      "Iteration:   1710    step:     1710     combined loss: 5643.27404     paf loss 24.30749     hm loss 5618.96655\n",
      "Iteration:   1715    step:     1715     combined loss: 4258.88688     paf loss 17.41007     hm loss 4241.47681\n",
      "Iteration:   1720    step:     1720     combined loss: 4367.90560     paf loss 17.14071     hm loss 4350.76489\n",
      "Iteration:   1725    step:     1725     combined loss: 3740.42190     paf loss 17.15798     hm loss 3723.26392\n",
      "Iteration:   1730    step:     1730     combined loss: 3817.31265     paf loss 20.05716     hm loss 3797.25549\n",
      "Iteration:   1735    step:     1735     combined loss: 4713.76347     paf loss 26.36333     hm loss 4687.40015\n",
      "Iteration:   1740    step:     1740     combined loss: 3821.12800     paf loss 21.59761     hm loss 3799.53040\n",
      "Iteration:   1745    step:     1745     combined loss: 5341.97085     paf loss 21.51504     hm loss 5320.45581\n",
      "Iteration:   1750    step:     1750     combined loss: 3162.50128     paf loss 16.05316     hm loss 3146.44812\n",
      "Iteration:   1755    step:     1755     combined loss: 3494.55934     paf loss 14.44874     hm loss 3480.11060\n",
      "Iteration:   1760    step:     1760     combined loss: 5954.20611     paf loss 27.17583     hm loss 5927.03027\n",
      "Iteration:   1765    step:     1765     combined loss: 5050.36839     paf loss 19.67966     hm loss 5030.68872\n",
      "Iteration:   1770    step:     1770     combined loss: 3404.41242     paf loss 14.92548     hm loss 3389.48694\n",
      "Iteration:   1775    step:     1775     combined loss: 2950.75629     paf loss 15.35138     hm loss 2935.40491\n",
      "Iteration:   1780    step:     1780     combined loss: 2550.78544     paf loss 18.25126     hm loss 2532.53418\n",
      "Iteration:   1785    step:     1785     combined loss: 5932.50265     paf loss 23.29147     hm loss 5909.21118\n",
      "Iteration:   1790    step:     1790     combined loss: 6413.14725     paf loss 24.22928     hm loss 6388.91797\n",
      "Iteration:   1795    step:     1795     combined loss: 3269.92341     paf loss 14.76143     hm loss 3255.16199\n",
      "Iteration:   1800    step:     1800     combined loss: 2641.25035     paf loss 20.97203     hm loss 2620.27832\n",
      "Iteration:   1805    step:     1805     combined loss: 5084.70043     paf loss 18.31713     hm loss 5066.38330\n",
      "Iteration:   1810    step:     1810     combined loss: 2363.18858     paf loss 14.86326     hm loss 2348.32532\n",
      "Iteration:   1815    step:     1815     combined loss: 4040.84751     paf loss 19.51340     hm loss 4021.33411\n",
      "Iteration:   1820    step:     1820     combined loss: 3003.72546     paf loss 20.61133     hm loss 2983.11414\n",
      "Iteration:   1825    step:     1825     combined loss: 5068.65591     paf loss 20.62246     hm loss 5048.03345\n",
      "Iteration:   1830    step:     1830     combined loss: 5082.62570     paf loss 17.36960     hm loss 5065.25610\n",
      "Iteration:   1835    step:     1835     combined loss: 3138.36307     paf loss 23.76517     hm loss 3114.59790\n",
      "Iteration:   1840    step:     1840     combined loss: 5776.71084     paf loss 16.44155     hm loss 5760.26929\n",
      "Iteration:   1845    step:     1845     combined loss: 4598.18897     paf loss 23.37964     hm loss 4574.80933\n",
      "Iteration:   1850    step:     1850     combined loss: 3816.52192     paf loss 16.44917     hm loss 3800.07275\n",
      "Iteration:   1855    step:     1855     combined loss: 3373.46333     paf loss 18.18122     hm loss 3355.28210\n",
      "Iteration:   1860    step:     1860     combined loss: 5014.15830     paf loss 18.97178     hm loss 4995.18652\n",
      "Iteration:   1865    step:     1865     combined loss: 4269.91134     paf loss 23.79294     hm loss 4246.11841\n",
      "Iteration:   1870    step:     1870     combined loss: 2681.35080     paf loss 14.07297     hm loss 2667.27783\n",
      "Iteration:   1875    step:     1875     combined loss: 3528.10959     paf loss 12.52560     hm loss 3515.58398\n",
      "Iteration:   1880    step:     1880     combined loss: 4479.33285     paf loss 23.46298     hm loss 4455.86987\n",
      "Iteration:   1885    step:     1885     combined loss: 5937.52978     paf loss 23.77148     hm loss 5913.75830\n",
      "Iteration:   1890    step:     1890     combined loss: 4351.97450     paf loss 19.27968     hm loss 4332.69482\n",
      "Iteration:   1895    step:     1895     combined loss: 4408.67302     paf loss 21.40593     hm loss 4387.26709\n",
      "Iteration:   1900    step:     1900     combined loss: 4738.50785     paf loss 23.94730     hm loss 4714.56055\n",
      "Iteration:   1905    step:     1905     combined loss: 5094.91578     paf loss 20.09937     hm loss 5074.81641\n",
      "Iteration:   1910    step:     1910     combined loss: 5290.80380     paf loss 21.26450     hm loss 5269.53931\n",
      "Iteration:   1915    step:     1915     combined loss: 3454.62976     paf loss 24.18603     hm loss 3430.44373\n",
      "Iteration:   1920    step:     1920     combined loss: 2874.91974     paf loss 17.95905     hm loss 2856.96069\n",
      "Iteration:   1925    step:     1925     combined loss: 2928.28193     paf loss 19.29536     hm loss 2908.98657\n",
      "Iteration:   1930    step:     1930     combined loss: 3979.30786     paf loss 17.13171     hm loss 3962.17615\n",
      "Iteration:   1935    step:     1935     combined loss: 2503.05585     paf loss 14.76447     hm loss 2488.29138\n",
      "Iteration:   1940    step:     1940     combined loss: 3556.19498     paf loss 19.40738     hm loss 3536.78760\n",
      "Iteration:   1945    step:     1945     combined loss: 7253.29923     paf loss 25.64323     hm loss 7227.65601\n",
      "Iteration:   1950    step:     1950     combined loss: 3743.69595     paf loss 20.25979     hm loss 3723.43616\n",
      "Iteration:   1955    step:     1955     combined loss: 5290.44262     paf loss 20.42284     hm loss 5270.01978\n",
      "Iteration:   1960    step:     1960     combined loss: 4002.38353     paf loss 20.34593     hm loss 3982.03760\n",
      "Iteration:   1965    step:     1965     combined loss: 3445.42071     paf loss 15.07305     hm loss 3430.34766\n",
      "Iteration:   1970    step:     1970     combined loss: 4781.26618     paf loss 19.20270     hm loss 4762.06348\n",
      "Iteration:   1975    step:     1975     combined loss: 4451.59734     paf loss 25.98064     hm loss 4425.61670\n",
      "Iteration:   1980    step:     1980     combined loss: 3044.45039     paf loss 22.35786     hm loss 3022.09253\n",
      "Iteration:   1985    step:     1985     combined loss: 4893.04071     paf loss 24.37348     hm loss 4868.66724\n",
      "Iteration:   1990    step:     1990     combined loss: 5061.84168     paf loss 22.39490     hm loss 5039.44678\n",
      "Iteration:   1995    step:     1995     combined loss: 4557.91722     paf loss 17.42112     hm loss 4540.49609\n",
      "VGG activated\n",
      "Iteration:   2000    step:     2000     combined loss: 4492.27763     paf loss 23.75175     hm loss 4468.52588\n",
      "Iteration:   2005    step:     2005     combined loss: 5622.39785     paf loss 23.44643     hm loss 5598.95142\n",
      "Iteration:   2010    step:     2010     combined loss: 3576.16393     paf loss 21.49560     hm loss 3554.66833\n",
      "Iteration:   2015    step:     2015     combined loss: 3968.80208     paf loss 23.23091     hm loss 3945.57117\n",
      "Iteration:   2020    step:     2020     combined loss: 5480.28817     paf loss 24.32699     hm loss 5455.96118\n",
      "Iteration:   2025    step:     2025     combined loss: 2677.80012     paf loss 13.51875     hm loss 2664.28137\n",
      "Iteration:   2030    step:     2030     combined loss: 3626.15646     paf loss 27.87057     hm loss 3598.28589\n",
      "Iteration:   2035    step:     2035     combined loss: 3511.28463     paf loss 16.10348     hm loss 3495.18115\n",
      "Iteration:   2040    step:     2040     combined loss: 5771.35273     paf loss 18.80171     hm loss 5752.55103\n",
      "Iteration:   2045    step:     2045     combined loss: 3073.21562     paf loss 16.69132     hm loss 3056.52429\n",
      "Iteration:   2050    step:     2050     combined loss: 6957.05250     paf loss 31.12452     hm loss 6925.92798\n",
      "Iteration:   2055    step:     2055     combined loss: 3489.29191     paf loss 21.65983     hm loss 3467.63208\n",
      "Iteration:   2060    step:     2060     combined loss: 4975.52181     paf loss 26.06722     hm loss 4949.45459\n",
      "Iteration:   2065    step:     2065     combined loss: 6466.11606     paf loss 19.18882     hm loss 6446.92725\n",
      "Iteration:   2070    step:     2070     combined loss: 3731.55221     paf loss 20.07638     hm loss 3711.47583\n",
      "Iteration:   2075    step:     2075     combined loss: 3304.83779     paf loss 19.31497     hm loss 3285.52283\n",
      "Iteration:   2080    step:     2080     combined loss: 3636.08135     paf loss 17.50139     hm loss 3618.57996\n",
      "Iteration:   2085    step:     2085     combined loss: 4428.55749     paf loss 20.24963     hm loss 4408.30786\n",
      "Iteration:   2090    step:     2090     combined loss: 4674.04485     paf loss 28.69500     hm loss 4645.34985\n",
      "Iteration:   2095    step:     2095     combined loss: 3838.07911     paf loss 21.15992     hm loss 3816.91919\n",
      "Iteration:   2100    step:     2100     combined loss: 4194.28720     paf loss 18.57577     hm loss 4175.71143\n",
      "Iteration:   2105    step:     2105     combined loss: 3531.06801     paf loss 17.62953     hm loss 3513.43848\n",
      "Iteration:   2110    step:     2110     combined loss: 2671.64881     paf loss 19.17713     hm loss 2652.47168\n",
      "Iteration:   2115    step:     2115     combined loss: 4973.46510     paf loss 24.75587     hm loss 4948.70923\n",
      "Iteration:   2120    step:     2120     combined loss: 3787.13847     paf loss 16.53093     hm loss 3770.60754\n",
      "Iteration:   2125    step:     2125     combined loss: 4651.80901     paf loss 23.25530     hm loss 4628.55371\n",
      "Iteration:   2130    step:     2130     combined loss: 5875.62070     paf loss 23.74814     hm loss 5851.87256\n",
      "Iteration:   2135    step:     2135     combined loss: 6034.84661     paf loss 26.73284     hm loss 6008.11377\n",
      "Iteration:   2140    step:     2140     combined loss: 5088.00129     paf loss 24.88630     hm loss 5063.11499\n",
      "Iteration:   2145    step:     2145     combined loss: 3484.59416     paf loss 21.65385     hm loss 3462.94031\n",
      "Iteration:   2150    step:     2150     combined loss: 4811.28340     paf loss 19.83589     hm loss 4791.44751\n",
      "Iteration:   2155    step:     2155     combined loss: 6036.51107     paf loss 20.64315     hm loss 6015.86792\n",
      "Iteration:   2160    step:     2160     combined loss: 4713.63983     paf loss 21.92596     hm loss 4691.71387\n",
      "Iteration:   2165    step:     2165     combined loss: 5130.57319     paf loss 23.61127     hm loss 5106.96191\n",
      "Iteration:   2170    step:     2170     combined loss: 2481.77391     paf loss 19.81456     hm loss 2461.95935\n",
      "Iteration:   2175    step:     2175     combined loss: 3977.21200     paf loss 17.49899     hm loss 3959.71301\n",
      "Iteration:   2180    step:     2180     combined loss: 3852.42767     paf loss 22.28436     hm loss 3830.14331\n",
      "Iteration:   2185    step:     2185     combined loss: 2842.04722     paf loss 17.89927     hm loss 2824.14795\n",
      "Iteration:   2190    step:     2190     combined loss: 3087.92208     paf loss 17.01729     hm loss 3070.90479\n",
      "Iteration:   2195    step:     2195     combined loss: 2500.58167     paf loss 16.34754     hm loss 2484.23413\n",
      "Iteration:   2200    step:     2200     combined loss: 4769.85707     paf loss 14.39394     hm loss 4755.46313\n",
      "Iteration:   2205    step:     2205     combined loss: 5175.63518     paf loss 21.13103     hm loss 5154.50415\n",
      "Iteration:   2210    step:     2210     combined loss: 2915.61107     paf loss 15.16075     hm loss 2900.45032\n",
      "Iteration:   2215    step:     2215     combined loss: 5072.99442     paf loss 24.11259     hm loss 5048.88184\n",
      "Iteration:   2220    step:     2220     combined loss: 4934.47634     paf loss 21.73244     hm loss 4912.74390\n",
      "Iteration:   2225    step:     2225     combined loss: 4387.66691     paf loss 21.94987     hm loss 4365.71704\n",
      "Iteration:   2230    step:     2230     combined loss: 4293.68632     paf loss 28.72709     hm loss 4264.95923\n",
      "Iteration:   2235    step:     2235     combined loss: 4828.09395     paf loss 19.51900     hm loss 4808.57495\n",
      "Iteration:   2240    step:     2240     combined loss: 4418.63422     paf loss 29.55414     hm loss 4389.08008\n",
      "Iteration:   2245    step:     2245     combined loss: 3502.82652     paf loss 20.50633     hm loss 3482.32019\n",
      "Iteration:   2250    step:     2250     combined loss: 4888.12653     paf loss 21.01496     hm loss 4867.11157\n",
      "Iteration:   2255    step:     2255     combined loss: 4793.08262     paf loss 17.52573     hm loss 4775.55688\n",
      "Iteration:   2260    step:     2260     combined loss: 4982.95591     paf loss 21.55088     hm loss 4961.40503\n",
      "Iteration:   2265    step:     2265     combined loss: 6219.86596     paf loss 21.65185     hm loss 6198.21411\n",
      "Iteration:   2270    step:     2270     combined loss: 4244.68883     paf loss 25.84923     hm loss 4218.83960\n",
      "Iteration:   2275    step:     2275     combined loss: 4548.37826     paf loss 18.42831     hm loss 4529.94995\n",
      "Iteration:   2280    step:     2280     combined loss: 7325.70118     paf loss 20.04663     hm loss 7305.65454\n",
      "Iteration:   2285    step:     2285     combined loss: 3673.20754     paf loss 24.34951     hm loss 3648.85803\n",
      "Iteration:   2290    step:     2290     combined loss: 3632.68317     paf loss 19.34223     hm loss 3613.34094\n",
      "Iteration:   2295    step:     2295     combined loss: 3919.45856     paf loss 16.78840     hm loss 3902.67017\n",
      "Iteration:   2300    step:     2300     combined loss: 5016.41546     paf loss 23.20086     hm loss 4993.21460\n",
      "Iteration:   2305    step:     2305     combined loss: 3180.80535     paf loss 17.96978     hm loss 3162.83557\n",
      "Iteration:   2310    step:     2310     combined loss: 3790.19900     paf loss 21.07339     hm loss 3769.12561\n",
      "Iteration:   2315    step:     2315     combined loss: 3036.73245     paf loss 19.70962     hm loss 3017.02283\n",
      "Iteration:   2320    step:     2320     combined loss: 2428.45576     paf loss 18.69368     hm loss 2409.76208\n",
      "Iteration:   2325    step:     2325     combined loss: 5109.34286     paf loss 29.00912     hm loss 5080.33374\n",
      "Iteration:   2330    step:     2330     combined loss: 3333.36372     paf loss 21.66206     hm loss 3311.70166\n",
      "Iteration:   2335    step:     2335     combined loss: 4307.58341     paf loss 19.01749     hm loss 4288.56592\n",
      "Iteration:   2340    step:     2340     combined loss: 2788.62597     paf loss 18.30273     hm loss 2770.32324\n",
      "Iteration:   2345    step:     2345     combined loss: 5129.97791     paf loss 19.98450     hm loss 5109.99341\n",
      "Iteration:   2350    step:     2350     combined loss: 3531.22996     paf loss 21.37791     hm loss 3509.85205\n",
      "Iteration:   2355    step:     2355     combined loss: 2810.09695     paf loss 17.25076     hm loss 2792.84619\n",
      "Iteration:   2360    step:     2360     combined loss: 5875.48596     paf loss 25.92078     hm loss 5849.56519\n",
      "Iteration:   2365    step:     2365     combined loss: 3775.92016     paf loss 20.61950     hm loss 3755.30066\n",
      "Iteration:   2370    step:     2370     combined loss: 3012.27857     paf loss 17.64710     hm loss 2994.63147\n",
      "Iteration:   2375    step:     2375     combined loss: 4855.92645     paf loss 17.98602     hm loss 4837.94043\n",
      "Iteration:   2380    step:     2380     combined loss: 2348.69542     paf loss 14.54942     hm loss 2334.14600\n",
      "Iteration:   2385    step:     2385     combined loss: 3010.01187     paf loss 18.19033     hm loss 2991.82153\n",
      "Iteration:   2390    step:     2390     combined loss: 3193.55365     paf loss 18.66742     hm loss 3174.88623\n",
      "Iteration:   2395    step:     2395     combined loss: 2739.87877     paf loss 23.47239     hm loss 2716.40637\n",
      "Iteration:   2400    step:     2400     combined loss: 3376.95129     paf loss 12.19640     hm loss 3364.75488\n",
      "Iteration:   2405    step:     2405     combined loss: 4892.20064     paf loss 25.46236     hm loss 4866.73828\n",
      "Iteration:   2410    step:     2410     combined loss: 3013.11313     paf loss 24.72995     hm loss 2988.38318\n",
      "Iteration:   2415    step:     2415     combined loss: 3457.49065     paf loss 19.01507     hm loss 3438.47559\n",
      "Iteration:   2420    step:     2420     combined loss: 2815.57858     paf loss 13.33798     hm loss 2802.24060\n",
      "Iteration:   2425    step:     2425     combined loss: 5202.28472     paf loss 18.57280     hm loss 5183.71191\n",
      "Iteration:   2430    step:     2430     combined loss: 6657.11189     paf loss 23.99665     hm loss 6633.11523\n",
      "Iteration:   2435    step:     2435     combined loss: 3775.56056     paf loss 22.62660     hm loss 3752.93396\n",
      "Iteration:   2440    step:     2440     combined loss: 3873.50658     paf loss 20.23717     hm loss 3853.26941\n",
      "Iteration:   2445    step:     2445     combined loss: 3074.64508     paf loss 15.91498     hm loss 3058.73010\n",
      "Iteration:   2450    step:     2450     combined loss: 4897.11558     paf loss 14.43565     hm loss 4882.67993\n",
      "Iteration:   2455    step:     2455     combined loss: 5595.27397     paf loss 26.65874     hm loss 5568.61523\n",
      "Iteration:   2460    step:     2460     combined loss: 5195.39303     paf loss 24.40719     hm loss 5170.98584\n",
      "Iteration:   2465    step:     2465     combined loss: 4221.97044     paf loss 14.74388     hm loss 4207.22656\n",
      "Iteration:   2470    step:     2470     combined loss: 4848.07607     paf loss 22.97573     hm loss 4825.10034\n",
      "Iteration:   2475    step:     2475     combined loss: 3445.81727     paf loss 22.80983     hm loss 3423.00745\n",
      "Iteration:   2480    step:     2480     combined loss: 6495.57491     paf loss 21.99044     hm loss 6473.58447\n",
      "Iteration:   2485    step:     2485     combined loss: 4049.42916     paf loss 21.85055     hm loss 4027.57861\n",
      "Iteration:   2490    step:     2490     combined loss: 4349.14180     paf loss 20.46944     hm loss 4328.67236\n",
      "Iteration:   2495    step:     2495     combined loss: 4011.01566     paf loss 18.63712     hm loss 3992.37854\n",
      "Iteration:   2500    step:     2500     combined loss: 4121.82029     paf loss 16.05735     hm loss 4105.76294\n",
      "Iteration:   2505    step:     2505     combined loss: 3819.51597     paf loss 17.44040     hm loss 3802.07556\n",
      "Iteration:   2510    step:     2510     combined loss: 5039.76066     paf loss 25.13419     hm loss 5014.62646\n",
      "Iteration:   2515    step:     2515     combined loss: 4836.04512     paf loss 21.55659     hm loss 4814.48853\n",
      "Iteration:   2520    step:     2520     combined loss: 3234.40650     paf loss 18.39356     hm loss 3216.01294\n",
      "Iteration:   2525    step:     2525     combined loss: 3277.94448     paf loss 24.20266     hm loss 3253.74182\n",
      "Iteration:   2530    step:     2530     combined loss: 3755.64633     paf loss 14.71005     hm loss 3740.93628\n",
      "Iteration:   2535    step:     2535     combined loss: 3091.99220     paf loss 14.38905     hm loss 3077.60315\n",
      "Iteration:   2540    step:     2540     combined loss: 3841.46492     paf loss 21.05623     hm loss 3820.40869\n",
      "Iteration:   2545    step:     2545     combined loss: 3636.60902     paf loss 20.25526     hm loss 3616.35376\n",
      "Iteration:   2550    step:     2550     combined loss: 5781.50571     paf loss 20.49106     hm loss 5761.01465\n",
      "Iteration:   2555    step:     2555     combined loss: 4812.88464     paf loss 23.10095     hm loss 4789.78369\n",
      "Iteration:   2560    step:     2560     combined loss: 4446.32794     paf loss 17.05817     hm loss 4429.26978\n",
      "Iteration:   2565    step:     2565     combined loss: 5000.48211     paf loss 23.94305     hm loss 4976.53906\n",
      "Iteration:   2570    step:     2570     combined loss: 3676.82027     paf loss 13.83968     hm loss 3662.98059\n",
      "Iteration:   2575    step:     2575     combined loss: 2141.62040     paf loss 13.03214     hm loss 2128.58826\n",
      "Iteration:   2580    step:     2580     combined loss: 2874.25111     paf loss 19.24477     hm loss 2855.00635\n",
      "Iteration:   2585    step:     2585     combined loss: 4287.47248     paf loss 15.62043     hm loss 4271.85205\n",
      "Iteration:   2590    step:     2590     combined loss: 6020.05206     paf loss 23.19171     hm loss 5996.86035\n",
      "Iteration:   2595    step:     2595     combined loss: 4135.19853     paf loss 23.02226     hm loss 4112.17627\n",
      "Iteration:   2600    step:     2600     combined loss: 6404.63680     paf loss 29.15437     hm loss 6375.48242\n",
      "Iteration:   2605    step:     2605     combined loss: 3632.13466     paf loss 16.95106     hm loss 3615.18359\n",
      "Iteration:   2610    step:     2610     combined loss: 6598.29395     paf loss 21.20411     hm loss 6577.08984\n",
      "Iteration:   2615    step:     2615     combined loss: 3861.81547     paf loss 19.39396     hm loss 3842.42151\n",
      "Iteration:   2620    step:     2620     combined loss: 4158.89190     paf loss 18.67486     hm loss 4140.21704\n",
      "Iteration:   2625    step:     2625     combined loss: 2908.01050     paf loss 19.60950     hm loss 2888.40100\n",
      "Iteration:   2630    step:     2630     combined loss: 2996.64788     paf loss 16.62310     hm loss 2980.02478\n",
      "Iteration:   2635    step:     2635     combined loss: 4116.09805     paf loss 24.69314     hm loss 4091.40491\n",
      "learning rate change: 0.001 --> 0.0005\n",
      "Iteration:   2640    step:     2640     combined loss: 5417.72985     paf loss 22.39538     hm loss 5395.33447\n",
      "Iteration:   2645    step:     2645     combined loss: 3820.94704     paf loss 19.54702     hm loss 3801.40002\n",
      "Iteration:   2650    step:     2650     combined loss: 6411.57477     paf loss 27.58990     hm loss 6383.98486\n",
      "Iteration:   2655    step:     2655     combined loss: 3611.51119     paf loss 17.20455     hm loss 3594.30664\n",
      "Iteration:   2660    step:     2660     combined loss: 2846.05092     paf loss 19.08498     hm loss 2826.96594\n",
      "Iteration:   2665    step:     2665     combined loss: 2660.83480     paf loss 18.46358     hm loss 2642.37122\n",
      "Iteration:   2670    step:     2670     combined loss: 4724.64638     paf loss 22.89613     hm loss 4701.75024\n",
      "Iteration:   2675    step:     2675     combined loss: 3836.04503     paf loss 18.38951     hm loss 3817.65552\n",
      "Iteration:   2680    step:     2680     combined loss: 4281.83605     paf loss 20.07067     hm loss 4261.76538\n",
      "Iteration:   2685    step:     2685     combined loss: 5533.35709     paf loss 28.23429     hm loss 5505.12280\n",
      "Iteration:   2690    step:     2690     combined loss: 5042.43240     paf loss 22.88773     hm loss 5019.54468\n",
      "Iteration:   2695    step:     2695     combined loss: 2254.14454     paf loss 17.12940     hm loss 2237.01514\n",
      "Iteration:   2700    step:     2700     combined loss: 2993.61141     paf loss 20.64889     hm loss 2972.96252\n",
      "Iteration:   2705    step:     2705     combined loss: 3277.10449     paf loss 22.59973     hm loss 3254.50476\n",
      "Iteration:   2710    step:     2710     combined loss: 2603.69344     paf loss 17.81465     hm loss 2585.87878\n",
      "Iteration:   2715    step:     2715     combined loss: 3764.68589     paf loss 23.90024     hm loss 3740.78564\n",
      "Iteration:   2720    step:     2720     combined loss: 3498.60786     paf loss 19.88496     hm loss 3478.72290\n",
      "Iteration:   2725    step:     2725     combined loss: 3695.90005     paf loss 18.34841     hm loss 3677.55164\n",
      "Iteration:   2730    step:     2730     combined loss: 3070.47018     paf loss 21.41659     hm loss 3049.05359\n",
      "Iteration:   2735    step:     2735     combined loss: 2986.80534     paf loss 17.77201     hm loss 2969.03333\n",
      "Iteration:   2740    step:     2740     combined loss: 5596.00842     paf loss 17.80017     hm loss 5578.20825\n",
      "Iteration:   2745    step:     2745     combined loss: 4257.12306     paf loss 18.54444     hm loss 4238.57861\n",
      "Iteration:   2750    step:     2750     combined loss: 6778.35724     paf loss 22.38556     hm loss 6755.97168\n",
      "Iteration:   2755    step:     2755     combined loss: 5181.95509     paf loss 17.26222     hm loss 5164.69287\n",
      "Iteration:   2760    step:     2760     combined loss: 5977.60126     paf loss 19.32928     hm loss 5958.27197\n",
      "Iteration:   2765    step:     2765     combined loss: 3337.18644     paf loss 17.15482     hm loss 3320.03162\n",
      "Iteration:   2770    step:     2770     combined loss: 4314.61787     paf loss 17.47676     hm loss 4297.14111\n",
      "Iteration:   2775    step:     2775     combined loss: 4023.57923     paf loss 19.35352     hm loss 4004.22571\n",
      "Iteration:   2780    step:     2780     combined loss: 4556.94127     paf loss 25.54234     hm loss 4531.39893\n",
      "Iteration:   2785    step:     2785     combined loss: 5094.62632     paf loss 20.56186     hm loss 5074.06445\n",
      "Iteration:   2790    step:     2790     combined loss: 5804.45344     paf loss 23.57185     hm loss 5780.88159\n",
      "Iteration:   2795    step:     2795     combined loss: 2997.32537     paf loss 16.32977     hm loss 2980.99561\n",
      "Iteration:   2800    step:     2800     combined loss: 3313.11871     paf loss 17.08575     hm loss 3296.03296\n",
      "Iteration:   2805    step:     2805     combined loss: 3605.25878     paf loss 18.88609     hm loss 3586.37268\n",
      "Iteration:   2810    step:     2810     combined loss: 6441.45820     paf loss 25.49433     hm loss 6415.96387\n",
      "Iteration:   2815    step:     2815     combined loss: 4213.87878     paf loss 22.52477     hm loss 4191.35400\n",
      "Iteration:   2820    step:     2820     combined loss: 4246.57872     paf loss 24.78844     hm loss 4221.79028\n",
      "Train Loss: 4395.3678    PAF Loss:  28.0053    HM Loss:  4367.3624    Acc: NA\n",
      "Val Loss: 4816.3957    PAF Loss:  16.8420    HM Loss:  4799.5537     Acc: NA\n",
      "Epoch 1/9\n",
      "----------\n",
      "Iteration:      0    step:     2824     combined loss: 3695.26422     paf loss 23.45197     hm loss 3671.81226\n",
      "Iteration:      5    step:     2829     combined loss: 4579.34368     paf loss 26.45574     hm loss 4552.88794\n",
      "Iteration:     10    step:     2834     combined loss: 5617.44595     paf loss 21.00821     hm loss 5596.43774\n",
      "Iteration:     15    step:     2839     combined loss: 3901.45773     paf loss 21.68015     hm loss 3879.77759\n",
      "Iteration:     20    step:     2844     combined loss: 6445.37752     paf loss 21.86605     hm loss 6423.51147\n",
      "Iteration:     25    step:     2849     combined loss: 3043.90206     paf loss 22.84078     hm loss 3021.06128\n",
      "Iteration:     30    step:     2854     combined loss: 1992.97430     paf loss 14.58074     hm loss 1978.39355\n",
      "Iteration:     35    step:     2859     combined loss: 4109.38316     paf loss 17.51487     hm loss 4091.86829\n",
      "Iteration:     40    step:     2864     combined loss: 4271.08082     paf loss 20.25904     hm loss 4250.82178\n",
      "Iteration:     45    step:     2869     combined loss: 3787.64125     paf loss 18.73781     hm loss 3768.90344\n",
      "Iteration:     50    step:     2874     combined loss: 2695.71267     paf loss 17.80373     hm loss 2677.90894\n",
      "Iteration:     55    step:     2879     combined loss: 3823.26776     paf loss 19.49164     hm loss 3803.77612\n",
      "Iteration:     60    step:     2884     combined loss: 6917.04063     paf loss 23.32140     hm loss 6893.71924\n",
      "Iteration:     65    step:     2889     combined loss: 3129.95586     paf loss 16.80876     hm loss 3113.14709\n",
      "Iteration:     70    step:     2894     combined loss: 3693.13752     paf loss 17.98688     hm loss 3675.15063\n",
      "Iteration:     75    step:     2899     combined loss: 5882.41402     paf loss 22.13374     hm loss 5860.28027\n",
      "Iteration:     80    step:     2904     combined loss: 3970.23456     paf loss 22.33307     hm loss 3947.90149\n",
      "Iteration:     85    step:     2909     combined loss: 3714.15541     paf loss 17.88917     hm loss 3696.26624\n",
      "Iteration:     90    step:     2914     combined loss: 5162.00502     paf loss 24.03603     hm loss 5137.96899\n",
      "Iteration:     95    step:     2919     combined loss: 3939.05446     paf loss 17.25685     hm loss 3921.79761\n",
      "Iteration:    100    step:     2924     combined loss: 4198.28368     paf loss 19.93310     hm loss 4178.35059\n",
      "Iteration:    105    step:     2929     combined loss: 3355.69189     paf loss 21.40234     hm loss 3334.28955\n",
      "Iteration:    110    step:     2934     combined loss: 4950.46011     paf loss 19.26748     hm loss 4931.19263\n",
      "Iteration:    115    step:     2939     combined loss: 4167.05665     paf loss 24.14430     hm loss 4142.91235\n",
      "Iteration:    120    step:     2944     combined loss: 3779.16585     paf loss 18.81478     hm loss 3760.35107\n",
      "Iteration:    125    step:     2949     combined loss: 3035.72571     paf loss 13.91186     hm loss 3021.81384\n",
      "Iteration:    130    step:     2954     combined loss: 3605.33184     paf loss 20.64984     hm loss 3584.68201\n",
      "Iteration:    135    step:     2959     combined loss: 2306.05267     paf loss 14.06586     hm loss 2291.98682\n",
      "Iteration:    140    step:     2964     combined loss: 4898.12867     paf loss 17.57911     hm loss 4880.54956\n",
      "Iteration:    145    step:     2969     combined loss: 2863.54666     paf loss 16.87234     hm loss 2846.67432\n",
      "Iteration:    150    step:     2974     combined loss: 4372.68207     paf loss 18.94648     hm loss 4353.73560\n",
      "Iteration:    155    step:     2979     combined loss: 4164.57159     paf loss 21.51129     hm loss 4143.06030\n",
      "Iteration:    160    step:     2984     combined loss: 3388.31305     paf loss 20.70526     hm loss 3367.60779\n",
      "Iteration:    165    step:     2989     combined loss: 4582.66393     paf loss 22.47911     hm loss 4560.18481\n",
      "Iteration:    170    step:     2994     combined loss: 4639.23395     paf loss 18.95612     hm loss 4620.27783\n",
      "Iteration:    175    step:     2999     combined loss: 3411.22236     paf loss 15.86275     hm loss 3395.35962\n",
      "Iteration:    180    step:     3004     combined loss: 4229.67551     paf loss 25.56467     hm loss 4204.11084\n",
      "Iteration:    185    step:     3009     combined loss: 3085.53308     paf loss 14.00818     hm loss 3071.52490\n",
      "Iteration:    190    step:     3014     combined loss: 4411.36567     paf loss 18.51557     hm loss 4392.85010\n",
      "Iteration:    195    step:     3019     combined loss: 3620.42484     paf loss 19.12101     hm loss 3601.30383\n",
      "Iteration:    200    step:     3024     combined loss: 5000.21257     paf loss 20.19670     hm loss 4980.01587\n",
      "Iteration:    205    step:     3029     combined loss: 4945.29310     paf loss 22.65857     hm loss 4922.63452\n",
      "Iteration:    210    step:     3034     combined loss: 4480.06884     paf loss 22.35009     hm loss 4457.71875\n",
      "Iteration:    215    step:     3039     combined loss: 3290.52490     paf loss 18.10218     hm loss 3272.42273\n",
      "Iteration:    220    step:     3044     combined loss: 3750.84305     paf loss 18.94254     hm loss 3731.90051\n",
      "Iteration:    225    step:     3049     combined loss: 3263.95730     paf loss 23.98611     hm loss 3239.97119\n",
      "Iteration:    230    step:     3054     combined loss: 3953.18832     paf loss 23.42196     hm loss 3929.76636\n",
      "Iteration:    235    step:     3059     combined loss: 3015.81989     paf loss 16.93244     hm loss 2998.88745\n",
      "Iteration:    240    step:     3064     combined loss: 4227.88584     paf loss 20.43930     hm loss 4207.44653\n",
      "Iteration:    245    step:     3069     combined loss: 4733.58856     paf loss 22.16034     hm loss 4711.42822\n",
      "Iteration:    250    step:     3074     combined loss: 5462.66832     paf loss 21.05162     hm loss 5441.61670\n",
      "Iteration:    255    step:     3079     combined loss: 3291.41593     paf loss 19.47342     hm loss 3271.94250\n",
      "Iteration:    260    step:     3084     combined loss: 4219.39384     paf loss 24.95097     hm loss 4194.44287\n",
      "Iteration:    265    step:     3089     combined loss: 3502.37042     paf loss 20.88397     hm loss 3481.48645\n",
      "Iteration:    270    step:     3094     combined loss: 5799.75439     paf loss 19.97852     hm loss 5779.77588\n",
      "Iteration:    275    step:     3099     combined loss: 5622.80745     paf loss 20.92928     hm loss 5601.87817\n",
      "Iteration:    280    step:     3104     combined loss: 4781.56932     paf loss 27.57005     hm loss 4753.99927\n",
      "Iteration:    285    step:     3109     combined loss: 3757.63924     paf loss 29.23873     hm loss 3728.40051\n",
      "Iteration:    290    step:     3114     combined loss: 4315.62637     paf loss 24.20229     hm loss 4291.42407\n",
      "Iteration:    295    step:     3119     combined loss: 5408.57812     paf loss 18.80346     hm loss 5389.77466\n",
      "Iteration:    300    step:     3124     combined loss: 4119.29883     paf loss 23.89905     hm loss 4095.39978\n",
      "Iteration:    305    step:     3129     combined loss: 3399.12824     paf loss 24.49884     hm loss 3374.62939\n",
      "Iteration:    310    step:     3134     combined loss: 3070.00806     paf loss 17.44849     hm loss 3052.55957\n",
      "Iteration:    315    step:     3139     combined loss: 3786.41271     paf loss 23.24840     hm loss 3763.16431\n",
      "Iteration:    320    step:     3144     combined loss: 4264.41113     paf loss 19.04687     hm loss 4245.36426\n",
      "Iteration:    325    step:     3149     combined loss: 4786.35834     paf loss 31.86933     hm loss 4754.48901\n",
      "Iteration:    330    step:     3154     combined loss: 3040.69285     paf loss 16.95262     hm loss 3023.74023\n",
      "Iteration:    335    step:     3159     combined loss: 3789.39498     paf loss 19.26339     hm loss 3770.13159\n",
      "Iteration:    340    step:     3164     combined loss: 3162.06850     paf loss 26.95473     hm loss 3135.11377\n",
      "Iteration:    345    step:     3169     combined loss: 4845.06145     paf loss 28.87322     hm loss 4816.18823\n",
      "Iteration:    350    step:     3174     combined loss: 5032.35177     paf loss 14.75069     hm loss 5017.60107\n",
      "Iteration:    355    step:     3179     combined loss: 3288.76840     paf loss 20.11862     hm loss 3268.64978\n",
      "Iteration:    360    step:     3184     combined loss: 5428.32114     paf loss 18.62534     hm loss 5409.69580\n",
      "Iteration:    365    step:     3189     combined loss: 3366.91892     paf loss 16.89548     hm loss 3350.02344\n",
      "Iteration:    370    step:     3194     combined loss: 3349.74693     paf loss 17.80504     hm loss 3331.94189\n",
      "Iteration:    375    step:     3199     combined loss: 5761.69362     paf loss 29.12575     hm loss 5732.56787\n",
      "Iteration:    380    step:     3204     combined loss: 4958.85687     paf loss 26.48602     hm loss 4932.37085\n",
      "Iteration:    385    step:     3209     combined loss: 4883.04625     paf loss 20.98790     hm loss 4862.05835\n",
      "Iteration:    390    step:     3214     combined loss: 3640.19523     paf loss 20.12431     hm loss 3620.07092\n",
      "Iteration:    395    step:     3219     combined loss: 3259.71989     paf loss 16.29839     hm loss 3243.42151\n",
      "Iteration:    400    step:     3224     combined loss: 4380.93003     paf loss 18.31114     hm loss 4362.61890\n",
      "Iteration:    405    step:     3229     combined loss: 3908.21495     paf loss 18.08434     hm loss 3890.13062\n",
      "Iteration:    410    step:     3234     combined loss: 4707.71521     paf loss 22.12366     hm loss 4685.59155\n",
      "Iteration:    415    step:     3239     combined loss: 2429.15388     paf loss 12.76960     hm loss 2416.38428\n",
      "Iteration:    420    step:     3244     combined loss: 4168.89888     paf loss 23.27925     hm loss 4145.61963\n",
      "Iteration:    425    step:     3249     combined loss: 3709.02542     paf loss 18.31070     hm loss 3690.71472\n",
      "Iteration:    430    step:     3254     combined loss: 4377.36835     paf loss 22.42280     hm loss 4354.94556\n",
      "Iteration:    435    step:     3259     combined loss: 5526.08183     paf loss 16.46830     hm loss 5509.61353\n",
      "Iteration:    440    step:     3264     combined loss: 4319.77712     paf loss 18.41531     hm loss 4301.36182\n",
      "Iteration:    445    step:     3269     combined loss: 3991.08484     paf loss 16.49499     hm loss 3974.58984\n",
      "Iteration:    450    step:     3274     combined loss: 3773.26350     paf loss 15.35420     hm loss 3757.90930\n",
      "Iteration:    455    step:     3279     combined loss: 2865.56980     paf loss 15.83750     hm loss 2849.73230\n",
      "Iteration:    460    step:     3284     combined loss: 4097.90891     paf loss 25.92026     hm loss 4071.98865\n",
      "Iteration:    465    step:     3289     combined loss: 2972.84807     paf loss 18.35246     hm loss 2954.49561\n",
      "Iteration:    470    step:     3294     combined loss: 4203.72320     paf loss 22.53399     hm loss 4181.18921\n",
      "Iteration:    475    step:     3299     combined loss: 3317.30277     paf loss 20.96805     hm loss 3296.33472\n",
      "Iteration:    480    step:     3304     combined loss: 4401.97440     paf loss 19.06644     hm loss 4382.90796\n",
      "Iteration:    485    step:     3309     combined loss: 5443.11641     paf loss 21.91890     hm loss 5421.19751\n",
      "Iteration:    490    step:     3314     combined loss: 2974.20708     paf loss 19.50908     hm loss 2954.69800\n",
      "Iteration:    495    step:     3319     combined loss: 3365.58140     paf loss 21.32652     hm loss 3344.25488\n",
      "Iteration:    500    step:     3324     combined loss: 4350.58763     paf loss 20.76854     hm loss 4329.81909\n",
      "Iteration:    505    step:     3329     combined loss: 2591.78083     paf loss 21.75751     hm loss 2570.02332\n",
      "Iteration:    510    step:     3334     combined loss: 4604.86233     paf loss 19.07058     hm loss 4585.79175\n",
      "Iteration:    515    step:     3339     combined loss: 5840.45841     paf loss 23.44987     hm loss 5817.00854\n",
      "Iteration:    520    step:     3344     combined loss: 4376.73668     paf loss 24.79479     hm loss 4351.94189\n",
      "Iteration:    525    step:     3349     combined loss: 3000.69682     paf loss 16.81377     hm loss 2983.88306\n",
      "Iteration:    530    step:     3354     combined loss: 3928.60652     paf loss 20.78816     hm loss 3907.81836\n",
      "Iteration:    535    step:     3359     combined loss: 3617.40753     paf loss 16.77252     hm loss 3600.63501\n",
      "Iteration:    540    step:     3364     combined loss: 2594.40619     paf loss 14.71856     hm loss 2579.68762\n",
      "Iteration:    545    step:     3369     combined loss: 6770.63145     paf loss 27.13316     hm loss 6743.49829\n",
      "Iteration:    550    step:     3374     combined loss: 4442.97349     paf loss 16.99522     hm loss 4425.97827\n",
      "Iteration:    555    step:     3379     combined loss: 5274.46070     paf loss 23.81836     hm loss 5250.64233\n",
      "Iteration:    560    step:     3384     combined loss: 3445.47634     paf loss 28.06813     hm loss 3417.40820\n",
      "Iteration:    565    step:     3389     combined loss: 4343.67220     paf loss 23.86116     hm loss 4319.81104\n",
      "Iteration:    570    step:     3394     combined loss: 4629.23046     paf loss 21.25805     hm loss 4607.97241\n",
      "Iteration:    575    step:     3399     combined loss: 2769.00833     paf loss 18.97318     hm loss 2750.03516\n",
      "Iteration:    580    step:     3404     combined loss: 3504.56935     paf loss 14.90418     hm loss 3489.66516\n",
      "Iteration:    585    step:     3409     combined loss: 3429.23033     paf loss 17.90965     hm loss 3411.32068\n",
      "Iteration:    590    step:     3414     combined loss: 3446.11538     paf loss 18.60293     hm loss 3427.51245\n",
      "Iteration:    595    step:     3419     combined loss: 4551.67263     paf loss 25.28957     hm loss 4526.38306\n",
      "Iteration:    600    step:     3424     combined loss: 4422.32487     paf loss 25.42058     hm loss 4396.90430\n",
      "Iteration:    605    step:     3429     combined loss: 3446.43265     paf loss 16.03482     hm loss 3430.39783\n",
      "Iteration:    610    step:     3434     combined loss: 4487.68420     paf loss 21.96887     hm loss 4465.71533\n",
      "Iteration:    615    step:     3439     combined loss: 4870.04941     paf loss 19.08603     hm loss 4850.96338\n",
      "Iteration:    620    step:     3444     combined loss: 4438.92363     paf loss 19.82402     hm loss 4419.09961\n",
      "Iteration:    625    step:     3449     combined loss: 3259.70717     paf loss 15.98122     hm loss 3243.72595\n",
      "Iteration:    630    step:     3454     combined loss: 4656.11747     paf loss 20.13114     hm loss 4635.98633\n",
      "Iteration:    635    step:     3459     combined loss: 4972.91380     paf loss 20.68016     hm loss 4952.23364\n",
      "Iteration:    640    step:     3464     combined loss: 3062.43368     paf loss 19.80612     hm loss 3042.62756\n",
      "Iteration:    645    step:     3469     combined loss: 4129.67207     paf loss 19.00532     hm loss 4110.66675\n",
      "Iteration:    650    step:     3474     combined loss: 3201.73723     paf loss 20.37310     hm loss 3181.36414\n",
      "Iteration:    655    step:     3479     combined loss: 6584.52078     paf loss 25.89993     hm loss 6558.62085\n",
      "Iteration:    660    step:     3484     combined loss: 5677.44687     paf loss 26.88804     hm loss 5650.55884\n",
      "Iteration:    665    step:     3489     combined loss: 5413.42396     paf loss 18.96619     hm loss 5394.45776\n",
      "Iteration:    670    step:     3494     combined loss: 5728.97911     paf loss 21.34630     hm loss 5707.63281\n",
      "Iteration:    675    step:     3499     combined loss: 4909.77228     paf loss 19.73493     hm loss 4890.03735\n",
      "Iteration:    680    step:     3504     combined loss: 4041.27304     paf loss 15.77890     hm loss 4025.49414\n",
      "Iteration:    685    step:     3509     combined loss: 4428.27818     paf loss 18.55845     hm loss 4409.71973\n",
      "Iteration:    690    step:     3514     combined loss: 4465.36773     paf loss 24.23687     hm loss 4441.13086\n",
      "Iteration:    695    step:     3519     combined loss: 4013.67188     paf loss 21.21167     hm loss 3992.46021\n",
      "Iteration:    700    step:     3524     combined loss: 3941.14518     paf loss 16.61551     hm loss 3924.52966\n",
      "Iteration:    705    step:     3529     combined loss: 3222.66108     paf loss 24.55817     hm loss 3198.10291\n",
      "Iteration:    710    step:     3534     combined loss: 5020.59453     paf loss 22.10014     hm loss 4998.49438\n",
      "Iteration:    715    step:     3539     combined loss: 3678.66243     paf loss 15.53963     hm loss 3663.12280\n",
      "Iteration:    720    step:     3544     combined loss: 4330.88647     paf loss 18.15673     hm loss 4312.72974\n",
      "Iteration:    725    step:     3549     combined loss: 3596.21134     paf loss 19.97232     hm loss 3576.23901\n",
      "Iteration:    730    step:     3554     combined loss: 4814.00786     paf loss 24.26201     hm loss 4789.74585\n",
      "Iteration:    735    step:     3559     combined loss: 3541.24320     paf loss 20.17973     hm loss 3521.06348\n",
      "Iteration:    740    step:     3564     combined loss: 5178.12153     paf loss 24.39643     hm loss 5153.72510\n",
      "Iteration:    745    step:     3569     combined loss: 3900.59434     paf loss 18.34971     hm loss 3882.24463\n",
      "Iteration:    750    step:     3574     combined loss: 3794.69437     paf loss 21.24405     hm loss 3773.45032\n",
      "Iteration:    755    step:     3579     combined loss: 5068.78048     paf loss 28.36080     hm loss 5040.41968\n",
      "Iteration:    760    step:     3584     combined loss: 3823.89514     paf loss 24.14160     hm loss 3799.75354\n",
      "Iteration:    765    step:     3589     combined loss: 4463.78178     paf loss 21.82842     hm loss 4441.95337\n",
      "Iteration:    770    step:     3594     combined loss: 4274.10243     paf loss 23.72352     hm loss 4250.37891\n",
      "Iteration:    775    step:     3599     combined loss: 2712.68533     paf loss 13.42471     hm loss 2699.26062\n",
      "Iteration:    780    step:     3604     combined loss: 5024.90710     paf loss 14.71862     hm loss 5010.18848\n",
      "Iteration:    785    step:     3609     combined loss: 3751.75596     paf loss 19.97129     hm loss 3731.78467\n",
      "Iteration:    790    step:     3614     combined loss: 3739.97423     paf loss 16.13232     hm loss 3723.84192\n",
      "Iteration:    795    step:     3619     combined loss: 2533.03963     paf loss 20.57564     hm loss 2512.46399\n",
      "Iteration:    800    step:     3624     combined loss: 6290.66042     paf loss 20.94020     hm loss 6269.72021\n",
      "Iteration:    805    step:     3629     combined loss: 3531.86470     paf loss 18.12337     hm loss 3513.74133\n",
      "Iteration:    810    step:     3634     combined loss: 2840.87129     paf loss 16.62409     hm loss 2824.24719\n",
      "Iteration:    815    step:     3639     combined loss: 4118.47531     paf loss 21.61630     hm loss 4096.85901\n",
      "Iteration:    820    step:     3644     combined loss: 4108.51081     paf loss 18.07477     hm loss 4090.43604\n",
      "Iteration:    825    step:     3649     combined loss: 3142.05467     paf loss 16.37144     hm loss 3125.68323\n",
      "Iteration:    830    step:     3654     combined loss: 4526.29651     paf loss 21.43860     hm loss 4504.85791\n",
      "Iteration:    835    step:     3659     combined loss: 8179.06588     paf loss 23.00631     hm loss 8156.05957\n",
      "Iteration:    840    step:     3664     combined loss: 3025.30854     paf loss 16.98688     hm loss 3008.32166\n",
      "Iteration:    845    step:     3669     combined loss: 2904.13833     paf loss 17.14211     hm loss 2886.99622\n",
      "Iteration:    850    step:     3674     combined loss: 3246.87300     paf loss 14.07307     hm loss 3232.79993\n",
      "Iteration:    855    step:     3679     combined loss: 4712.92856     paf loss 16.16904     hm loss 4696.75952\n",
      "Iteration:    860    step:     3684     combined loss: 4233.58591     paf loss 18.74582     hm loss 4214.84009\n",
      "Iteration:    865    step:     3689     combined loss: 3594.31833     paf loss 19.14596     hm loss 3575.17236\n",
      "Iteration:    870    step:     3694     combined loss: 4439.61459     paf loss 22.67148     hm loss 4416.94312\n",
      "Iteration:    875    step:     3699     combined loss: 3439.50478     paf loss 15.41408     hm loss 3424.09070\n",
      "Iteration:    880    step:     3704     combined loss: 3077.11149     paf loss 22.35063     hm loss 3054.76086\n",
      "Iteration:    885    step:     3709     combined loss: 4701.88577     paf loss 23.21805     hm loss 4678.66772\n",
      "Iteration:    890    step:     3714     combined loss: 3793.40271     paf loss 22.00659     hm loss 3771.39612\n",
      "Iteration:    895    step:     3719     combined loss: 3619.56725     paf loss 17.36120     hm loss 3602.20605\n",
      "Iteration:    900    step:     3724     combined loss: 4054.89290     paf loss 20.68770     hm loss 4034.20520\n",
      "Iteration:    905    step:     3729     combined loss: 4868.23900     paf loss 21.41575     hm loss 4846.82324\n",
      "Iteration:    910    step:     3734     combined loss: 3878.68151     paf loss 22.97021     hm loss 3855.71130\n",
      "Iteration:    915    step:     3739     combined loss: 2890.36016     paf loss 17.09002     hm loss 2873.27014\n",
      "Iteration:    920    step:     3744     combined loss: 2965.17946     paf loss 21.63564     hm loss 2943.54382\n",
      "Iteration:    925    step:     3749     combined loss: 2843.78648     paf loss 21.74705     hm loss 2822.03943\n",
      "Iteration:    930    step:     3754     combined loss: 4741.85367     paf loss 24.94034     hm loss 4716.91333\n",
      "Iteration:    935    step:     3759     combined loss: 3183.14779     paf loss 15.60140     hm loss 3167.54639\n",
      "Iteration:    940    step:     3764     combined loss: 5268.40721     paf loss 27.08739     hm loss 5241.31982\n",
      "Iteration:    945    step:     3769     combined loss: 5992.56149     paf loss 18.60690     hm loss 5973.95459\n",
      "Iteration:    950    step:     3774     combined loss: 2157.67067     paf loss 13.48281     hm loss 2144.18787\n",
      "Iteration:    955    step:     3779     combined loss: 3262.07213     paf loss 19.53502     hm loss 3242.53711\n",
      "Iteration:    960    step:     3784     combined loss: 3825.92916     paf loss 17.02157     hm loss 3808.90759\n",
      "Iteration:    965    step:     3789     combined loss: 2276.97680     paf loss 16.95421     hm loss 2260.02258\n",
      "Iteration:    970    step:     3794     combined loss: 2946.93799     paf loss 14.98401     hm loss 2931.95398\n",
      "Iteration:    975    step:     3799     combined loss: 5493.85541     paf loss 20.70649     hm loss 5473.14893\n",
      "Iteration:    980    step:     3804     combined loss: 3530.82092     paf loss 14.96570     hm loss 3515.85522\n",
      "Iteration:    985    step:     3809     combined loss: 3143.12448     paf loss 19.53769     hm loss 3123.58679\n",
      "Iteration:    990    step:     3814     combined loss: 3089.35584     paf loss 22.51465     hm loss 3066.84119\n",
      "Iteration:    995    step:     3819     combined loss: 3039.69910     paf loss 18.58228     hm loss 3021.11682\n",
      "Iteration:   1000    step:     3824     combined loss: 3180.19279     paf loss 22.27812     hm loss 3157.91467\n",
      "Iteration:   1005    step:     3829     combined loss: 3273.52347     paf loss 19.61490     hm loss 3253.90857\n",
      "Iteration:   1010    step:     3834     combined loss: 4763.35816     paf loss 22.53761     hm loss 4740.82056\n",
      "Iteration:   1015    step:     3839     combined loss: 3696.50088     paf loss 21.06167     hm loss 3675.43921\n",
      "Iteration:   1020    step:     3844     combined loss: 2941.75365     paf loss 24.75182     hm loss 2917.00183\n",
      "Iteration:   1025    step:     3849     combined loss: 3995.55696     paf loss 16.77913     hm loss 3978.77783\n",
      "Iteration:   1030    step:     3854     combined loss: 3584.88655     paf loss 16.28291     hm loss 3568.60364\n",
      "Iteration:   1035    step:     3859     combined loss: 6064.77896     paf loss 22.52115     hm loss 6042.25781\n",
      "Iteration:   1040    step:     3864     combined loss: 3015.02414     paf loss 22.81161     hm loss 2992.21252\n",
      "Iteration:   1045    step:     3869     combined loss: 6413.14387     paf loss 21.57404     hm loss 6391.56982\n",
      "Iteration:   1050    step:     3874     combined loss: 3874.99066     paf loss 21.64471     hm loss 3853.34595\n",
      "Iteration:   1055    step:     3879     combined loss: 3453.91086     paf loss 18.45212     hm loss 3435.45874\n",
      "Iteration:   1060    step:     3884     combined loss: 3189.39326     paf loss 18.52460     hm loss 3170.86865\n",
      "Iteration:   1065    step:     3889     combined loss: 2981.16435     paf loss 16.17790     hm loss 2964.98645\n",
      "Iteration:   1070    step:     3894     combined loss: 3579.67255     paf loss 15.80097     hm loss 3563.87158\n",
      "Iteration:   1075    step:     3899     combined loss: 3927.87712     paf loss 18.87346     hm loss 3909.00366\n",
      "Iteration:   1080    step:     3904     combined loss: 3477.21665     paf loss 16.77793     hm loss 3460.43872\n",
      "Iteration:   1085    step:     3909     combined loss: 3186.88117     paf loss 17.53816     hm loss 3169.34302\n",
      "Iteration:   1090    step:     3914     combined loss: 3751.73403     paf loss 16.58498     hm loss 3735.14905\n",
      "Iteration:   1095    step:     3919     combined loss: 3249.91901     paf loss 17.61457     hm loss 3232.30444\n",
      "Iteration:   1100    step:     3924     combined loss: 3259.74281     paf loss 20.66468     hm loss 3239.07812\n",
      "Iteration:   1105    step:     3929     combined loss: 3281.42802     paf loss 17.01383     hm loss 3264.41418\n",
      "Iteration:   1110    step:     3934     combined loss: 3438.58752     paf loss 24.91382     hm loss 3413.67371\n",
      "Iteration:   1115    step:     3939     combined loss: 3064.89498     paf loss 18.44308     hm loss 3046.45190\n",
      "Iteration:   1120    step:     3944     combined loss: 4793.16121     paf loss 20.34480     hm loss 4772.81641\n",
      "Iteration:   1125    step:     3949     combined loss: 4851.34023     paf loss 20.27261     hm loss 4831.06763\n",
      "Iteration:   1130    step:     3954     combined loss: 2861.71010     paf loss 17.51308     hm loss 2844.19702\n",
      "Iteration:   1135    step:     3959     combined loss: 3823.57263     paf loss 20.64526     hm loss 3802.92737\n",
      "Iteration:   1140    step:     3964     combined loss: 4135.72759     paf loss 20.59868     hm loss 4115.12891\n",
      "Iteration:   1145    step:     3969     combined loss: 4685.04990     paf loss 18.83017     hm loss 4666.21973\n",
      "Iteration:   1150    step:     3974     combined loss: 4702.83067     paf loss 20.56968     hm loss 4682.26099\n",
      "Iteration:   1155    step:     3979     combined loss: 4450.49159     paf loss 17.39637     hm loss 4433.09521\n",
      "Iteration:   1160    step:     3984     combined loss: 2988.38937     paf loss 18.58968     hm loss 2969.79968\n",
      "Iteration:   1165    step:     3989     combined loss: 3729.98320     paf loss 22.71330     hm loss 3707.26990\n",
      "Iteration:   1170    step:     3994     combined loss: 3353.11223     paf loss 17.85063     hm loss 3335.26160\n",
      "Iteration:   1175    step:     3999     combined loss: 2804.94009     paf loss 20.25967     hm loss 2784.68042\n",
      "Iteration:   1180    step:     4004     combined loss: 5902.61108     paf loss 21.22363     hm loss 5881.38745\n",
      "Iteration:   1185    step:     4009     combined loss: 3797.17753     paf loss 23.54398     hm loss 3773.63354\n",
      "Iteration:   1190    step:     4014     combined loss: 4192.13571     paf loss 19.93453     hm loss 4172.20117\n",
      "Iteration:   1195    step:     4019     combined loss: 4010.96288     paf loss 27.09789     hm loss 3983.86499\n",
      "Iteration:   1200    step:     4024     combined loss: 3488.24539     paf loss 18.19962     hm loss 3470.04578\n",
      "Iteration:   1205    step:     4029     combined loss: 2994.20857     paf loss 19.86934     hm loss 2974.33923\n",
      "Iteration:   1210    step:     4034     combined loss: 6298.77849     paf loss 23.92839     hm loss 6274.85010\n",
      "Iteration:   1215    step:     4039     combined loss: 3444.53267     paf loss 20.75142     hm loss 3423.78125\n",
      "Iteration:   1220    step:     4044     combined loss: 4631.63246     paf loss 17.93154     hm loss 4613.70093\n",
      "Iteration:   1225    step:     4049     combined loss: 3476.81942     paf loss 21.28512     hm loss 3455.53430\n",
      "Iteration:   1230    step:     4054     combined loss: 3646.84136     paf loss 18.67278     hm loss 3628.16858\n",
      "Iteration:   1235    step:     4059     combined loss: 2538.94064     paf loss 14.37948     hm loss 2524.56116\n",
      "Iteration:   1240    step:     4064     combined loss: 3082.53608     paf loss 16.01227     hm loss 3066.52380\n",
      "Iteration:   1245    step:     4069     combined loss: 3324.97729     paf loss 15.44958     hm loss 3309.52771\n",
      "Iteration:   1250    step:     4074     combined loss: 2974.34384     paf loss 16.61191     hm loss 2957.73193\n",
      "Iteration:   1255    step:     4079     combined loss: 3182.51152     paf loss 17.27385     hm loss 3165.23767\n",
      "Iteration:   1260    step:     4084     combined loss: 6599.38138     paf loss 19.84379     hm loss 6579.53760\n",
      "Iteration:   1265    step:     4089     combined loss: 4794.35007     paf loss 22.23679     hm loss 4772.11328\n",
      "Iteration:   1270    step:     4094     combined loss: 5591.28856     paf loss 27.02514     hm loss 5564.26343\n",
      "Iteration:   1275    step:     4099     combined loss: 4937.23754     paf loss 21.63036     hm loss 4915.60718\n",
      "Iteration:   1280    step:     4104     combined loss: 3245.02925     paf loss 17.20418     hm loss 3227.82507\n",
      "Iteration:   1285    step:     4109     combined loss: 4370.22107     paf loss 26.85242     hm loss 4343.36865\n",
      "Iteration:   1290    step:     4114     combined loss: 3154.79445     paf loss 23.12990     hm loss 3131.66455\n",
      "Iteration:   1295    step:     4119     combined loss: 4701.79640     paf loss 18.93092     hm loss 4682.86548\n",
      "Iteration:   1300    step:     4124     combined loss: 2647.67864     paf loss 21.97857     hm loss 2625.70007\n",
      "Iteration:   1305    step:     4129     combined loss: 4399.54772     paf loss 19.34533     hm loss 4380.20239\n",
      "Iteration:   1310    step:     4134     combined loss: 6504.07989     paf loss 19.66143     hm loss 6484.41846\n",
      "Iteration:   1315    step:     4139     combined loss: 4544.04234     paf loss 16.72521     hm loss 4527.31714\n",
      "Iteration:   1320    step:     4144     combined loss: 4172.10950     paf loss 15.99256     hm loss 4156.11694\n",
      "Iteration:   1325    step:     4149     combined loss: 4439.52889     paf loss 21.16341     hm loss 4418.36548\n",
      "Iteration:   1330    step:     4154     combined loss: 5547.94426     paf loss 29.73040     hm loss 5518.21387\n",
      "Iteration:   1335    step:     4159     combined loss: 6721.90469     paf loss 25.62222     hm loss 6696.28247\n",
      "Iteration:   1340    step:     4164     combined loss: 4040.61562     paf loss 18.35769     hm loss 4022.25793\n",
      "Iteration:   1345    step:     4169     combined loss: 4677.89842     paf loss 20.33006     hm loss 4657.56836\n",
      "Iteration:   1350    step:     4174     combined loss: 4349.02684     paf loss 15.76195     hm loss 4333.26489\n",
      "Iteration:   1355    step:     4179     combined loss: 3800.40957     paf loss 21.77957     hm loss 3778.63000\n",
      "Iteration:   1360    step:     4184     combined loss: 4088.07888     paf loss 22.95217     hm loss 4065.12671\n",
      "Iteration:   1365    step:     4189     combined loss: 2696.30048     paf loss 18.40925     hm loss 2677.89124\n",
      "Iteration:   1370    step:     4194     combined loss: 3371.54136     paf loss 22.89695     hm loss 3348.64441\n",
      "Iteration:   1375    step:     4199     combined loss: 4870.59616     paf loss 20.36911     hm loss 4850.22705\n",
      "Iteration:   1380    step:     4204     combined loss: 4501.09675     paf loss 22.57820     hm loss 4478.51855\n",
      "Iteration:   1385    step:     4209     combined loss: 3806.73532     paf loss 17.58957     hm loss 3789.14575\n",
      "Iteration:   1390    step:     4214     combined loss: 3009.76578     paf loss 16.25638     hm loss 2993.50940\n",
      "Iteration:   1395    step:     4219     combined loss: 3725.66524     paf loss 19.66219     hm loss 3706.00305\n",
      "Iteration:   1400    step:     4224     combined loss: 4495.95036     paf loss 14.66227     hm loss 4481.28809\n",
      "Iteration:   1405    step:     4229     combined loss: 3856.61267     paf loss 16.59815     hm loss 3840.01453\n",
      "Iteration:   1410    step:     4234     combined loss: 2608.02859     paf loss 16.15957     hm loss 2591.86902\n",
      "Iteration:   1415    step:     4239     combined loss: 2969.11940     paf loss 18.59620     hm loss 2950.52319\n",
      "Iteration:   1420    step:     4244     combined loss: 8157.59022     paf loss 21.71181     hm loss 8135.87842\n",
      "Iteration:   1425    step:     4249     combined loss: 4737.18304     paf loss 21.13861     hm loss 4716.04443\n",
      "Iteration:   1430    step:     4254     combined loss: 4272.62650     paf loss 22.86381     hm loss 4249.76270\n",
      "Iteration:   1435    step:     4259     combined loss: 3161.94282     paf loss 21.96296     hm loss 3139.97986\n",
      "Iteration:   1440    step:     4264     combined loss: 3867.36879     paf loss 28.53725     hm loss 3838.83154\n",
      "Iteration:   1445    step:     4269     combined loss: 4069.32280     paf loss 24.54986     hm loss 4044.77295\n",
      "Iteration:   1450    step:     4274     combined loss: 5380.09405     paf loss 15.34722     hm loss 5364.74683\n",
      "Iteration:   1455    step:     4279     combined loss: 3704.65564     paf loss 18.34326     hm loss 3686.31238\n",
      "Iteration:   1460    step:     4284     combined loss: 3575.48324     paf loss 21.23458     hm loss 3554.24866\n",
      "Iteration:   1465    step:     4289     combined loss: 5680.43568     paf loss 24.57850     hm loss 5655.85718\n",
      "Iteration:   1470    step:     4294     combined loss: 2559.66015     paf loss 16.19701     hm loss 2543.46313\n",
      "Iteration:   1475    step:     4299     combined loss: 2176.98977     paf loss 14.37978     hm loss 2162.60999\n",
      "Iteration:   1480    step:     4304     combined loss: 5460.57431     paf loss 20.19589     hm loss 5440.37842\n",
      "Iteration:   1485    step:     4309     combined loss: 3198.75342     paf loss 18.68750     hm loss 3180.06592\n",
      "Iteration:   1490    step:     4314     combined loss: 3419.05779     paf loss 20.45244     hm loss 3398.60535\n",
      "Iteration:   1495    step:     4319     combined loss: 2966.39539     paf loss 16.58094     hm loss 2949.81445\n",
      "Iteration:   1500    step:     4324     combined loss: 4927.16486     paf loss 18.31598     hm loss 4908.84888\n",
      "Iteration:   1505    step:     4329     combined loss: 4261.21638     paf loss 17.91926     hm loss 4243.29712\n",
      "Iteration:   1510    step:     4334     combined loss: 4139.89340     paf loss 18.31674     hm loss 4121.57666\n",
      "Iteration:   1515    step:     4339     combined loss: 4618.42292     paf loss 18.75959     hm loss 4599.66333\n",
      "Iteration:   1520    step:     4344     combined loss: 2929.95594     paf loss 17.86085     hm loss 2912.09509\n",
      "Iteration:   1525    step:     4349     combined loss: 3985.47642     paf loss 21.14475     hm loss 3964.33167\n",
      "Iteration:   1530    step:     4354     combined loss: 4349.51014     paf loss 24.86145     hm loss 4324.64868\n",
      "Iteration:   1535    step:     4359     combined loss: 5151.10457     paf loss 24.15657     hm loss 5126.94800\n",
      "Iteration:   1540    step:     4364     combined loss: 4710.59102     paf loss 19.90645     hm loss 4690.68457\n",
      "Iteration:   1545    step:     4369     combined loss: 4620.65255     paf loss 21.37667     hm loss 4599.27588\n",
      "Iteration:   1550    step:     4374     combined loss: 3921.89053     paf loss 24.65909     hm loss 3897.23145\n",
      "Iteration:   1555    step:     4379     combined loss: 3724.71621     paf loss 16.10317     hm loss 3708.61304\n",
      "Iteration:   1560    step:     4384     combined loss: 3528.78783     paf loss 19.70201     hm loss 3509.08582\n",
      "Iteration:   1565    step:     4389     combined loss: 4374.66958     paf loss 27.01236     hm loss 4347.65723\n",
      "Iteration:   1570    step:     4394     combined loss: 2733.48384     paf loss 16.46773     hm loss 2717.01611\n",
      "Iteration:   1575    step:     4399     combined loss: 4277.78852     paf loss 20.86298     hm loss 4256.92554\n",
      "Iteration:   1580    step:     4404     combined loss: 3241.52105     paf loss 15.93804     hm loss 3225.58301\n",
      "Iteration:   1585    step:     4409     combined loss: 2887.56449     paf loss 18.20829     hm loss 2869.35620\n",
      "Iteration:   1590    step:     4414     combined loss: 3438.88849     paf loss 14.39521     hm loss 3424.49329\n",
      "Iteration:   1595    step:     4419     combined loss: 3795.94108     paf loss 19.38700     hm loss 3776.55408\n",
      "Iteration:   1600    step:     4424     combined loss: 3721.65131     paf loss 18.72760     hm loss 3702.92371\n",
      "Iteration:   1605    step:     4429     combined loss: 4719.96645     paf loss 17.33412     hm loss 4702.63232\n",
      "Iteration:   1610    step:     4434     combined loss: 4084.94149     paf loss 19.11532     hm loss 4065.82617\n",
      "Iteration:   1615    step:     4439     combined loss: 4325.65085     paf loss 26.19552     hm loss 4299.45532\n",
      "Iteration:   1620    step:     4444     combined loss: 2491.40646     paf loss 16.30124     hm loss 2475.10522\n",
      "Iteration:   1625    step:     4449     combined loss: 3374.47315     paf loss 19.63013     hm loss 3354.84302\n",
      "Iteration:   1630    step:     4454     combined loss: 3784.33485     paf loss 15.99415     hm loss 3768.34070\n",
      "Iteration:   1635    step:     4459     combined loss: 2277.34670     paf loss 11.70315     hm loss 2265.64355\n",
      "Iteration:   1640    step:     4464     combined loss: 4491.16045     paf loss 24.52031     hm loss 4466.64014\n",
      "Iteration:   1645    step:     4469     combined loss: 4149.56924     paf loss 25.24709     hm loss 4124.32214\n",
      "Iteration:   1650    step:     4474     combined loss: 4106.03003     paf loss 21.81604     hm loss 4084.21399\n",
      "Iteration:   1655    step:     4479     combined loss: 3913.12404     paf loss 25.41920     hm loss 3887.70483\n",
      "Iteration:   1660    step:     4484     combined loss: 3930.25142     paf loss 22.51949     hm loss 3907.73193\n",
      "Iteration:   1665    step:     4489     combined loss: 3077.42637     paf loss 18.42064     hm loss 3059.00574\n",
      "Iteration:   1670    step:     4494     combined loss: 2926.16293     paf loss 19.57125     hm loss 2906.59167\n",
      "Iteration:   1675    step:     4499     combined loss: 3321.97638     paf loss 22.48700     hm loss 3299.48938\n",
      "Iteration:   1680    step:     4504     combined loss: 5709.51903     paf loss 19.79369     hm loss 5689.72534\n",
      "Iteration:   1685    step:     4509     combined loss: 3860.68648     paf loss 18.94588     hm loss 3841.74060\n",
      "Iteration:   1690    step:     4514     combined loss: 3355.20789     paf loss 22.97718     hm loss 3332.23071\n",
      "Iteration:   1695    step:     4519     combined loss: 3459.70163     paf loss 17.84640     hm loss 3441.85522\n",
      "Iteration:   1700    step:     4524     combined loss: 3566.77304     paf loss 18.84494     hm loss 3547.92810\n",
      "Iteration:   1705    step:     4529     combined loss: 4201.76139     paf loss 21.66251     hm loss 4180.09888\n",
      "Iteration:   1710    step:     4534     combined loss: 3789.92707     paf loss 21.25800     hm loss 3768.66907\n",
      "Iteration:   1715    step:     4539     combined loss: 5995.00036     paf loss 26.74059     hm loss 5968.25977\n",
      "Iteration:   1720    step:     4544     combined loss: 3322.13448     paf loss 20.15144     hm loss 3301.98303\n",
      "Iteration:   1725    step:     4549     combined loss: 3463.09605     paf loss 23.42930     hm loss 3439.66675\n",
      "Iteration:   1730    step:     4554     combined loss: 5100.35340     paf loss 19.92761     hm loss 5080.42578\n",
      "Iteration:   1735    step:     4559     combined loss: 3259.66232     paf loss 17.06467     hm loss 3242.59766\n",
      "Iteration:   1740    step:     4564     combined loss: 2282.01954     paf loss 19.32642     hm loss 2262.69312\n",
      "Iteration:   1745    step:     4569     combined loss: 2385.86001     paf loss 16.61855     hm loss 2369.24146\n",
      "Iteration:   1750    step:     4574     combined loss: 4996.99909     paf loss 18.13263     hm loss 4978.86646\n",
      "Iteration:   1755    step:     4579     combined loss: 2863.94299     paf loss 19.43066     hm loss 2844.51233\n",
      "Iteration:   1760    step:     4584     combined loss: 4640.28105     paf loss 22.08891     hm loss 4618.19214\n",
      "Iteration:   1765    step:     4589     combined loss: 5725.10591     paf loss 20.64912     hm loss 5704.45679\n",
      "Iteration:   1770    step:     4594     combined loss: 3088.64509     paf loss 20.52046     hm loss 3068.12463\n",
      "Iteration:   1775    step:     4599     combined loss: 6677.95824     paf loss 21.45653     hm loss 6656.50171\n",
      "Iteration:   1780    step:     4604     combined loss: 3153.58262     paf loss 20.41136     hm loss 3133.17126\n",
      "Iteration:   1785    step:     4609     combined loss: 3204.15026     paf loss 18.19506     hm loss 3185.95520\n",
      "Iteration:   1790    step:     4614     combined loss: 5192.88239     paf loss 16.40119     hm loss 5176.48120\n",
      "Iteration:   1795    step:     4619     combined loss: 4526.65624     paf loss 26.82103     hm loss 4499.83521\n",
      "Iteration:   1800    step:     4624     combined loss: 3543.99439     paf loss 19.23902     hm loss 3524.75537\n",
      "Iteration:   1805    step:     4629     combined loss: 3240.47304     paf loss 19.74477     hm loss 3220.72827\n",
      "Iteration:   1810    step:     4634     combined loss: 2900.86742     paf loss 19.93504     hm loss 2880.93237\n",
      "learning rate change: 0.0005 --> 0.00025\n",
      "Iteration:   1815    step:     4639     combined loss: 5064.42402     paf loss 24.72261     hm loss 5039.70142\n",
      "Iteration:   1820    step:     4644     combined loss: 4811.02075     paf loss 18.40552     hm loss 4792.61523\n",
      "Iteration:   1825    step:     4649     combined loss: 3289.53521     paf loss 20.87773     hm loss 3268.65747\n",
      "Iteration:   1830    step:     4654     combined loss: 7793.24956     paf loss 22.75249     hm loss 7770.49707\n",
      "Iteration:   1835    step:     4659     combined loss: 3123.27374     paf loss 17.65106     hm loss 3105.62268\n",
      "Iteration:   1840    step:     4664     combined loss: 4311.68479     paf loss 22.20579     hm loss 4289.47900\n",
      "Iteration:   1845    step:     4669     combined loss: 2822.17804     paf loss 19.73200     hm loss 2802.44604\n",
      "Iteration:   1850    step:     4674     combined loss: 4735.30614     paf loss 22.51366     hm loss 4712.79248\n",
      "Iteration:   1855    step:     4679     combined loss: 4048.98545     paf loss 22.36399     hm loss 4026.62146\n",
      "Iteration:   1860    step:     4684     combined loss: 4214.59738     paf loss 19.98264     hm loss 4194.61475\n",
      "Iteration:   1865    step:     4689     combined loss: 3756.04601     paf loss 21.01269     hm loss 3735.03333\n",
      "Iteration:   1870    step:     4694     combined loss: 3614.29438     paf loss 19.41632     hm loss 3594.87805\n",
      "Iteration:   1875    step:     4699     combined loss: 3251.41526     paf loss 17.83274     hm loss 3233.58252\n",
      "Iteration:   1880    step:     4704     combined loss: 5456.92292     paf loss 19.53058     hm loss 5437.39233\n",
      "Iteration:   1885    step:     4709     combined loss: 3698.47383     paf loss 23.79743     hm loss 3674.67639\n",
      "Iteration:   1890    step:     4714     combined loss: 5481.07731     paf loss 20.27116     hm loss 5460.80615\n",
      "Iteration:   1895    step:     4719     combined loss: 2977.93805     paf loss 19.27240     hm loss 2958.66565\n",
      "Iteration:   1900    step:     4724     combined loss: 5047.60625     paf loss 26.81010     hm loss 5020.79614\n",
      "Iteration:   1905    step:     4729     combined loss: 4530.68385     paf loss 15.80446     hm loss 4514.87939\n",
      "Iteration:   1910    step:     4734     combined loss: 3694.27046     paf loss 18.69380     hm loss 3675.57666\n",
      "Iteration:   1915    step:     4739     combined loss: 4596.50464     paf loss 27.42627     hm loss 4569.07837\n",
      "Iteration:   1920    step:     4744     combined loss: 2323.71507     paf loss 19.34825     hm loss 2304.36682\n",
      "Iteration:   1925    step:     4749     combined loss: 3141.01594     paf loss 18.89509     hm loss 3122.12085\n",
      "Iteration:   1930    step:     4754     combined loss: 2441.77907     paf loss 16.02468     hm loss 2425.75439\n",
      "Iteration:   1935    step:     4759     combined loss: 5149.02262     paf loss 21.02140     hm loss 5128.00122\n",
      "Iteration:   1940    step:     4764     combined loss: 2426.52615     paf loss 16.10952     hm loss 2410.41663\n",
      "Iteration:   1945    step:     4769     combined loss: 4975.43391     paf loss 22.07404     hm loss 4953.35986\n",
      "Iteration:   1950    step:     4774     combined loss: 5240.33776     paf loss 22.82312     hm loss 5217.51465\n",
      "Iteration:   1955    step:     4779     combined loss: 3286.67485     paf loss 18.68425     hm loss 3267.99060\n",
      "Iteration:   1960    step:     4784     combined loss: 3313.91525     paf loss 14.81234     hm loss 3299.10291\n",
      "Iteration:   1965    step:     4789     combined loss: 3210.38742     paf loss 20.98703     hm loss 3189.40039\n",
      "Iteration:   1970    step:     4794     combined loss: 3406.38816     paf loss 19.54673     hm loss 3386.84143\n",
      "Iteration:   1975    step:     4799     combined loss: 4408.28384     paf loss 26.86074     hm loss 4381.42310\n",
      "Iteration:   1980    step:     4804     combined loss: 6308.70680     paf loss 21.60792     hm loss 6287.09888\n",
      "Iteration:   1985    step:     4809     combined loss: 4242.00667     paf loss 21.21907     hm loss 4220.78760\n",
      "Iteration:   1990    step:     4814     combined loss: 3086.15854     paf loss 16.47934     hm loss 3069.67920\n",
      "Iteration:   1995    step:     4819     combined loss: 3513.62550     paf loss 16.92055     hm loss 3496.70496\n",
      "Iteration:   2000    step:     4824     combined loss: 4320.66761     paf loss 20.36121     hm loss 4300.30640\n",
      "Iteration:   2005    step:     4829     combined loss: 3331.10151     paf loss 15.48334     hm loss 3315.61816\n",
      "Iteration:   2010    step:     4834     combined loss: 5534.07531     paf loss 28.06750     hm loss 5506.00781\n",
      "Iteration:   2015    step:     4839     combined loss: 5807.71207     paf loss 21.50529     hm loss 5786.20679\n",
      "Iteration:   2020    step:     4844     combined loss: 3691.40182     paf loss 26.67135     hm loss 3664.73047\n",
      "Iteration:   2025    step:     4849     combined loss: 4959.53148     paf loss 21.38719     hm loss 4938.14429\n",
      "Iteration:   2030    step:     4854     combined loss: 3488.26776     paf loss 17.17340     hm loss 3471.09436\n",
      "Iteration:   2035    step:     4859     combined loss: 4265.34244     paf loss 20.66007     hm loss 4244.68237\n",
      "Iteration:   2040    step:     4864     combined loss: 4621.12450     paf loss 17.18675     hm loss 4603.93774\n",
      "Iteration:   2045    step:     4869     combined loss: 2831.59175     paf loss 13.20332     hm loss 2818.38843\n",
      "Iteration:   2050    step:     4874     combined loss: 4799.61245     paf loss 20.42715     hm loss 4779.18530\n",
      "Iteration:   2055    step:     4879     combined loss: 2763.43010     paf loss 16.89275     hm loss 2746.53735\n",
      "Iteration:   2060    step:     4884     combined loss: 2777.69703     paf loss 17.64259     hm loss 2760.05444\n",
      "Iteration:   2065    step:     4889     combined loss: 3371.85905     paf loss 18.88957     hm loss 3352.96948\n",
      "Iteration:   2070    step:     4894     combined loss: 4391.94649     paf loss 24.00924     hm loss 4367.93726\n",
      "Iteration:   2075    step:     4899     combined loss: 5425.35554     paf loss 17.03156     hm loss 5408.32397\n",
      "Iteration:   2080    step:     4904     combined loss: 5215.77655     paf loss 23.25311     hm loss 5192.52344\n",
      "Iteration:   2085    step:     4909     combined loss: 5171.96294     paf loss 25.76690     hm loss 5146.19604\n",
      "Iteration:   2090    step:     4914     combined loss: 4642.07731     paf loss 21.07682     hm loss 4621.00049\n",
      "Iteration:   2095    step:     4919     combined loss: 3556.68071     paf loss 26.95414     hm loss 3529.72656\n",
      "Iteration:   2100    step:     4924     combined loss: 4847.74229     paf loss 20.69444     hm loss 4827.04785\n",
      "Iteration:   2105    step:     4929     combined loss: 5842.65330     paf loss 28.99046     hm loss 5813.66284\n",
      "Iteration:   2110    step:     4934     combined loss: 4090.13434     paf loss 21.30914     hm loss 4068.82520\n",
      "Iteration:   2115    step:     4939     combined loss: 2993.82152     paf loss 25.48632     hm loss 2968.33521\n",
      "Iteration:   2120    step:     4944     combined loss: 4907.52833     paf loss 23.21485     hm loss 4884.31348\n",
      "Iteration:   2125    step:     4949     combined loss: 7319.98042     paf loss 30.55024     hm loss 7289.43018\n",
      "Iteration:   2130    step:     4954     combined loss: 3803.74514     paf loss 18.32595     hm loss 3785.41919\n",
      "Iteration:   2135    step:     4959     combined loss: 3936.07255     paf loss 18.86552     hm loss 3917.20703\n",
      "Iteration:   2140    step:     4964     combined loss: 5507.85224     paf loss 20.67182     hm loss 5487.18042\n",
      "Iteration:   2145    step:     4969     combined loss: 4519.41779     paf loss 21.98273     hm loss 4497.43506\n",
      "Iteration:   2150    step:     4974     combined loss: 4408.60745     paf loss 17.57865     hm loss 4391.02881\n",
      "Iteration:   2155    step:     4979     combined loss: 5001.76444     paf loss 21.47415     hm loss 4980.29028\n",
      "Iteration:   2160    step:     4984     combined loss: 3148.92662     paf loss 17.50340     hm loss 3131.42322\n",
      "Iteration:   2165    step:     4989     combined loss: 4127.78509     paf loss 17.53875     hm loss 4110.24634\n",
      "Iteration:   2170    step:     4994     combined loss: 3476.57073     paf loss 21.11395     hm loss 3455.45679\n",
      "Iteration:   2175    step:     4999     combined loss: 4376.84260     paf loss 20.12556     hm loss 4356.71704\n",
      "Iteration:   2180    step:     5004     combined loss: 3280.79000     paf loss 20.15341     hm loss 3260.63660\n",
      "Iteration:   2185    step:     5009     combined loss: 2614.43538     paf loss 13.19221     hm loss 2601.24316\n",
      "Iteration:   2190    step:     5014     combined loss: 5353.53946     paf loss 22.92545     hm loss 5330.61401\n",
      "Iteration:   2195    step:     5019     combined loss: 5034.06958     paf loss 21.95777     hm loss 5012.11182\n",
      "Iteration:   2200    step:     5024     combined loss: 3192.66347     paf loss 18.15358     hm loss 3174.50989\n",
      "Iteration:   2205    step:     5029     combined loss: 4173.13214     paf loss 18.54815     hm loss 4154.58398\n",
      "Iteration:   2210    step:     5034     combined loss: 5736.15240     paf loss 31.57843     hm loss 5704.57397\n",
      "Iteration:   2215    step:     5039     combined loss: 4095.28200     paf loss 19.73025     hm loss 4075.55176\n",
      "Iteration:   2220    step:     5044     combined loss: 4311.21609     paf loss 15.17044     hm loss 4296.04565\n",
      "Iteration:   2225    step:     5049     combined loss: 3923.84017     paf loss 20.67794     hm loss 3903.16223\n",
      "Iteration:   2230    step:     5054     combined loss: 3669.83456     paf loss 20.24447     hm loss 3649.59009\n",
      "Iteration:   2235    step:     5059     combined loss: 3293.16523     paf loss 19.31367     hm loss 3273.85156\n",
      "Iteration:   2240    step:     5064     combined loss: 2716.60071     paf loss 13.29200     hm loss 2703.30872\n",
      "Iteration:   2245    step:     5069     combined loss: 4377.44459     paf loss 21.13990     hm loss 4356.30469\n",
      "Iteration:   2250    step:     5074     combined loss: 2912.40295     paf loss 18.04333     hm loss 2894.35962\n",
      "Iteration:   2255    step:     5079     combined loss: 5970.52124     paf loss 24.31421     hm loss 5946.20703\n",
      "Iteration:   2260    step:     5084     combined loss: 2549.89338     paf loss 15.04963     hm loss 2534.84375\n",
      "Iteration:   2265    step:     5089     combined loss: 2762.44288     paf loss 13.14283     hm loss 2749.30005\n",
      "Iteration:   2270    step:     5094     combined loss: 3930.50220     paf loss 19.02051     hm loss 3911.48169\n",
      "Iteration:   2275    step:     5099     combined loss: 3332.37534     paf loss 25.97251     hm loss 3306.40283\n",
      "Iteration:   2280    step:     5104     combined loss: 3498.29209     paf loss 21.45847     hm loss 3476.83362\n",
      "Iteration:   2285    step:     5109     combined loss: 2692.28937     paf loss 15.16083     hm loss 2677.12854\n",
      "Iteration:   2290    step:     5114     combined loss: 3646.94518     paf loss 16.65563     hm loss 3630.28955\n",
      "Iteration:   2295    step:     5119     combined loss: 3918.25719     paf loss 24.84020     hm loss 3893.41699\n",
      "Iteration:   2300    step:     5124     combined loss: 3403.03422     paf loss 17.24125     hm loss 3385.79297\n",
      "Iteration:   2305    step:     5129     combined loss: 3368.82501     paf loss 17.11115     hm loss 3351.71387\n",
      "Iteration:   2310    step:     5134     combined loss: 3778.25212     paf loss 21.39885     hm loss 3756.85327\n",
      "Iteration:   2315    step:     5139     combined loss: 2609.23267     paf loss 17.99402     hm loss 2591.23865\n",
      "Iteration:   2320    step:     5144     combined loss: 4794.25060     paf loss 21.61534     hm loss 4772.63525\n",
      "Iteration:   2325    step:     5149     combined loss: 3935.91272     paf loss 17.10034     hm loss 3918.81238\n",
      "Iteration:   2330    step:     5154     combined loss: 3643.59559     paf loss 15.85694     hm loss 3627.73865\n",
      "Iteration:   2335    step:     5159     combined loss: 2904.81235     paf loss 14.47446     hm loss 2890.33789\n",
      "Iteration:   2340    step:     5164     combined loss: 3157.32476     paf loss 21.18926     hm loss 3136.13550\n",
      "Iteration:   2345    step:     5169     combined loss: 3848.68878     paf loss 20.96930     hm loss 3827.71948\n",
      "Iteration:   2350    step:     5174     combined loss: 4811.04746     paf loss 23.17734     hm loss 4787.87012\n",
      "Iteration:   2355    step:     5179     combined loss: 4119.37806     paf loss 20.71571     hm loss 4098.66235\n",
      "Iteration:   2360    step:     5184     combined loss: 4978.50500     paf loss 19.60925     hm loss 4958.89575\n",
      "Iteration:   2365    step:     5189     combined loss: 3824.85029     paf loss 19.43659     hm loss 3805.41370\n",
      "Iteration:   2370    step:     5194     combined loss: 5644.10807     paf loss 14.12833     hm loss 5629.97974\n",
      "Iteration:   2375    step:     5199     combined loss: 5204.28799     paf loss 23.16201     hm loss 5181.12598\n",
      "Iteration:   2380    step:     5204     combined loss: 2730.90365     paf loss 15.08883     hm loss 2715.81482\n",
      "Iteration:   2385    step:     5209     combined loss: 3660.24335     paf loss 13.96918     hm loss 3646.27417\n",
      "Iteration:   2390    step:     5214     combined loss: 6320.68417     paf loss 18.33358     hm loss 6302.35059\n",
      "Iteration:   2395    step:     5219     combined loss: 3824.09284     paf loss 22.02375     hm loss 3802.06909\n",
      "Iteration:   2400    step:     5224     combined loss: 5143.76839     paf loss 20.71126     hm loss 5123.05713\n",
      "Iteration:   2405    step:     5229     combined loss: 5417.61777     paf loss 24.69272     hm loss 5392.92505\n",
      "Iteration:   2410    step:     5234     combined loss: 3991.37312     paf loss 15.55220     hm loss 3975.82092\n",
      "Iteration:   2415    step:     5239     combined loss: 3286.71514     paf loss 17.30681     hm loss 3269.40833\n",
      "Iteration:   2420    step:     5244     combined loss: 2525.19741     paf loss 19.67678     hm loss 2505.52063\n",
      "Iteration:   2425    step:     5249     combined loss: 3509.49528     paf loss 17.05070     hm loss 3492.44458\n",
      "Iteration:   2430    step:     5254     combined loss: 4793.02752     paf loss 25.56707     hm loss 4767.46045\n",
      "Iteration:   2435    step:     5259     combined loss: 4311.15419     paf loss 18.45082     hm loss 4292.70337\n",
      "Iteration:   2440    step:     5264     combined loss: 3645.99006     paf loss 22.32734     hm loss 3623.66272\n",
      "Iteration:   2445    step:     5269     combined loss: 3256.04073     paf loss 20.15926     hm loss 3235.88147\n",
      "Iteration:   2450    step:     5274     combined loss: 3581.81250     paf loss 14.67420     hm loss 3567.13831\n",
      "Iteration:   2455    step:     5279     combined loss: 4414.19988     paf loss 25.86907     hm loss 4388.33081\n",
      "Iteration:   2460    step:     5284     combined loss: 3414.53673     paf loss 20.61034     hm loss 3393.92639\n",
      "Iteration:   2465    step:     5289     combined loss: 3491.11915     paf loss 20.89772     hm loss 3470.22144\n",
      "Iteration:   2470    step:     5294     combined loss: 2744.53196     paf loss 15.94431     hm loss 2728.58765\n",
      "Iteration:   2475    step:     5299     combined loss: 3711.94500     paf loss 21.48211     hm loss 3690.46289\n",
      "Iteration:   2480    step:     5304     combined loss: 3706.24608     paf loss 16.41600     hm loss 3689.83008\n",
      "Iteration:   2485    step:     5309     combined loss: 5081.55450     paf loss 19.85528     hm loss 5061.69922\n",
      "Iteration:   2490    step:     5314     combined loss: 3316.02619     paf loss 13.50983     hm loss 3302.51636\n",
      "Iteration:   2495    step:     5319     combined loss: 5160.41773     paf loss 23.90796     hm loss 5136.50977\n",
      "Iteration:   2500    step:     5324     combined loss: 4901.41189     paf loss 23.92776     hm loss 4877.48413\n",
      "Iteration:   2505    step:     5329     combined loss: 3762.97778     paf loss 23.85827     hm loss 3739.11951\n",
      "Iteration:   2510    step:     5334     combined loss: 4186.33786     paf loss 21.04221     hm loss 4165.29565\n",
      "Iteration:   2515    step:     5339     combined loss: 3371.84027     paf loss 18.64886     hm loss 3353.19141\n",
      "Iteration:   2520    step:     5344     combined loss: 2962.02527     paf loss 17.45618     hm loss 2944.56909\n",
      "Iteration:   2525    step:     5349     combined loss: 3187.04337     paf loss 22.47623     hm loss 3164.56714\n",
      "Iteration:   2530    step:     5354     combined loss: 2661.12530     paf loss 17.44182     hm loss 2643.68347\n",
      "Iteration:   2535    step:     5359     combined loss: 5048.32027     paf loss 21.13594     hm loss 5027.18433\n",
      "Iteration:   2540    step:     5364     combined loss: 4184.22443     paf loss 17.48957     hm loss 4166.73486\n",
      "Iteration:   2545    step:     5369     combined loss: 2869.85609     paf loss 13.52991     hm loss 2856.32617\n",
      "Iteration:   2550    step:     5374     combined loss: 2948.55858     paf loss 12.45189     hm loss 2936.10669\n",
      "Iteration:   2555    step:     5379     combined loss: 3057.75920     paf loss 18.99187     hm loss 3038.76733\n",
      "Iteration:   2560    step:     5384     combined loss: 4125.05129     paf loss 23.13845     hm loss 4101.91284\n",
      "Iteration:   2565    step:     5389     combined loss: 5694.69120     paf loss 20.86771     hm loss 5673.82349\n",
      "Iteration:   2570    step:     5394     combined loss: 5505.79870     paf loss 20.90465     hm loss 5484.89404\n",
      "Iteration:   2575    step:     5399     combined loss: 3465.07989     paf loss 16.00994     hm loss 3449.06995\n",
      "Iteration:   2580    step:     5404     combined loss: 4243.66161     paf loss 26.06053     hm loss 4217.60107\n",
      "Iteration:   2585    step:     5409     combined loss: 4673.99400     paf loss 21.00206     hm loss 4652.99194\n",
      "Iteration:   2590    step:     5414     combined loss: 3890.36964     paf loss 21.13966     hm loss 3869.22998\n",
      "Iteration:   2595    step:     5419     combined loss: 4490.27606     paf loss 21.52484     hm loss 4468.75122\n",
      "Iteration:   2600    step:     5424     combined loss: 5922.10427     paf loss 25.69069     hm loss 5896.41357\n",
      "Iteration:   2605    step:     5429     combined loss: 3477.26060     paf loss 24.94065     hm loss 3452.31995\n",
      "Iteration:   2610    step:     5434     combined loss: 4438.23376     paf loss 22.46032     hm loss 4415.77344\n",
      "Iteration:   2615    step:     5439     combined loss: 4597.73065     paf loss 27.95990     hm loss 4569.77075\n",
      "Iteration:   2620    step:     5444     combined loss: 3834.82796     paf loss 14.72823     hm loss 3820.09973\n",
      "Iteration:   2625    step:     5449     combined loss: 3647.37176     paf loss 21.70807     hm loss 3625.66370\n",
      "Iteration:   2630    step:     5454     combined loss: 4275.11852     paf loss 22.08288     hm loss 4253.03564\n",
      "Iteration:   2635    step:     5459     combined loss: 4901.38383     paf loss 17.20976     hm loss 4884.17407\n",
      "Iteration:   2640    step:     5464     combined loss: 2611.14966     paf loss 16.24622     hm loss 2594.90344\n",
      "Iteration:   2645    step:     5469     combined loss: 3635.19266     paf loss 18.88577     hm loss 3616.30688\n",
      "Iteration:   2650    step:     5474     combined loss: 3137.10571     paf loss 23.05347     hm loss 3114.05225\n",
      "Iteration:   2655    step:     5479     combined loss: 3570.07764     paf loss 14.54895     hm loss 3555.52869\n",
      "Iteration:   2660    step:     5484     combined loss: 3921.33764     paf loss 18.07190     hm loss 3903.26575\n",
      "Iteration:   2665    step:     5489     combined loss: 4116.20990     paf loss 19.36066     hm loss 4096.84924\n",
      "Iteration:   2670    step:     5494     combined loss: 4891.30462     paf loss 24.94622     hm loss 4866.35840\n",
      "Iteration:   2675    step:     5499     combined loss: 4240.25201     paf loss 17.72735     hm loss 4222.52466\n",
      "Iteration:   2680    step:     5504     combined loss: 5216.58044     paf loss 24.16345     hm loss 5192.41699\n",
      "Iteration:   2685    step:     5509     combined loss: 3659.38987     paf loss 17.15696     hm loss 3642.23291\n",
      "Iteration:   2690    step:     5514     combined loss: 3519.03979     paf loss 20.22106     hm loss 3498.81873\n",
      "Iteration:   2695    step:     5519     combined loss: 3694.11462     paf loss 19.55322     hm loss 3674.56140\n",
      "Iteration:   2700    step:     5524     combined loss: 3487.29640     paf loss 19.80129     hm loss 3467.49512\n",
      "Iteration:   2705    step:     5529     combined loss: 5014.91284     paf loss 26.18433     hm loss 4988.72852\n",
      "Iteration:   2710    step:     5534     combined loss: 2964.72259     paf loss 13.10504     hm loss 2951.61755\n",
      "Iteration:   2715    step:     5539     combined loss: 3678.25119     paf loss 20.47446     hm loss 3657.77673\n",
      "Iteration:   2720    step:     5544     combined loss: 3050.12414     paf loss 15.78540     hm loss 3034.33875\n",
      "Iteration:   2725    step:     5549     combined loss: 4087.56928     paf loss 15.71845     hm loss 4071.85083\n",
      "Iteration:   2730    step:     5554     combined loss: 2755.74518     paf loss 16.18683     hm loss 2739.55835\n",
      "Iteration:   2735    step:     5559     combined loss: 4977.20051     paf loss 32.23152     hm loss 4944.96899\n",
      "Iteration:   2740    step:     5564     combined loss: 2409.29573     paf loss 18.33943     hm loss 2390.95630\n",
      "Iteration:   2745    step:     5569     combined loss: 3727.86886     paf loss 17.06320     hm loss 3710.80566\n",
      "Iteration:   2750    step:     5574     combined loss: 4431.79526     paf loss 24.83017     hm loss 4406.96509\n",
      "Iteration:   2755    step:     5579     combined loss: 4577.27081     paf loss 21.76910     hm loss 4555.50171\n",
      "Iteration:   2760    step:     5584     combined loss: 4062.04319     paf loss 23.70225     hm loss 4038.34094\n",
      "Iteration:   2765    step:     5589     combined loss: 2763.19148     paf loss 18.03974     hm loss 2745.15173\n",
      "Iteration:   2770    step:     5594     combined loss: 3329.66754     paf loss 20.92230     hm loss 3308.74524\n",
      "Iteration:   2775    step:     5599     combined loss: 3057.31571     paf loss 15.13468     hm loss 3042.18103\n",
      "Iteration:   2780    step:     5604     combined loss: 2975.93045     paf loss 18.67228     hm loss 2957.25818\n",
      "Iteration:   2785    step:     5609     combined loss: 3752.47257     paf loss 16.02445     hm loss 3736.44812\n",
      "Iteration:   2790    step:     5614     combined loss: 4242.05494     paf loss 18.70973     hm loss 4223.34521\n",
      "Iteration:   2795    step:     5619     combined loss: 3559.11290     paf loss 21.54295     hm loss 3537.56995\n",
      "Iteration:   2800    step:     5624     combined loss: 3817.22240     paf loss 22.17076     hm loss 3795.05164\n",
      "Iteration:   2805    step:     5629     combined loss: 3197.03538     paf loss 19.72801     hm loss 3177.30737\n",
      "Iteration:   2810    step:     5634     combined loss: 3755.67939     paf loss 20.50055     hm loss 3735.17883\n",
      "Iteration:   2815    step:     5639     combined loss: 3701.28974     paf loss 14.36579     hm loss 3686.92395\n",
      "Iteration:   2820    step:     5644     combined loss: 4233.92206     paf loss 17.96869     hm loss 4215.95337\n",
      "Train Loss: 4064.9189    PAF Loss:  20.2743    HM Loss:  4044.6446    Acc: NA\n",
      "Val Loss: 4976.0371    PAF Loss:  16.7818    HM Loss:  4959.2553     Acc: NA\n",
      "Epoch 2/9\n",
      "----------\n",
      "Iteration:      0    step:     5648     combined loss: 3035.17164     paf loss 19.41652     hm loss 3015.75513\n",
      "Iteration:      5    step:     5653     combined loss: 5412.87230     paf loss 26.55540     hm loss 5386.31689\n",
      "Iteration:     10    step:     5658     combined loss: 5555.13988     paf loss 15.98631     hm loss 5539.15356\n",
      "Iteration:     15    step:     5663     combined loss: 5773.61824     paf loss 24.00691     hm loss 5749.61133\n",
      "Iteration:     20    step:     5668     combined loss: 6075.82906     paf loss 31.64156     hm loss 6044.18750\n",
      "Iteration:     25    step:     5673     combined loss: 3679.18419     paf loss 19.55785     hm loss 3659.62634\n",
      "Iteration:     30    step:     5678     combined loss: 6094.68572     paf loss 17.59881     hm loss 6077.08691\n",
      "Iteration:     35    step:     5683     combined loss: 4861.24850     paf loss 23.49533     hm loss 4837.75317\n",
      "Iteration:     40    step:     5688     combined loss: 3957.79318     paf loss 12.60873     hm loss 3945.18445\n",
      "Iteration:     45    step:     5693     combined loss: 5149.64565     paf loss 27.41860     hm loss 5122.22705\n",
      "Iteration:     50    step:     5698     combined loss: 3739.08090     paf loss 24.90915     hm loss 3714.17175\n",
      "Iteration:     55    step:     5703     combined loss: 4295.17481     paf loss 17.49512     hm loss 4277.67969\n",
      "Iteration:     60    step:     5708     combined loss: 5403.94572     paf loss 24.03117     hm loss 5379.91455\n",
      "Iteration:     65    step:     5713     combined loss: 3312.83966     paf loss 14.60028     hm loss 3298.23938\n",
      "Iteration:     70    step:     5718     combined loss: 3209.72666     paf loss 14.71433     hm loss 3195.01233\n",
      "Iteration:     75    step:     5723     combined loss: 4749.76351     paf loss 18.23739     hm loss 4731.52612\n",
      "Iteration:     80    step:     5728     combined loss: 4815.88487     paf loss 19.36924     hm loss 4796.51562\n",
      "Iteration:     85    step:     5733     combined loss: 6054.50334     paf loss 23.09782     hm loss 6031.40552\n",
      "Iteration:     90    step:     5738     combined loss: 2546.96007     paf loss 17.49229     hm loss 2529.46777\n",
      "Iteration:     95    step:     5743     combined loss: 3792.25787     paf loss 18.60077     hm loss 3773.65710\n",
      "Iteration:    100    step:     5748     combined loss: 3821.57621     paf loss 25.87626     hm loss 3795.69995\n",
      "Iteration:    105    step:     5753     combined loss: 3822.91267     paf loss 19.99006     hm loss 3802.92261\n",
      "Iteration:    110    step:     5758     combined loss: 4304.17787     paf loss 17.60805     hm loss 4286.56982\n",
      "Iteration:    115    step:     5763     combined loss: 3871.66074     paf loss 23.51914     hm loss 3848.14160\n",
      "Iteration:    120    step:     5768     combined loss: 4814.20023     paf loss 19.41898     hm loss 4794.78125\n",
      "Iteration:    125    step:     5773     combined loss: 3221.53618     paf loss 15.40458     hm loss 3206.13159\n",
      "Iteration:    130    step:     5778     combined loss: 5136.68335     paf loss 21.74927     hm loss 5114.93408\n",
      "Iteration:    135    step:     5783     combined loss: 4320.25136     paf loss 20.30190     hm loss 4299.94946\n",
      "Iteration:    140    step:     5788     combined loss: 4960.32898     paf loss 22.07678     hm loss 4938.25220\n",
      "Iteration:    145    step:     5793     combined loss: 5438.19946     paf loss 19.28198     hm loss 5418.91748\n",
      "Iteration:    150    step:     5798     combined loss: 2788.67791     paf loss 17.95794     hm loss 2770.71997\n",
      "Iteration:    155    step:     5803     combined loss: 4113.40586     paf loss 28.41539     hm loss 4084.99048\n",
      "Iteration:    160    step:     5808     combined loss: 2919.70917     paf loss 15.17059     hm loss 2904.53857\n",
      "Iteration:    165    step:     5813     combined loss: 4011.88127     paf loss 26.63127     hm loss 3985.25000\n",
      "Iteration:    170    step:     5818     combined loss: 3413.06088     paf loss 17.45077     hm loss 3395.61011\n",
      "Iteration:    175    step:     5823     combined loss: 4492.20632     paf loss 21.25954     hm loss 4470.94678\n",
      "Iteration:    180    step:     5828     combined loss: 3840.03502     paf loss 13.63207     hm loss 3826.40295\n",
      "Iteration:    185    step:     5833     combined loss: 2991.81749     paf loss 17.87608     hm loss 2973.94141\n",
      "Iteration:    190    step:     5838     combined loss: 3489.18913     paf loss 22.59135     hm loss 3466.59778\n",
      "Iteration:    195    step:     5843     combined loss: 4437.36548     paf loss 23.43262     hm loss 4413.93286\n",
      "Iteration:    200    step:     5848     combined loss: 2254.87999     paf loss 17.20189     hm loss 2237.67810\n",
      "Iteration:    205    step:     5853     combined loss: 3461.98253     paf loss 17.17101     hm loss 3444.81152\n",
      "Iteration:    210    step:     5858     combined loss: 4053.95788     paf loss 26.91125     hm loss 4027.04663\n",
      "Iteration:    215    step:     5863     combined loss: 5002.28401     paf loss 21.25837     hm loss 4981.02563\n",
      "Iteration:    220    step:     5868     combined loss: 2608.43354     paf loss 16.22370     hm loss 2592.20984\n",
      "Iteration:    225    step:     5873     combined loss: 4839.34551     paf loss 23.44438     hm loss 4815.90112\n",
      "Iteration:    230    step:     5878     combined loss: 2921.72814     paf loss 18.83190     hm loss 2902.89624\n",
      "Iteration:    235    step:     5883     combined loss: 4012.94295     paf loss 20.80941     hm loss 3992.13354\n",
      "Iteration:    240    step:     5888     combined loss: 3483.06752     paf loss 18.88295     hm loss 3464.18457\n",
      "Iteration:    245    step:     5893     combined loss: 5012.81470     paf loss 23.57031     hm loss 4989.24438\n",
      "Iteration:    250    step:     5898     combined loss: 4028.17398     paf loss 18.61319     hm loss 4009.56079\n",
      "Iteration:    255    step:     5903     combined loss: 2764.90770     paf loss 18.94262     hm loss 2745.96509\n",
      "Iteration:    260    step:     5908     combined loss: 2866.68449     paf loss 21.77934     hm loss 2844.90515\n",
      "Iteration:    265    step:     5913     combined loss: 3095.58237     paf loss 12.58945     hm loss 3082.99292\n",
      "Iteration:    270    step:     5918     combined loss: 3689.35437     paf loss 20.89319     hm loss 3668.46118\n",
      "Iteration:    275    step:     5923     combined loss: 4679.45215     paf loss 21.93702     hm loss 4657.51514\n",
      "Iteration:    280    step:     5928     combined loss: 4057.38736     paf loss 23.88517     hm loss 4033.50220\n",
      "Iteration:    285    step:     5933     combined loss: 5149.29248     paf loss 22.96167     hm loss 5126.33081\n",
      "Iteration:    290    step:     5938     combined loss: 4531.01931     paf loss 23.82692     hm loss 4507.19238\n",
      "Iteration:    295    step:     5943     combined loss: 3325.27904     paf loss 21.37206     hm loss 3303.90698\n",
      "Iteration:    300    step:     5948     combined loss: 5305.82835     paf loss 23.31321     hm loss 5282.51514\n",
      "Iteration:    305    step:     5953     combined loss: 4490.30483     paf loss 22.05214     hm loss 4468.25269\n",
      "Iteration:    310    step:     5958     combined loss: 3709.50628     paf loss 20.45989     hm loss 3689.04639\n",
      "Iteration:    315    step:     5963     combined loss: 4502.21162     paf loss 21.46699     hm loss 4480.74463\n",
      "Iteration:    320    step:     5968     combined loss: 3082.90270     paf loss 20.66284     hm loss 3062.23987\n",
      "Iteration:    325    step:     5973     combined loss: 2520.06286     paf loss 19.05565     hm loss 2501.00720\n",
      "Iteration:    330    step:     5978     combined loss: 5653.57898     paf loss 23.88929     hm loss 5629.68970\n",
      "Iteration:    335    step:     5983     combined loss: 2919.92434     paf loss 21.93545     hm loss 2897.98889\n",
      "Iteration:    340    step:     5988     combined loss: 3442.81497     paf loss 15.75857     hm loss 3427.05640\n",
      "Iteration:    345    step:     5993     combined loss: 5288.61655     paf loss 29.53550     hm loss 5259.08105\n",
      "Iteration:    350    step:     5998     combined loss: 4914.58693     paf loss 19.38723     hm loss 4895.19971\n",
      "Iteration:    355    step:     6003     combined loss: 3050.44215     paf loss 18.57716     hm loss 3031.86499\n",
      "Iteration:    360    step:     6008     combined loss: 4060.73592     paf loss 28.46139     hm loss 4032.27454\n",
      "Iteration:    365    step:     6013     combined loss: 2357.43249     paf loss 17.10998     hm loss 2340.32251\n",
      "Iteration:    370    step:     6018     combined loss: 4947.39217     paf loss 24.58504     hm loss 4922.80713\n",
      "Iteration:    375    step:     6023     combined loss: 3848.66709     paf loss 19.51719     hm loss 3829.14990\n",
      "Iteration:    380    step:     6028     combined loss: 4621.95427     paf loss 19.50383     hm loss 4602.45044\n",
      "Iteration:    385    step:     6033     combined loss: 2858.87790     paf loss 15.10569     hm loss 2843.77222\n",
      "Iteration:    390    step:     6038     combined loss: 4556.26161     paf loss 22.75600     hm loss 4533.50562\n",
      "Iteration:    395    step:     6043     combined loss: 2631.64051     paf loss 18.78346     hm loss 2612.85706\n",
      "Iteration:    400    step:     6048     combined loss: 3402.76131     paf loss 11.89510     hm loss 3390.86621\n",
      "Iteration:    405    step:     6053     combined loss: 3920.30577     paf loss 15.65086     hm loss 3904.65491\n",
      "Iteration:    410    step:     6058     combined loss: 3405.59263     paf loss 19.36412     hm loss 3386.22852\n",
      "Iteration:    415    step:     6063     combined loss: 3856.27741     paf loss 21.29523     hm loss 3834.98218\n",
      "Iteration:    420    step:     6068     combined loss: 2776.92616     paf loss 17.71120     hm loss 2759.21497\n",
      "Iteration:    425    step:     6073     combined loss: 3124.43223     paf loss 14.87339     hm loss 3109.55884\n",
      "Iteration:    430    step:     6078     combined loss: 3033.86022     paf loss 15.94946     hm loss 3017.91077\n",
      "Iteration:    435    step:     6083     combined loss: 3162.54719     paf loss 17.21650     hm loss 3145.33069\n",
      "Iteration:    440    step:     6088     combined loss: 3234.08132     paf loss 20.93215     hm loss 3213.14917\n",
      "Iteration:    445    step:     6093     combined loss: 2633.20954     paf loss 14.90461     hm loss 2618.30493\n",
      "Iteration:    450    step:     6098     combined loss: 3219.73763     paf loss 19.24325     hm loss 3200.49438\n",
      "Iteration:    455    step:     6103     combined loss: 2825.83385     paf loss 20.37205     hm loss 2805.46179\n",
      "Iteration:    460    step:     6108     combined loss: 5119.65008     paf loss 24.71478     hm loss 5094.93530\n",
      "Iteration:    465    step:     6113     combined loss: 4723.70387     paf loss 22.84059     hm loss 4700.86328\n",
      "Iteration:    470    step:     6118     combined loss: 4541.95984     paf loss 26.73840     hm loss 4515.22144\n",
      "Iteration:    475    step:     6123     combined loss: 6391.06483     paf loss 26.33241     hm loss 6364.73242\n",
      "Iteration:    480    step:     6128     combined loss: 5462.58445     paf loss 24.64915     hm loss 5437.93530\n",
      "Iteration:    485    step:     6133     combined loss: 5386.56011     paf loss 27.42925     hm loss 5359.13086\n",
      "Iteration:    490    step:     6138     combined loss: 3238.40924     paf loss 17.23016     hm loss 3221.17908\n",
      "Iteration:    495    step:     6143     combined loss: 3946.93655     paf loss 19.69742     hm loss 3927.23914\n",
      "Iteration:    500    step:     6148     combined loss: 3633.59351     paf loss 25.62158     hm loss 3607.97192\n",
      "Iteration:    505    step:     6153     combined loss: 3966.49753     paf loss 23.77780     hm loss 3942.71973\n",
      "Iteration:    510    step:     6158     combined loss: 3919.99374     paf loss 23.40133     hm loss 3896.59241\n",
      "Iteration:    515    step:     6163     combined loss: 5429.97801     paf loss 32.17625     hm loss 5397.80176\n",
      "Iteration:    520    step:     6168     combined loss: 3498.78536     paf loss 24.93539     hm loss 3473.84998\n",
      "Iteration:    525    step:     6173     combined loss: 4907.28299     paf loss 24.03372     hm loss 4883.24927\n",
      "Iteration:    530    step:     6178     combined loss: 3183.66564     paf loss 18.85473     hm loss 3164.81091\n",
      "Iteration:    535    step:     6183     combined loss: 3164.45423     paf loss 19.47987     hm loss 3144.97437\n",
      "Iteration:    540    step:     6188     combined loss: 5337.27306     paf loss 21.40856     hm loss 5315.86450\n",
      "Iteration:    545    step:     6193     combined loss: 3007.77541     paf loss 18.08950     hm loss 2989.68591\n",
      "Iteration:    550    step:     6198     combined loss: 4938.82064     paf loss 22.89437     hm loss 4915.92627\n",
      "Iteration:    555    step:     6203     combined loss: 3651.89646     paf loss 18.49742     hm loss 3633.39905\n",
      "Iteration:    560    step:     6208     combined loss: 2765.72976     paf loss 17.35122     hm loss 2748.37854\n",
      "Iteration:    565    step:     6213     combined loss: 3564.66881     paf loss 18.01977     hm loss 3546.64905\n",
      "Iteration:    570    step:     6218     combined loss: 5061.79566     paf loss 24.93873     hm loss 5036.85693\n",
      "Iteration:    575    step:     6223     combined loss: 6126.35054     paf loss 21.81099     hm loss 6104.53955\n",
      "Iteration:    580    step:     6228     combined loss: 3880.77560     paf loss 19.57028     hm loss 3861.20532\n",
      "Iteration:    585    step:     6233     combined loss: 4138.55755     paf loss 24.05560     hm loss 4114.50195\n",
      "Iteration:    590    step:     6238     combined loss: 4190.51472     paf loss 23.84236     hm loss 4166.67236\n",
      "Iteration:    595    step:     6243     combined loss: 4131.05428     paf loss 16.76986     hm loss 4114.28442\n",
      "Iteration:    600    step:     6248     combined loss: 2865.60235     paf loss 17.52300     hm loss 2848.07935\n",
      "Iteration:    605    step:     6253     combined loss: 6245.58323     paf loss 24.54050     hm loss 6221.04272\n",
      "Iteration:    610    step:     6258     combined loss: 2884.02276     paf loss 19.47699     hm loss 2864.54578\n",
      "Iteration:    615    step:     6263     combined loss: 4464.19967     paf loss 28.88254     hm loss 4435.31714\n",
      "Iteration:    620    step:     6268     combined loss: 3126.69497     paf loss 20.50137     hm loss 3106.19360\n",
      "Iteration:    625    step:     6273     combined loss: 4185.83738     paf loss 20.54710     hm loss 4165.29028\n",
      "Iteration:    630    step:     6278     combined loss: 3890.30818     paf loss 16.52498     hm loss 3873.78320\n",
      "Iteration:    635    step:     6283     combined loss: 2958.52127     paf loss 21.80545     hm loss 2936.71582\n",
      "Iteration:    640    step:     6288     combined loss: 3615.57043     paf loss 19.43298     hm loss 3596.13745\n",
      "Iteration:    645    step:     6293     combined loss: 3503.70335     paf loss 15.43675     hm loss 3488.26660\n",
      "Iteration:    650    step:     6298     combined loss: 3006.72521     paf loss 22.86254     hm loss 2983.86267\n",
      "Iteration:    655    step:     6303     combined loss: 3477.53888     paf loss 18.37677     hm loss 3459.16211\n",
      "Iteration:    660    step:     6308     combined loss: 3717.88640     paf loss 17.48284     hm loss 3700.40356\n",
      "Iteration:    665    step:     6313     combined loss: 3696.35905     paf loss 17.79802     hm loss 3678.56104\n",
      "Iteration:    670    step:     6318     combined loss: 2963.32046     paf loss 18.50368     hm loss 2944.81677\n",
      "Iteration:    675    step:     6323     combined loss: 3898.99834     paf loss 20.00335     hm loss 3878.99500\n",
      "Iteration:    680    step:     6328     combined loss: 4343.28562     paf loss 28.63743     hm loss 4314.64819\n",
      "Iteration:    685    step:     6333     combined loss: 3845.44915     paf loss 21.31378     hm loss 3824.13538\n",
      "Iteration:    690    step:     6338     combined loss: 3432.23596     paf loss 19.46997     hm loss 3412.76599\n",
      "Iteration:    695    step:     6343     combined loss: 5633.16797     paf loss 28.69312     hm loss 5604.47485\n",
      "Iteration:    700    step:     6348     combined loss: 3644.56029     paf loss 18.54845     hm loss 3626.01184\n",
      "Iteration:    705    step:     6353     combined loss: 2557.20492     paf loss 16.31490     hm loss 2540.89001\n",
      "Iteration:    710    step:     6358     combined loss: 3510.96076     paf loss 16.16242     hm loss 3494.79834\n",
      "Iteration:    715    step:     6363     combined loss: 2695.82443     paf loss 18.87057     hm loss 2676.95386\n",
      "Iteration:    720    step:     6368     combined loss: 3429.82370     paf loss 19.67965     hm loss 3410.14404\n",
      "Iteration:    725    step:     6373     combined loss: 3230.37741     paf loss 13.47323     hm loss 3216.90417\n",
      "Iteration:    730    step:     6378     combined loss: 3034.61530     paf loss 21.48383     hm loss 3013.13147\n",
      "Iteration:    735    step:     6383     combined loss: 3501.61851     paf loss 21.75975     hm loss 3479.85876\n",
      "Iteration:    740    step:     6388     combined loss: 4003.18005     paf loss 18.24047     hm loss 3984.93958\n",
      "Iteration:    745    step:     6393     combined loss: 4571.93835     paf loss 22.14977     hm loss 4549.78857\n",
      "Iteration:    750    step:     6398     combined loss: 3026.32002     paf loss 13.92586     hm loss 3012.39417\n",
      "Iteration:    755    step:     6403     combined loss: 3714.73878     paf loss 21.47633     hm loss 3693.26245\n",
      "Iteration:    760    step:     6408     combined loss: 2864.49846     paf loss 18.75371     hm loss 2845.74475\n",
      "Iteration:    765    step:     6413     combined loss: 4526.49051     paf loss 26.71683     hm loss 4499.77368\n",
      "Iteration:    770    step:     6418     combined loss: 2771.25243     paf loss 15.63023     hm loss 2755.62219\n",
      "Iteration:    775    step:     6423     combined loss: 3854.53680     paf loss 20.43988     hm loss 3834.09692\n",
      "Iteration:    780    step:     6428     combined loss: 2622.78844     paf loss 20.66894     hm loss 2602.11951\n",
      "Iteration:    785    step:     6433     combined loss: 6661.97357     paf loss 22.24945     hm loss 6639.72412\n",
      "Iteration:    790    step:     6438     combined loss: 3649.26037     paf loss 21.88708     hm loss 3627.37329\n",
      "Iteration:    795    step:     6443     combined loss: 4013.23171     paf loss 15.43508     hm loss 3997.79663\n",
      "Iteration:    800    step:     6448     combined loss: 3598.49249     paf loss 25.12848     hm loss 3573.36401\n",
      "Iteration:    805    step:     6453     combined loss: 4629.37379     paf loss 24.63282     hm loss 4604.74097\n",
      "Iteration:    810    step:     6458     combined loss: 3362.20684     paf loss 18.82989     hm loss 3343.37695\n",
      "Iteration:    815    step:     6463     combined loss: 4520.29705     paf loss 23.52459     hm loss 4496.77246\n",
      "Iteration:    820    step:     6468     combined loss: 4404.75281     paf loss 19.68030     hm loss 4385.07251\n",
      "Iteration:    825    step:     6473     combined loss: 2983.15663     paf loss 17.86158     hm loss 2965.29504\n",
      "Iteration:    830    step:     6478     combined loss: 4299.65725     paf loss 25.09695     hm loss 4274.56030\n",
      "Iteration:    835    step:     6483     combined loss: 4388.14289     paf loss 21.44099     hm loss 4366.70190\n",
      "Iteration:    840    step:     6488     combined loss: 3923.34093     paf loss 19.97667     hm loss 3903.36426\n",
      "Iteration:    845    step:     6493     combined loss: 4534.65561     paf loss 21.75180     hm loss 4512.90381\n",
      "Iteration:    850    step:     6498     combined loss: 3217.54736     paf loss 22.97790     hm loss 3194.56946\n",
      "Iteration:    855    step:     6503     combined loss: 3134.51606     paf loss 21.23175     hm loss 3113.28430\n",
      "Iteration:    860    step:     6508     combined loss: 3794.81012     paf loss 19.30877     hm loss 3775.50134\n",
      "Iteration:    865    step:     6513     combined loss: 3436.65733     paf loss 20.39256     hm loss 3416.26477\n",
      "Iteration:    870    step:     6518     combined loss: 3576.34732     paf loss 18.54812     hm loss 3557.79919\n",
      "Iteration:    875    step:     6523     combined loss: 4776.89815     paf loss 24.66744     hm loss 4752.23071\n",
      "Iteration:    880    step:     6528     combined loss: 4480.06919     paf loss 21.91733     hm loss 4458.15186\n",
      "Iteration:    885    step:     6533     combined loss: 3660.16325     paf loss 19.05180     hm loss 3641.11145\n",
      "Iteration:    890    step:     6538     combined loss: 3795.36550     paf loss 21.57021     hm loss 3773.79529\n",
      "Iteration:    895    step:     6543     combined loss: 3766.43901     paf loss 20.60417     hm loss 3745.83484\n",
      "Iteration:    900    step:     6548     combined loss: 3972.40693     paf loss 18.33442     hm loss 3954.07251\n",
      "Iteration:    905    step:     6553     combined loss: 3686.83929     paf loss 24.66241     hm loss 3662.17688\n",
      "Iteration:    910    step:     6558     combined loss: 3837.51398     paf loss 18.77044     hm loss 3818.74353\n",
      "Iteration:    915    step:     6563     combined loss: 3168.98040     paf loss 16.85405     hm loss 3152.12634\n",
      "Iteration:    920    step:     6568     combined loss: 4441.82100     paf loss 27.00117     hm loss 4414.81982\n",
      "Iteration:    925    step:     6573     combined loss: 2728.21269     paf loss 18.27848     hm loss 2709.93420\n",
      "Iteration:    930    step:     6578     combined loss: 2608.26718     paf loss 22.92074     hm loss 2585.34644\n",
      "Iteration:    935    step:     6583     combined loss: 3809.19931     paf loss 19.98629     hm loss 3789.21301\n",
      "Iteration:    940    step:     6588     combined loss: 4420.22716     paf loss 20.15343     hm loss 4400.07373\n",
      "Iteration:    945    step:     6593     combined loss: 2595.04513     paf loss 16.44113     hm loss 2578.60400\n",
      "Iteration:    950    step:     6598     combined loss: 3501.38485     paf loss 18.55745     hm loss 3482.82739\n",
      "Iteration:    955    step:     6603     combined loss: 4128.71778     paf loss 18.56593     hm loss 4110.15186\n",
      "Iteration:    960    step:     6608     combined loss: 2572.93161     paf loss 15.63632     hm loss 2557.29529\n",
      "Iteration:    965    step:     6613     combined loss: 3945.16756     paf loss 19.58248     hm loss 3925.58508\n",
      "Iteration:    970    step:     6618     combined loss: 4579.76005     paf loss 17.74711     hm loss 4562.01294\n",
      "Iteration:    975    step:     6623     combined loss: 2387.10782     paf loss 16.35245     hm loss 2370.75537\n",
      "Iteration:    980    step:     6628     combined loss: 3590.41113     paf loss 20.98023     hm loss 3569.43091\n",
      "Iteration:    985    step:     6633     combined loss: 2790.19267     paf loss 19.82548     hm loss 2770.36719\n",
      "Iteration:    990    step:     6638     combined loss: 3840.05413     paf loss 21.31964     hm loss 3818.73450\n",
      "learning rate change: 0.00025 --> 0.000125\n",
      "Iteration:    995    step:     6643     combined loss: 2896.17484     paf loss 18.17142     hm loss 2878.00342\n",
      "Iteration:   1000    step:     6648     combined loss: 3494.53663     paf loss 17.45900     hm loss 3477.07764\n",
      "Iteration:   1005    step:     6653     combined loss: 3984.14621     paf loss 18.85813     hm loss 3965.28809\n",
      "Iteration:   1010    step:     6658     combined loss: 2634.61927     paf loss 15.67042     hm loss 2618.94885\n",
      "Iteration:   1015    step:     6663     combined loss: 3341.63805     paf loss 17.27355     hm loss 3324.36450\n",
      "Iteration:   1020    step:     6668     combined loss: 5775.25492     paf loss 24.12553     hm loss 5751.12939\n",
      "Iteration:   1025    step:     6673     combined loss: 3904.17490     paf loss 21.10117     hm loss 3883.07373\n",
      "Iteration:   1030    step:     6678     combined loss: 4169.78062     paf loss 21.01841     hm loss 4148.76221\n",
      "Iteration:   1035    step:     6683     combined loss: 3541.16842     paf loss 20.00814     hm loss 3521.16028\n",
      "Iteration:   1040    step:     6688     combined loss: 3188.39265     paf loss 19.44343     hm loss 3168.94922\n",
      "Iteration:   1045    step:     6693     combined loss: 4220.50863     paf loss 23.11165     hm loss 4197.39697\n",
      "Iteration:   1050    step:     6698     combined loss: 3776.15954     paf loss 20.06494     hm loss 3756.09460\n",
      "Iteration:   1055    step:     6703     combined loss: 4193.48878     paf loss 21.43678     hm loss 4172.05200\n",
      "Iteration:   1060    step:     6708     combined loss: 4032.73541     paf loss 16.88922     hm loss 4015.84619\n",
      "Iteration:   1065    step:     6713     combined loss: 5542.28625     paf loss 22.13806     hm loss 5520.14819\n",
      "Iteration:   1070    step:     6718     combined loss: 3008.94748     paf loss 20.98728     hm loss 2987.96021\n",
      "Iteration:   1075    step:     6723     combined loss: 4231.09270     paf loss 19.59514     hm loss 4211.49756\n",
      "Iteration:   1080    step:     6728     combined loss: 3554.72661     paf loss 12.69976     hm loss 3542.02686\n",
      "Iteration:   1085    step:     6733     combined loss: 5646.73861     paf loss 24.13265     hm loss 5622.60596\n",
      "Iteration:   1090    step:     6738     combined loss: 3829.06975     paf loss 18.29375     hm loss 3810.77600\n",
      "Iteration:   1095    step:     6743     combined loss: 3108.82277     paf loss 16.26381     hm loss 3092.55896\n",
      "Iteration:   1100    step:     6748     combined loss: 2816.99034     paf loss 21.58324     hm loss 2795.40710\n",
      "Iteration:   1105    step:     6753     combined loss: 5251.71963     paf loss 24.88149     hm loss 5226.83813\n",
      "Iteration:   1110    step:     6758     combined loss: 4297.43209     paf loss 22.57028     hm loss 4274.86182\n",
      "Iteration:   1115    step:     6763     combined loss: 3772.44830     paf loss 13.47882     hm loss 3758.96948\n",
      "Iteration:   1120    step:     6768     combined loss: 3826.24233     paf loss 21.59719     hm loss 3804.64514\n",
      "Iteration:   1125    step:     6773     combined loss: 3208.35766     paf loss 18.66247     hm loss 3189.69519\n",
      "Iteration:   1130    step:     6778     combined loss: 4826.52499     paf loss 17.95321     hm loss 4808.57178\n",
      "Iteration:   1135    step:     6783     combined loss: 2932.62989     paf loss 12.29102     hm loss 2920.33887\n",
      "Iteration:   1140    step:     6788     combined loss: 3777.85447     paf loss 25.08506     hm loss 3752.76941\n",
      "Iteration:   1145    step:     6793     combined loss: 3344.03402     paf loss 19.21591     hm loss 3324.81812\n",
      "Iteration:   1150    step:     6798     combined loss: 3673.41528     paf loss 20.86450     hm loss 3652.55078\n",
      "Iteration:   1155    step:     6803     combined loss: 2032.92006     paf loss 14.79170     hm loss 2018.12836\n",
      "Iteration:   1160    step:     6808     combined loss: 2516.03702     paf loss 16.94534     hm loss 2499.09167\n",
      "Iteration:   1165    step:     6813     combined loss: 3521.21157     paf loss 16.04360     hm loss 3505.16797\n",
      "Iteration:   1170    step:     6818     combined loss: 3615.22486     paf loss 18.18007     hm loss 3597.04480\n",
      "Iteration:   1175    step:     6823     combined loss: 3225.18244     paf loss 13.70954     hm loss 3211.47290\n",
      "Iteration:   1180    step:     6828     combined loss: 3477.77712     paf loss 21.91238     hm loss 3455.86475\n",
      "Iteration:   1185    step:     6833     combined loss: 2579.46772     paf loss 16.29487     hm loss 2563.17285\n",
      "Iteration:   1190    step:     6838     combined loss: 3628.60327     paf loss 16.52954     hm loss 3612.07373\n",
      "Iteration:   1195    step:     6843     combined loss: 3442.93759     paf loss 23.06137     hm loss 3419.87622\n",
      "Iteration:   1200    step:     6848     combined loss: 3279.69308     paf loss 17.39657     hm loss 3262.29651\n",
      "Iteration:   1205    step:     6853     combined loss: 4269.28735     paf loss 22.73218     hm loss 4246.55518\n",
      "Iteration:   1210    step:     6858     combined loss: 4069.91977     paf loss 21.28073     hm loss 4048.63904\n",
      "Iteration:   1215    step:     6863     combined loss: 3095.76706     paf loss 24.20322     hm loss 3071.56384\n",
      "Iteration:   1220    step:     6868     combined loss: 6457.77642     paf loss 30.43951     hm loss 6427.33691\n",
      "Iteration:   1225    step:     6873     combined loss: 3344.21526     paf loss 20.72246     hm loss 3323.49280\n",
      "Iteration:   1230    step:     6878     combined loss: 2752.18402     paf loss 12.78827     hm loss 2739.39575\n",
      "Iteration:   1235    step:     6883     combined loss: 3110.09953     paf loss 22.95378     hm loss 3087.14575\n",
      "Iteration:   1240    step:     6888     combined loss: 3080.24287     paf loss 20.22322     hm loss 3060.01965\n",
      "Iteration:   1245    step:     6893     combined loss: 3668.82765     paf loss 22.52272     hm loss 3646.30493\n",
      "Iteration:   1250    step:     6898     combined loss: 3963.12903     paf loss 24.33668     hm loss 3938.79236\n",
      "Iteration:   1255    step:     6903     combined loss: 3039.73015     paf loss 17.66924     hm loss 3022.06091\n",
      "Iteration:   1260    step:     6908     combined loss: 3356.97833     paf loss 20.68280     hm loss 3336.29553\n",
      "Iteration:   1265    step:     6913     combined loss: 3707.67951     paf loss 27.66486     hm loss 3680.01465\n",
      "Iteration:   1270    step:     6918     combined loss: 3357.22235     paf loss 15.87091     hm loss 3341.35144\n",
      "Iteration:   1275    step:     6923     combined loss: 2674.77290     paf loss 18.50642     hm loss 2656.26648\n",
      "Iteration:   1280    step:     6928     combined loss: 3096.76634     paf loss 22.08702     hm loss 3074.67932\n",
      "Iteration:   1285    step:     6933     combined loss: 5275.13296     paf loss 19.07437     hm loss 5256.05859\n",
      "Iteration:   1290    step:     6938     combined loss: 4446.07058     paf loss 17.22634     hm loss 4428.84424\n",
      "Iteration:   1295    step:     6943     combined loss: 3094.68929     paf loss 20.25154     hm loss 3074.43774\n",
      "Iteration:   1300    step:     6948     combined loss: 4501.19026     paf loss 23.15779     hm loss 4478.03247\n",
      "Iteration:   1305    step:     6953     combined loss: 2999.18359     paf loss 23.31432     hm loss 2975.86926\n",
      "Iteration:   1310    step:     6958     combined loss: 2997.25807     paf loss 16.01137     hm loss 2981.24670\n",
      "Iteration:   1315    step:     6963     combined loss: 4038.02429     paf loss 13.45825     hm loss 4024.56604\n",
      "Iteration:   1320    step:     6968     combined loss: 6476.76325     paf loss 17.43244     hm loss 6459.33081\n",
      "Iteration:   1325    step:     6973     combined loss: 3766.57473     paf loss 22.74623     hm loss 3743.82849\n",
      "Iteration:   1330    step:     6978     combined loss: 4922.19883     paf loss 25.61070     hm loss 4896.58813\n",
      "Iteration:   1335    step:     6983     combined loss: 3887.24741     paf loss 21.52757     hm loss 3865.71985\n",
      "Iteration:   1340    step:     6988     combined loss: 3444.67931     paf loss 20.20933     hm loss 3424.46997\n",
      "Iteration:   1345    step:     6993     combined loss: 2787.84768     paf loss 20.81887     hm loss 2767.02881\n",
      "Iteration:   1350    step:     6998     combined loss: 2402.61721     paf loss 14.63992     hm loss 2387.97729\n",
      "Iteration:   1355    step:     7003     combined loss: 4693.08115     paf loss 24.06137     hm loss 4669.01978\n",
      "Iteration:   1360    step:     7008     combined loss: 3360.67734     paf loss 17.06845     hm loss 3343.60889\n",
      "Iteration:   1365    step:     7013     combined loss: 3409.68766     paf loss 17.72111     hm loss 3391.96655\n",
      "Iteration:   1370    step:     7018     combined loss: 3528.49931     paf loss 17.99748     hm loss 3510.50183\n",
      "Iteration:   1375    step:     7023     combined loss: 3459.49390     paf loss 17.24646     hm loss 3442.24744\n",
      "Iteration:   1380    step:     7028     combined loss: 3180.33789     paf loss 22.90942     hm loss 3157.42847\n",
      "Iteration:   1385    step:     7033     combined loss: 3919.82547     paf loss 19.89456     hm loss 3899.93091\n",
      "Iteration:   1390    step:     7038     combined loss: 3452.92907     paf loss 23.73962     hm loss 3429.18945\n",
      "Iteration:   1395    step:     7043     combined loss: 5560.01434     paf loss 26.81073     hm loss 5533.20361\n",
      "Iteration:   1400    step:     7048     combined loss: 2691.92404     paf loss 15.70285     hm loss 2676.22119\n",
      "Iteration:   1405    step:     7053     combined loss: 4616.16680     paf loss 18.16875     hm loss 4597.99805\n",
      "Iteration:   1410    step:     7058     combined loss: 3265.74071     paf loss 18.79112     hm loss 3246.94958\n",
      "Iteration:   1415    step:     7063     combined loss: 4095.63281     paf loss 21.91724     hm loss 4073.71558\n",
      "Iteration:   1420    step:     7068     combined loss: 4379.31883     paf loss 22.03758     hm loss 4357.28125\n",
      "Iteration:   1425    step:     7073     combined loss: 3057.40164     paf loss 20.85135     hm loss 3036.55029\n",
      "Iteration:   1430    step:     7078     combined loss: 2900.15504     paf loss 12.77772     hm loss 2887.37732\n",
      "Iteration:   1435    step:     7083     combined loss: 3580.46502     paf loss 22.65508     hm loss 3557.80994\n",
      "Iteration:   1440    step:     7088     combined loss: 3098.15988     paf loss 26.46823     hm loss 3071.69165\n",
      "Iteration:   1445    step:     7093     combined loss: 4743.22146     paf loss 21.33572     hm loss 4721.88574\n",
      "Iteration:   1450    step:     7098     combined loss: 4924.84458     paf loss 21.12754     hm loss 4903.71704\n",
      "Iteration:   1455    step:     7103     combined loss: 6865.55103     paf loss 18.73682     hm loss 6846.81421\n",
      "Iteration:   1460    step:     7108     combined loss: 4707.89173     paf loss 21.59656     hm loss 4686.29517\n",
      "Iteration:   1465    step:     7113     combined loss: 3498.76236     paf loss 21.82230     hm loss 3476.94006\n",
      "Iteration:   1470    step:     7118     combined loss: 3001.01098     paf loss 19.10376     hm loss 2981.90723\n",
      "Iteration:   1475    step:     7123     combined loss: 2551.61023     paf loss 17.07190     hm loss 2534.53833\n",
      "Iteration:   1480    step:     7128     combined loss: 5056.05337     paf loss 20.45888     hm loss 5035.59448\n",
      "Iteration:   1485    step:     7133     combined loss: 3041.21526     paf loss 18.21770     hm loss 3022.99756\n",
      "Iteration:   1490    step:     7138     combined loss: 4081.82181     paf loss 16.05008     hm loss 4065.77173\n",
      "Iteration:   1495    step:     7143     combined loss: 2740.06491     paf loss 12.20615     hm loss 2727.85876\n",
      "Iteration:   1500    step:     7148     combined loss: 5247.04283     paf loss 23.54454     hm loss 5223.49829\n",
      "Iteration:   1505    step:     7153     combined loss: 5598.86803     paf loss 25.91491     hm loss 5572.95312\n",
      "Iteration:   1510    step:     7158     combined loss: 3547.12836     paf loss 17.12713     hm loss 3530.00122\n",
      "Iteration:   1515    step:     7163     combined loss: 6694.53377     paf loss 17.45443     hm loss 6677.07935\n",
      "Iteration:   1520    step:     7168     combined loss: 3138.95461     paf loss 19.50124     hm loss 3119.45337\n",
      "Iteration:   1525    step:     7173     combined loss: 3674.35358     paf loss 18.64215     hm loss 3655.71143\n",
      "Iteration:   1530    step:     7178     combined loss: 3128.45294     paf loss 22.71734     hm loss 3105.73560\n",
      "Iteration:   1535    step:     7183     combined loss: 3751.04884     paf loss 25.82874     hm loss 3725.22009\n",
      "Iteration:   1540    step:     7188     combined loss: 4544.44573     paf loss 25.95892     hm loss 4518.48682\n",
      "Iteration:   1545    step:     7193     combined loss: 3437.23234     paf loss 12.17570     hm loss 3425.05664\n",
      "Iteration:   1550    step:     7198     combined loss: 3952.06481     paf loss 18.40490     hm loss 3933.65991\n",
      "Iteration:   1555    step:     7203     combined loss: 3048.44746     paf loss 21.90877     hm loss 3026.53870\n",
      "Iteration:   1560    step:     7208     combined loss: 3180.06996     paf loss 13.80543     hm loss 3166.26453\n",
      "Iteration:   1565    step:     7213     combined loss: 3711.29322     paf loss 20.36830     hm loss 3690.92493\n",
      "Iteration:   1570    step:     7218     combined loss: 5909.52840     paf loss 20.85408     hm loss 5888.67432\n",
      "Iteration:   1575    step:     7223     combined loss: 3465.12172     paf loss 15.55616     hm loss 3449.56555\n",
      "Iteration:   1580    step:     7228     combined loss: 5403.25624     paf loss 22.42152     hm loss 5380.83472\n",
      "Iteration:   1585    step:     7233     combined loss: 3758.36308     paf loss 19.68095     hm loss 3738.68213\n",
      "Iteration:   1590    step:     7238     combined loss: 2853.47367     paf loss 20.41703     hm loss 2833.05664\n",
      "Iteration:   1595    step:     7243     combined loss: 3615.09523     paf loss 24.79897     hm loss 3590.29626\n",
      "Iteration:   1600    step:     7248     combined loss: 2737.65547     paf loss 20.29878     hm loss 2717.35669\n",
      "Iteration:   1605    step:     7253     combined loss: 4366.57230     paf loss 23.45828     hm loss 4343.11401\n",
      "Iteration:   1610    step:     7258     combined loss: 2971.82879     paf loss 15.22295     hm loss 2956.60583\n",
      "Iteration:   1615    step:     7263     combined loss: 4177.01068     paf loss 23.90496     hm loss 4153.10571\n",
      "Iteration:   1620    step:     7268     combined loss: 2669.80017     paf loss 18.84656     hm loss 2650.95361\n",
      "Iteration:   1625    step:     7273     combined loss: 5268.75333     paf loss 27.16764     hm loss 5241.58569\n",
      "Iteration:   1630    step:     7278     combined loss: 4303.02304     paf loss 23.49863     hm loss 4279.52441\n",
      "Iteration:   1635    step:     7283     combined loss: 3662.84695     paf loss 18.98428     hm loss 3643.86267\n",
      "Iteration:   1640    step:     7288     combined loss: 3532.20197     paf loss 16.85053     hm loss 3515.35144\n",
      "Iteration:   1645    step:     7293     combined loss: 5494.42560     paf loss 22.91119     hm loss 5471.51440\n",
      "Iteration:   1650    step:     7298     combined loss: 4246.44248     paf loss 20.40073     hm loss 4226.04175\n",
      "Iteration:   1655    step:     7303     combined loss: 4240.73608     paf loss 23.83203     hm loss 4216.90405\n",
      "Iteration:   1660    step:     7308     combined loss: 2975.00366     paf loss 16.20117     hm loss 2958.80249\n",
      "Iteration:   1665    step:     7313     combined loss: 3807.59323     paf loss 19.41232     hm loss 3788.18091\n",
      "Iteration:   1670    step:     7318     combined loss: 3234.69813     paf loss 18.18982     hm loss 3216.50830\n",
      "Iteration:   1675    step:     7323     combined loss: 4925.77876     paf loss 21.28901     hm loss 4904.48975\n",
      "Iteration:   1680    step:     7328     combined loss: 3615.69203     paf loss 18.40284     hm loss 3597.28918\n",
      "Iteration:   1685    step:     7333     combined loss: 3708.38308     paf loss 22.14614     hm loss 3686.23694\n",
      "Iteration:   1690    step:     7338     combined loss: 3449.68556     paf loss 21.11928     hm loss 3428.56628\n",
      "Iteration:   1695    step:     7343     combined loss: 5038.13071     paf loss 17.37949     hm loss 5020.75122\n",
      "Iteration:   1700    step:     7348     combined loss: 3655.69612     paf loss 21.97676     hm loss 3633.71936\n",
      "Iteration:   1705    step:     7353     combined loss: 2631.77407     paf loss 20.05495     hm loss 2611.71912\n",
      "Iteration:   1710    step:     7358     combined loss: 2082.05646     paf loss 14.53155     hm loss 2067.52490\n",
      "Iteration:   1715    step:     7363     combined loss: 3244.78476     paf loss 21.11093     hm loss 3223.67383\n",
      "Iteration:   1720    step:     7368     combined loss: 5288.18166     paf loss 17.98757     hm loss 5270.19409\n",
      "Iteration:   1725    step:     7373     combined loss: 2694.10681     paf loss 15.93762     hm loss 2678.16919\n",
      "Iteration:   1730    step:     7378     combined loss: 4542.81687     paf loss 18.65525     hm loss 4524.16162\n",
      "Iteration:   1735    step:     7383     combined loss: 3075.05731     paf loss 21.00506     hm loss 3054.05225\n",
      "Iteration:   1740    step:     7388     combined loss: 2675.41502     paf loss 16.59410     hm loss 2658.82092\n",
      "Iteration:   1745    step:     7393     combined loss: 2862.00681     paf loss 17.74668     hm loss 2844.26013\n",
      "Iteration:   1750    step:     7398     combined loss: 4081.88091     paf loss 23.11394     hm loss 4058.76697\n",
      "Iteration:   1755    step:     7403     combined loss: 3598.70051     paf loss 24.99262     hm loss 3573.70789\n",
      "Iteration:   1760    step:     7408     combined loss: 4724.79418     paf loss 15.66796     hm loss 4709.12622\n",
      "Iteration:   1765    step:     7413     combined loss: 4087.41780     paf loss 20.91743     hm loss 4066.50037\n",
      "Iteration:   1770    step:     7418     combined loss: 3558.92255     paf loss 21.89143     hm loss 3537.03113\n",
      "Iteration:   1775    step:     7423     combined loss: 3626.90485     paf loss 18.40119     hm loss 3608.50366\n",
      "Iteration:   1780    step:     7428     combined loss: 2705.92789     paf loss 15.12503     hm loss 2690.80286\n",
      "Iteration:   1785    step:     7433     combined loss: 2307.30285     paf loss 13.51879     hm loss 2293.78406\n",
      "Iteration:   1790    step:     7438     combined loss: 2853.10845     paf loss 16.79351     hm loss 2836.31494\n",
      "Iteration:   1795    step:     7443     combined loss: 5384.71893     paf loss 21.91741     hm loss 5362.80151\n",
      "Iteration:   1800    step:     7448     combined loss: 5058.01414     paf loss 20.93626     hm loss 5037.07788\n",
      "Iteration:   1805    step:     7453     combined loss: 4145.43995     paf loss 19.12696     hm loss 4126.31299\n",
      "Iteration:   1810    step:     7458     combined loss: 2935.60880     paf loss 15.40189     hm loss 2920.20691\n",
      "Iteration:   1815    step:     7463     combined loss: 2830.27025     paf loss 19.59630     hm loss 2810.67395\n",
      "Iteration:   1820    step:     7468     combined loss: 3509.35402     paf loss 17.41933     hm loss 3491.93469\n",
      "Iteration:   1825    step:     7473     combined loss: 5115.90740     paf loss 21.43279     hm loss 5094.47461\n",
      "Iteration:   1830    step:     7478     combined loss: 4827.28095     paf loss 23.82318     hm loss 4803.45776\n",
      "Iteration:   1835    step:     7483     combined loss: 3056.11197     paf loss 19.66019     hm loss 3036.45178\n",
      "Iteration:   1840    step:     7488     combined loss: 4322.81043     paf loss 19.15101     hm loss 4303.65942\n",
      "Iteration:   1845    step:     7493     combined loss: 4735.78957     paf loss 22.99099     hm loss 4712.79858\n",
      "Iteration:   1850    step:     7498     combined loss: 3638.83719     paf loss 19.98526     hm loss 3618.85193\n",
      "Iteration:   1855    step:     7503     combined loss: 4568.73738     paf loss 21.91731     hm loss 4546.82007\n",
      "Iteration:   1860    step:     7508     combined loss: 4306.80633     paf loss 28.13152     hm loss 4278.67480\n",
      "Iteration:   1865    step:     7513     combined loss: 4622.32380     paf loss 19.89411     hm loss 4602.42969\n",
      "Iteration:   1870    step:     7518     combined loss: 3655.14285     paf loss 20.18679     hm loss 3634.95605\n",
      "Iteration:   1875    step:     7523     combined loss: 4340.86552     paf loss 19.87309     hm loss 4320.99243\n",
      "Iteration:   1880    step:     7528     combined loss: 3727.31327     paf loss 18.89823     hm loss 3708.41504\n",
      "Iteration:   1885    step:     7533     combined loss: 4003.31912     paf loss 22.57583     hm loss 3980.74329\n",
      "Iteration:   1890    step:     7538     combined loss: 4641.01144     paf loss 23.05685     hm loss 4617.95459\n",
      "Iteration:   1895    step:     7543     combined loss: 3428.29889     paf loss 16.36468     hm loss 3411.93420\n",
      "Iteration:   1900    step:     7548     combined loss: 4539.15806     paf loss 22.66636     hm loss 4516.49170\n",
      "Iteration:   1905    step:     7553     combined loss: 3330.83911     paf loss 22.52844     hm loss 3308.31067\n",
      "Iteration:   1910    step:     7558     combined loss: 3446.95995     paf loss 20.72374     hm loss 3426.23621\n",
      "Iteration:   1915    step:     7563     combined loss: 3362.43364     paf loss 16.13164     hm loss 3346.30200\n",
      "Iteration:   1920    step:     7568     combined loss: 4211.22029     paf loss 18.39461     hm loss 4192.82568\n",
      "Iteration:   1925    step:     7573     combined loss: 3844.63446     paf loss 23.19793     hm loss 3821.43652\n",
      "Iteration:   1930    step:     7578     combined loss: 2702.67313     paf loss 21.03995     hm loss 2681.63318\n",
      "Iteration:   1935    step:     7583     combined loss: 3411.61607     paf loss 17.80564     hm loss 3393.81042\n",
      "Iteration:   1940    step:     7588     combined loss: 4783.94397     paf loss 21.98499     hm loss 4761.95898\n",
      "Iteration:   1945    step:     7593     combined loss: 3045.78918     paf loss 19.23254     hm loss 3026.55664\n",
      "Iteration:   1950    step:     7598     combined loss: 2957.48121     paf loss 16.91407     hm loss 2940.56714\n",
      "Iteration:   1955    step:     7603     combined loss: 3309.65040     paf loss 16.78968     hm loss 3292.86072\n",
      "Iteration:   1960    step:     7608     combined loss: 3651.93262     paf loss 22.01977     hm loss 3629.91284\n",
      "Iteration:   1965    step:     7613     combined loss: 3693.94305     paf loss 16.47137     hm loss 3677.47168\n",
      "Iteration:   1970    step:     7618     combined loss: 3895.57629     paf loss 19.35863     hm loss 3876.21765\n",
      "Iteration:   1975    step:     7623     combined loss: 3465.42565     paf loss 21.43053     hm loss 3443.99512\n",
      "Iteration:   1980    step:     7628     combined loss: 4009.40261     paf loss 27.15554     hm loss 3982.24707\n",
      "Iteration:   1985    step:     7633     combined loss: 3461.60458     paf loss 17.10495     hm loss 3444.49963\n",
      "Iteration:   1990    step:     7638     combined loss: 2570.99107     paf loss 19.64647     hm loss 2551.34460\n",
      "Iteration:   1995    step:     7643     combined loss: 4795.05336     paf loss 22.29628     hm loss 4772.75708\n",
      "Iteration:   2000    step:     7648     combined loss: 2845.14122     paf loss 22.97374     hm loss 2822.16748\n",
      "Iteration:   2005    step:     7653     combined loss: 2741.98886     paf loss 20.73947     hm loss 2721.24939\n",
      "Iteration:   2010    step:     7658     combined loss: 3572.73460     paf loss 18.10581     hm loss 3554.62878\n",
      "Iteration:   2015    step:     7663     combined loss: 4155.06588     paf loss 14.52950     hm loss 4140.53638\n",
      "Iteration:   2020    step:     7668     combined loss: 3313.88074     paf loss 10.94666     hm loss 3302.93408\n",
      "Iteration:   2025    step:     7673     combined loss: 3711.23819     paf loss 22.04861     hm loss 3689.18958\n",
      "Iteration:   2030    step:     7678     combined loss: 3183.45206     paf loss 21.27665     hm loss 3162.17542\n",
      "Iteration:   2035    step:     7683     combined loss: 5072.45330     paf loss 26.58807     hm loss 5045.86523\n",
      "Iteration:   2040    step:     7688     combined loss: 2565.96772     paf loss 16.46186     hm loss 2549.50586\n",
      "Iteration:   2045    step:     7693     combined loss: 3126.57463     paf loss 16.81865     hm loss 3109.75598\n",
      "Iteration:   2050    step:     7698     combined loss: 5511.92809     paf loss 19.78478     hm loss 5492.14331\n",
      "Iteration:   2055    step:     7703     combined loss: 4817.38130     paf loss 22.07588     hm loss 4795.30542\n",
      "Iteration:   2060    step:     7708     combined loss: 4283.39732     paf loss 29.36851     hm loss 4254.02881\n",
      "Iteration:   2065    step:     7713     combined loss: 3153.15959     paf loss 15.50627     hm loss 3137.65332\n",
      "Iteration:   2070    step:     7718     combined loss: 3755.39264     paf loss 21.11969     hm loss 3734.27295\n",
      "Iteration:   2075    step:     7723     combined loss: 3763.99724     paf loss 23.23149     hm loss 3740.76575\n",
      "Iteration:   2080    step:     7728     combined loss: 4903.00477     paf loss 27.62025     hm loss 4875.38452\n",
      "Iteration:   2085    step:     7733     combined loss: 3545.10575     paf loss 21.77470     hm loss 3523.33105\n",
      "Iteration:   2090    step:     7738     combined loss: 4778.11070     paf loss 27.99425     hm loss 4750.11646\n",
      "Iteration:   2095    step:     7743     combined loss: 4300.71967     paf loss 14.40082     hm loss 4286.31885\n",
      "Iteration:   2100    step:     7748     combined loss: 3245.77199     paf loss 16.82789     hm loss 3228.94409\n",
      "Iteration:   2105    step:     7753     combined loss: 2982.64816     paf loss 18.86007     hm loss 2963.78809\n",
      "Iteration:   2110    step:     7758     combined loss: 4008.82032     paf loss 23.42664     hm loss 3985.39368\n",
      "Iteration:   2115    step:     7763     combined loss: 3588.74110     paf loss 22.27650     hm loss 3566.46460\n",
      "Iteration:   2120    step:     7768     combined loss: 2194.26868     paf loss 17.41748     hm loss 2176.85120\n",
      "Iteration:   2125    step:     7773     combined loss: 3109.29053     paf loss 18.83839     hm loss 3090.45215\n",
      "Iteration:   2130    step:     7778     combined loss: 3822.04489     paf loss 16.55222     hm loss 3805.49268\n",
      "Iteration:   2135    step:     7783     combined loss: 4058.72489     paf loss 14.92643     hm loss 4043.79846\n",
      "Iteration:   2140    step:     7788     combined loss: 4059.88119     paf loss 23.66573     hm loss 4036.21545\n",
      "Iteration:   2145    step:     7793     combined loss: 3299.72047     paf loss 22.05714     hm loss 3277.66333\n",
      "Iteration:   2150    step:     7798     combined loss: 3167.77587     paf loss 21.94200     hm loss 3145.83386\n",
      "Iteration:   2155    step:     7803     combined loss: 3696.20253     paf loss 18.83913     hm loss 3677.36340\n",
      "Iteration:   2160    step:     7808     combined loss: 3827.99821     paf loss 17.79118     hm loss 3810.20703\n",
      "Iteration:   2165    step:     7813     combined loss: 3331.76937     paf loss 16.32906     hm loss 3315.44031\n",
      "Iteration:   2170    step:     7818     combined loss: 3625.70302     paf loss 20.54457     hm loss 3605.15845\n",
      "Iteration:   2175    step:     7823     combined loss: 3815.39843     paf loss 21.85412     hm loss 3793.54431\n",
      "Iteration:   2180    step:     7828     combined loss: 3445.56100     paf loss 18.48360     hm loss 3427.07739\n",
      "Iteration:   2185    step:     7833     combined loss: 3976.13693     paf loss 23.99655     hm loss 3952.14038\n",
      "Iteration:   2190    step:     7838     combined loss: 4273.17634     paf loss 19.43465     hm loss 4253.74170\n",
      "Iteration:   2195    step:     7843     combined loss: 2734.44506     paf loss 13.24475     hm loss 2721.20032\n",
      "Iteration:   2200    step:     7848     combined loss: 3177.26279     paf loss 19.09238     hm loss 3158.17041\n",
      "Iteration:   2205    step:     7853     combined loss: 4304.70586     paf loss 26.27470     hm loss 4278.43115\n",
      "Iteration:   2210    step:     7858     combined loss: 3436.88741     paf loss 19.29500     hm loss 3417.59241\n",
      "Iteration:   2215    step:     7863     combined loss: 3733.37040     paf loss 21.40885     hm loss 3711.96155\n",
      "Iteration:   2220    step:     7868     combined loss: 3066.38643     paf loss 18.26912     hm loss 3048.11731\n",
      "Iteration:   2225    step:     7873     combined loss: 2524.41095     paf loss 13.90351     hm loss 2510.50745\n",
      "Iteration:   2230    step:     7878     combined loss: 2895.41603     paf loss 15.05251     hm loss 2880.36353\n",
      "Iteration:   2235    step:     7883     combined loss: 2818.35291     paf loss 17.88269     hm loss 2800.47021\n",
      "Iteration:   2240    step:     7888     combined loss: 3648.54122     paf loss 19.57516     hm loss 3628.96606\n",
      "Iteration:   2245    step:     7893     combined loss: 4316.16322     paf loss 21.96230     hm loss 4294.20093\n",
      "Iteration:   2250    step:     7898     combined loss: 5445.83281     paf loss 19.89287     hm loss 5425.93994\n",
      "Iteration:   2255    step:     7903     combined loss: 5796.97014     paf loss 33.09807     hm loss 5763.87207\n",
      "Iteration:   2260    step:     7908     combined loss: 2939.15668     paf loss 16.21918     hm loss 2922.93750\n",
      "Iteration:   2265    step:     7913     combined loss: 3990.64987     paf loss 20.45822     hm loss 3970.19165\n",
      "Iteration:   2270    step:     7918     combined loss: 4430.82354     paf loss 15.30254     hm loss 4415.52100\n",
      "Iteration:   2275    step:     7923     combined loss: 2713.97869     paf loss 13.07207     hm loss 2700.90662\n",
      "Iteration:   2280    step:     7928     combined loss: 3912.45380     paf loss 24.89448     hm loss 3887.55933\n",
      "Iteration:   2285    step:     7933     combined loss: 3795.24040     paf loss 13.85697     hm loss 3781.38342\n",
      "Iteration:   2290    step:     7938     combined loss: 3148.14242     paf loss 17.81893     hm loss 3130.32349\n",
      "Iteration:   2295    step:     7943     combined loss: 4261.72558     paf loss 25.80102     hm loss 4235.92456\n",
      "Iteration:   2300    step:     7948     combined loss: 2616.02028     paf loss 20.52981     hm loss 2595.49048\n",
      "Iteration:   2305    step:     7953     combined loss: 4420.23963     paf loss 35.83778     hm loss 4384.40186\n",
      "Iteration:   2310    step:     7958     combined loss: 2135.85884     paf loss 14.71260     hm loss 2121.14624\n",
      "Iteration:   2315    step:     7963     combined loss: 2773.64541     paf loss 20.83804     hm loss 2752.80737\n",
      "Iteration:   2320    step:     7968     combined loss: 2726.79771     paf loss 21.39537     hm loss 2705.40234\n",
      "Iteration:   2325    step:     7973     combined loss: 2675.87712     paf loss 13.53361     hm loss 2662.34351\n",
      "Iteration:   2330    step:     7978     combined loss: 5205.33179     paf loss 17.42212     hm loss 5187.90967\n",
      "Iteration:   2335    step:     7983     combined loss: 3348.04754     paf loss 15.64825     hm loss 3332.39929\n",
      "Iteration:   2340    step:     7988     combined loss: 2402.48205     paf loss 14.46960     hm loss 2388.01245\n",
      "Iteration:   2345    step:     7993     combined loss: 3038.85466     paf loss 14.91985     hm loss 3023.93481\n",
      "Iteration:   2350    step:     7998     combined loss: 3722.80135     paf loss 15.85726     hm loss 3706.94409\n",
      "Iteration:   2355    step:     8003     combined loss: 4385.13132     paf loss 25.95847     hm loss 4359.17285\n",
      "Iteration:   2360    step:     8008     combined loss: 3249.34461     paf loss 14.11438     hm loss 3235.23022\n",
      "Iteration:   2365    step:     8013     combined loss: 3458.00439     paf loss 16.28906     hm loss 3441.71533\n",
      "Iteration:   2370    step:     8018     combined loss: 6356.34306     paf loss 22.29692     hm loss 6334.04614\n",
      "Iteration:   2375    step:     8023     combined loss: 3257.26344     paf loss 22.49501     hm loss 3234.76843\n",
      "Iteration:   2380    step:     8028     combined loss: 2584.47971     paf loss 18.78086     hm loss 2565.69885\n",
      "Iteration:   2385    step:     8033     combined loss: 3982.08714     paf loss 27.40696     hm loss 3954.68018\n",
      "Iteration:   2390    step:     8038     combined loss: 4187.78075     paf loss 19.30883     hm loss 4168.47192\n",
      "Iteration:   2395    step:     8043     combined loss: 3897.75150     paf loss 20.44999     hm loss 3877.30151\n",
      "Iteration:   2400    step:     8048     combined loss: 3293.26379     paf loss 17.07079     hm loss 3276.19299\n",
      "Iteration:   2405    step:     8053     combined loss: 3739.44072     paf loss 19.45451     hm loss 3719.98621\n",
      "Iteration:   2410    step:     8058     combined loss: 3108.19661     paf loss 19.21029     hm loss 3088.98633\n",
      "Iteration:   2415    step:     8063     combined loss: 3433.63826     paf loss 14.56782     hm loss 3419.07043\n",
      "Iteration:   2420    step:     8068     combined loss: 3736.02891     paf loss 19.40501     hm loss 3716.62390\n",
      "Iteration:   2425    step:     8073     combined loss: 3757.11426     paf loss 22.11963     hm loss 3734.99463\n",
      "Iteration:   2430    step:     8078     combined loss: 2250.03052     paf loss 13.52747     hm loss 2236.50305\n",
      "Iteration:   2435    step:     8083     combined loss: 4661.87009     paf loss 17.59666     hm loss 4644.27344\n",
      "Iteration:   2440    step:     8088     combined loss: 2571.08368     paf loss 17.92657     hm loss 2553.15710\n",
      "Iteration:   2445    step:     8093     combined loss: 3076.58048     paf loss 25.02580     hm loss 3051.55469\n",
      "Iteration:   2450    step:     8098     combined loss: 2742.06570     paf loss 18.25637     hm loss 2723.80933\n",
      "Iteration:   2455    step:     8103     combined loss: 2450.82738     paf loss 20.60851     hm loss 2430.21887\n",
      "Iteration:   2460    step:     8108     combined loss: 3460.02379     paf loss 27.88317     hm loss 3432.14062\n",
      "Iteration:   2465    step:     8113     combined loss: 3490.04851     paf loss 20.64104     hm loss 3469.40747\n",
      "Iteration:   2470    step:     8118     combined loss: 4546.01580     paf loss 25.28412     hm loss 4520.73169\n",
      "Iteration:   2475    step:     8123     combined loss: 3457.80511     paf loss 20.36017     hm loss 3437.44495\n",
      "Iteration:   2480    step:     8128     combined loss: 2820.46781     paf loss 17.89762     hm loss 2802.57019\n",
      "Iteration:   2485    step:     8133     combined loss: 3399.21351     paf loss 16.89796     hm loss 3382.31555\n",
      "Iteration:   2490    step:     8138     combined loss: 4092.61087     paf loss 10.78763     hm loss 4081.82324\n",
      "Iteration:   2495    step:     8143     combined loss: 3858.04653     paf loss 17.68069     hm loss 3840.36584\n",
      "Iteration:   2500    step:     8148     combined loss: 4020.17412     paf loss 23.24687     hm loss 3996.92725\n",
      "Iteration:   2505    step:     8153     combined loss: 3169.83312     paf loss 14.63756     hm loss 3155.19556\n",
      "Iteration:   2510    step:     8158     combined loss: 3539.49844     paf loss 23.89212     hm loss 3515.60632\n",
      "Iteration:   2515    step:     8163     combined loss: 3794.11443     paf loss 15.86773     hm loss 3778.24670\n",
      "Iteration:   2520    step:     8168     combined loss: 4694.70214     paf loss 17.41283     hm loss 4677.28931\n",
      "Iteration:   2525    step:     8173     combined loss: 3483.42776     paf loss 24.63674     hm loss 3458.79102\n",
      "Iteration:   2530    step:     8178     combined loss: 3233.36890     paf loss 17.29968     hm loss 3216.06921\n",
      "Iteration:   2535    step:     8183     combined loss: 5566.64063     paf loss 22.27783     hm loss 5544.36279\n",
      "Iteration:   2540    step:     8188     combined loss: 3156.65937     paf loss 19.02949     hm loss 3137.62988\n",
      "Iteration:   2545    step:     8193     combined loss: 2882.41601     paf loss 20.85876     hm loss 2861.55725\n",
      "Iteration:   2550    step:     8198     combined loss: 4808.30371     paf loss 25.35840     hm loss 4782.94531\n",
      "Iteration:   2555    step:     8203     combined loss: 3485.29034     paf loss 18.16876     hm loss 3467.12158\n",
      "Iteration:   2560    step:     8208     combined loss: 4767.02763     paf loss 21.58891     hm loss 4745.43872\n",
      "Iteration:   2565    step:     8213     combined loss: 5560.57514     paf loss 23.65986     hm loss 5536.91528\n",
      "Iteration:   2570    step:     8218     combined loss: 3459.16867     paf loss 20.63339     hm loss 3438.53528\n",
      "Iteration:   2575    step:     8223     combined loss: 3607.82960     paf loss 28.88612     hm loss 3578.94348\n",
      "Iteration:   2580    step:     8228     combined loss: 3164.71981     paf loss 17.45956     hm loss 3147.26025\n",
      "Iteration:   2585    step:     8233     combined loss: 5006.94072     paf loss 25.46001     hm loss 4981.48071\n",
      "Iteration:   2590    step:     8238     combined loss: 3111.54886     paf loss 25.63235     hm loss 3085.91650\n",
      "Iteration:   2595    step:     8243     combined loss: 6332.01344     paf loss 27.56130     hm loss 6304.45215\n",
      "Iteration:   2600    step:     8248     combined loss: 4355.00239     paf loss 23.01704     hm loss 4331.98535\n",
      "Iteration:   2605    step:     8253     combined loss: 3627.88836     paf loss 16.29461     hm loss 3611.59375\n",
      "Iteration:   2610    step:     8258     combined loss: 4182.81218     paf loss 20.03728     hm loss 4162.77490\n",
      "Iteration:   2615    step:     8263     combined loss: 2496.76907     paf loss 10.30788     hm loss 2486.46118\n",
      "Iteration:   2620    step:     8268     combined loss: 4307.79089     paf loss 21.13562     hm loss 4286.65527\n",
      "Iteration:   2625    step:     8273     combined loss: 3143.38222     paf loss 15.82937     hm loss 3127.55286\n",
      "Iteration:   2630    step:     8278     combined loss: 4451.88414     paf loss 22.22618     hm loss 4429.65796\n",
      "Iteration:   2635    step:     8283     combined loss: 4596.66904     paf loss 22.76719     hm loss 4573.90186\n",
      "Iteration:   2640    step:     8288     combined loss: 4716.89284     paf loss 20.46925     hm loss 4696.42358\n",
      "Iteration:   2645    step:     8293     combined loss: 3626.00670     paf loss 22.56127     hm loss 3603.44543\n",
      "Iteration:   2650    step:     8298     combined loss: 4535.12455     paf loss 22.51884     hm loss 4512.60571\n",
      "Iteration:   2655    step:     8303     combined loss: 3670.81146     paf loss 19.39118     hm loss 3651.42029\n",
      "Iteration:   2660    step:     8308     combined loss: 3026.57622     paf loss 18.82707     hm loss 3007.74915\n",
      "Iteration:   2665    step:     8313     combined loss: 3036.57686     paf loss 16.40157     hm loss 3020.17529\n",
      "Iteration:   2670    step:     8318     combined loss: 5201.10961     paf loss 24.82665     hm loss 5176.28296\n",
      "Iteration:   2675    step:     8323     combined loss: 3026.78522     paf loss 14.20637     hm loss 3012.57886\n",
      "Iteration:   2680    step:     8328     combined loss: 5555.80401     paf loss 24.92950     hm loss 5530.87451\n",
      "Iteration:   2685    step:     8333     combined loss: 3345.71399     paf loss 11.41406     hm loss 3334.29993\n",
      "Iteration:   2690    step:     8338     combined loss: 3673.61068     paf loss 20.82968     hm loss 3652.78101\n",
      "Iteration:   2695    step:     8343     combined loss: 2606.39429     paf loss 15.97645     hm loss 2590.41785\n",
      "Iteration:   2700    step:     8348     combined loss: 4499.32089     paf loss 19.11460     hm loss 4480.20630\n",
      "Iteration:   2705    step:     8353     combined loss: 3796.64920     paf loss 23.00064     hm loss 3773.64856\n",
      "Iteration:   2710    step:     8358     combined loss: 2723.59117     paf loss 13.34325     hm loss 2710.24792\n",
      "Iteration:   2715    step:     8363     combined loss: 3331.18798     paf loss 18.73839     hm loss 3312.44958\n",
      "Iteration:   2720    step:     8368     combined loss: 3681.57988     paf loss 16.22039     hm loss 3665.35950\n",
      "Iteration:   2725    step:     8373     combined loss: 4223.60766     paf loss 25.89697     hm loss 4197.71069\n",
      "Iteration:   2730    step:     8378     combined loss: 5111.28393     paf loss 23.19604     hm loss 5088.08789\n",
      "Iteration:   2735    step:     8383     combined loss: 4115.47058     paf loss 20.79285     hm loss 4094.67773\n",
      "Iteration:   2740    step:     8388     combined loss: 3133.30851     paf loss 16.08378     hm loss 3117.22473\n",
      "Iteration:   2745    step:     8393     combined loss: 4204.77121     paf loss 18.44968     hm loss 4186.32153\n",
      "Iteration:   2750    step:     8398     combined loss: 3470.09865     paf loss 19.00673     hm loss 3451.09192\n",
      "Iteration:   2755    step:     8403     combined loss: 3519.53863     paf loss 22.89081     hm loss 3496.64783\n",
      "Iteration:   2760    step:     8408     combined loss: 3563.90083     paf loss 20.96809     hm loss 3542.93274\n",
      "Iteration:   2765    step:     8413     combined loss: 3363.38687     paf loss 21.60012     hm loss 3341.78674\n",
      "Iteration:   2770    step:     8418     combined loss: 3547.48187     paf loss 18.80853     hm loss 3528.67334\n",
      "Iteration:   2775    step:     8423     combined loss: 4161.88059     paf loss 27.61521     hm loss 4134.26538\n",
      "Iteration:   2780    step:     8428     combined loss: 4278.83833     paf loss 23.93135     hm loss 4254.90698\n",
      "Iteration:   2785    step:     8433     combined loss: 4315.47153     paf loss 19.63022     hm loss 4295.84131\n",
      "Iteration:   2790    step:     8438     combined loss: 4703.89472     paf loss 14.04145     hm loss 4689.85327\n",
      "Iteration:   2795    step:     8443     combined loss: 3804.30168     paf loss 19.75199     hm loss 3784.54968\n",
      "Iteration:   2800    step:     8448     combined loss: 2724.23149     paf loss 17.44597     hm loss 2706.78552\n",
      "Iteration:   2805    step:     8453     combined loss: 5704.20338     paf loss 24.89576     hm loss 5679.30762\n",
      "Iteration:   2810    step:     8458     combined loss: 3624.55551     paf loss 23.57125     hm loss 3600.98425\n",
      "Iteration:   2815    step:     8463     combined loss: 3006.96152     paf loss 20.27389     hm loss 2986.68762\n",
      "Iteration:   2820    step:     8468     combined loss: 4559.85160     paf loss 19.86014     hm loss 4539.99146\n",
      "Train Loss: 3836.4889    PAF Loss:  20.0761    HM Loss:  3816.4129    Acc: NA\n",
      "Iteration:     65    step:     8537     combined loss: 3245.12724     paf loss 19.39616     hm loss 3225.73108\n",
      "Iteration:     70    step:     8542     combined loss: 2881.97933     paf loss 24.47799     hm loss 2857.50134\n",
      "Iteration:     75    step:     8547     combined loss: 2686.88480     paf loss 20.39322     hm loss 2666.49158\n",
      "Iteration:    175    step:     8647     combined loss: 4294.60182     paf loss 20.28932     hm loss 4274.31250\n",
      "Iteration:    180    step:     8652     combined loss: 6049.11810     paf loss 29.77947     hm loss 6019.33862\n",
      "Iteration:    185    step:     8657     combined loss: 5003.50767     paf loss 25.32676     hm loss 4978.18091\n",
      "Iteration:    190    step:     8662     combined loss: 3009.83590     paf loss 19.15096     hm loss 2990.68494\n",
      "Iteration:    195    step:     8667     combined loss: 3186.46074     paf loss 21.21465     hm loss 3165.24609\n",
      "Iteration:    200    step:     8672     combined loss: 4429.22481     paf loss 16.73653     hm loss 4412.48828\n",
      "Iteration:    205    step:     8677     combined loss: 3082.06077     paf loss 17.90513     hm loss 3064.15564\n",
      "Iteration:    210    step:     8682     combined loss: 3922.68787     paf loss 16.75562     hm loss 3905.93225\n",
      "Iteration:    215    step:     8687     combined loss: 2924.01948     paf loss 11.42744     hm loss 2912.59204\n",
      "Iteration:    220    step:     8692     combined loss: 3297.39039     paf loss 19.89905     hm loss 3277.49133\n",
      "Iteration:    225    step:     8697     combined loss: 3831.60647     paf loss 23.86563     hm loss 3807.74084\n",
      "Iteration:    230    step:     8702     combined loss: 2323.41170     paf loss 16.16878     hm loss 2307.24292\n",
      "Iteration:    235    step:     8707     combined loss: 7048.86275     paf loss 22.71895     hm loss 7026.14380\n",
      "Iteration:    240    step:     8712     combined loss: 2682.00084     paf loss 15.43589     hm loss 2666.56494\n",
      "Iteration:    245    step:     8717     combined loss: 3771.72404     paf loss 18.50333     hm loss 3753.22070\n",
      "Iteration:    250    step:     8722     combined loss: 2693.73660     paf loss 15.31631     hm loss 2678.42029\n",
      "Iteration:    255    step:     8727     combined loss: 4231.88427     paf loss 21.00707     hm loss 4210.87720\n",
      "Iteration:    260    step:     8732     combined loss: 3904.92385     paf loss 19.20742     hm loss 3885.71643\n",
      "Iteration:    265    step:     8737     combined loss: 4034.53649     paf loss 18.55346     hm loss 4015.98303\n",
      "Iteration:    270    step:     8742     combined loss: 3665.39325     paf loss 27.11518     hm loss 3638.27808\n",
      "Iteration:    275    step:     8747     combined loss: 5155.33923     paf loss 26.59289     hm loss 5128.74634\n",
      "Iteration:    280    step:     8752     combined loss: 3081.40795     paf loss 17.92480     hm loss 3063.48315\n",
      "Iteration:    285    step:     8757     combined loss: 5463.52903     paf loss 18.07737     hm loss 5445.45166\n",
      "Iteration:    290    step:     8762     combined loss: 4097.53039     paf loss 23.09033     hm loss 4074.44006\n",
      "Iteration:    295    step:     8767     combined loss: 3070.59775     paf loss 17.53610     hm loss 3053.06165\n",
      "Iteration:    300    step:     8772     combined loss: 5288.46788     paf loss 23.05137     hm loss 5265.41650\n",
      "Iteration:    305    step:     8777     combined loss: 2648.25456     paf loss 19.78117     hm loss 2628.47339\n",
      "Iteration:    310    step:     8782     combined loss: 4257.95899     paf loss 18.55543     hm loss 4239.40356\n",
      "Iteration:    315    step:     8787     combined loss: 4871.85458     paf loss 21.94418     hm loss 4849.91040\n",
      "Iteration:    320    step:     8792     combined loss: 3475.06752     paf loss 19.41749     hm loss 3455.65002\n",
      "Iteration:    325    step:     8797     combined loss: 3638.89273     paf loss 24.10294     hm loss 3614.78979\n",
      "Iteration:    330    step:     8802     combined loss: 2501.76936     paf loss 19.54280     hm loss 2482.22656\n",
      "Iteration:    335    step:     8807     combined loss: 3545.69103     paf loss 19.50231     hm loss 3526.18872\n",
      "Iteration:    340    step:     8812     combined loss: 4321.26901     paf loss 15.34298     hm loss 4305.92603\n",
      "Iteration:    345    step:     8817     combined loss: 2660.14380     paf loss 22.36499     hm loss 2637.77881\n",
      "Iteration:    350    step:     8822     combined loss: 4764.03820     paf loss 20.40685     hm loss 4743.63135\n",
      "Iteration:    355    step:     8827     combined loss: 3549.62105     paf loss 21.00557     hm loss 3528.61548\n",
      "Iteration:    360    step:     8832     combined loss: 3564.39376     paf loss 18.94185     hm loss 3545.45190\n",
      "Iteration:    365    step:     8837     combined loss: 3382.25330     paf loss 18.92640     hm loss 3363.32690\n",
      "Iteration:    370    step:     8842     combined loss: 3116.23439     paf loss 19.50795     hm loss 3096.72644\n",
      "Iteration:    375    step:     8847     combined loss: 3959.63628     paf loss 19.01482     hm loss 3940.62146\n",
      "Iteration:    380    step:     8852     combined loss: 3761.72775     paf loss 22.60092     hm loss 3739.12683\n",
      "Iteration:    385    step:     8857     combined loss: 4450.07089     paf loss 22.05526     hm loss 4428.01562\n",
      "Iteration:    390    step:     8862     combined loss: 2934.84117     paf loss 21.69676     hm loss 2913.14441\n",
      "Iteration:    395    step:     8867     combined loss: 2730.39763     paf loss 14.91826     hm loss 2715.47937\n",
      "Iteration:    400    step:     8872     combined loss: 4450.50531     paf loss 20.89203     hm loss 4429.61328\n",
      "Iteration:    405    step:     8877     combined loss: 6954.23787     paf loss 24.22005     hm loss 6930.01782\n",
      "Iteration:    410    step:     8882     combined loss: 3279.49059     paf loss 15.48912     hm loss 3264.00146\n",
      "Iteration:    415    step:     8887     combined loss: 2784.55914     paf loss 13.84259     hm loss 2770.71655\n",
      "Iteration:    420    step:     8892     combined loss: 3855.17473     paf loss 16.44353     hm loss 3838.73120\n",
      "Iteration:    425    step:     8897     combined loss: 3549.06051     paf loss 15.90634     hm loss 3533.15417\n",
      "Iteration:    430    step:     8902     combined loss: 4358.10393     paf loss 23.73894     hm loss 4334.36499\n",
      "Iteration:    435    step:     8907     combined loss: 3446.39953     paf loss 16.38549     hm loss 3430.01404\n",
      "Iteration:    440    step:     8912     combined loss: 4309.55427     paf loss 18.28376     hm loss 4291.27051\n",
      "Iteration:    445    step:     8917     combined loss: 4549.10769     paf loss 16.62868     hm loss 4532.47900\n",
      "Iteration:    450    step:     8922     combined loss: 2635.95570     paf loss 17.12697     hm loss 2618.82874\n",
      "Iteration:    455    step:     8927     combined loss: 2607.23325     paf loss 18.46616     hm loss 2588.76709\n",
      "Iteration:    460    step:     8932     combined loss: 3800.75730     paf loss 25.86118     hm loss 3774.89612\n",
      "Iteration:    465    step:     8937     combined loss: 2515.16799     paf loss 16.45058     hm loss 2498.71741\n",
      "Iteration:    470    step:     8942     combined loss: 3557.78622     paf loss 20.27364     hm loss 3537.51257\n",
      "Iteration:    475    step:     8947     combined loss: 4075.45683     paf loss 19.26518     hm loss 4056.19165\n",
      "Iteration:    480    step:     8952     combined loss: 2913.25541     paf loss 18.63029     hm loss 2894.62512\n",
      "Iteration:    485    step:     8957     combined loss: 2843.78173     paf loss 15.40490     hm loss 2828.37683\n",
      "Iteration:    490    step:     8962     combined loss: 5220.45106     paf loss 21.53260     hm loss 5198.91846\n",
      "Iteration:    495    step:     8967     combined loss: 3268.12770     paf loss 16.16591     hm loss 3251.96179\n",
      "Iteration:    500    step:     8972     combined loss: 3572.93148     paf loss 21.91293     hm loss 3551.01855\n",
      "Iteration:    505    step:     8977     combined loss: 3152.17803     paf loss 17.44475     hm loss 3134.73328\n",
      "Iteration:    510    step:     8982     combined loss: 3427.85219     paf loss 20.05568     hm loss 3407.79651\n",
      "Iteration:    515    step:     8987     combined loss: 3483.04556     paf loss 18.71072     hm loss 3464.33484\n",
      "Iteration:    520    step:     8992     combined loss: 3279.61924     paf loss 17.25083     hm loss 3262.36841\n",
      "Iteration:    525    step:     8997     combined loss: 4367.96504     paf loss 19.38984     hm loss 4348.57520\n",
      "Iteration:    530    step:     9002     combined loss: 3686.97934     paf loss 18.39389     hm loss 3668.58545\n",
      "Iteration:    535    step:     9007     combined loss: 3111.15352     paf loss 13.87398     hm loss 3097.27954\n",
      "Iteration:    540    step:     9012     combined loss: 4645.23841     paf loss 22.62708     hm loss 4622.61133\n",
      "Iteration:    545    step:     9017     combined loss: 2907.20006     paf loss 17.55943     hm loss 2889.64062\n",
      "Iteration:    550    step:     9022     combined loss: 3065.84945     paf loss 18.04171     hm loss 3047.80774\n",
      "Iteration:    555    step:     9027     combined loss: 3599.76926     paf loss 17.91147     hm loss 3581.85779\n",
      "Iteration:    560    step:     9032     combined loss: 4545.29574     paf loss 19.87362     hm loss 4525.42212\n",
      "Iteration:    565    step:     9037     combined loss: 4262.48230     paf loss 18.15760     hm loss 4244.32471\n",
      "Iteration:    570    step:     9042     combined loss: 3373.70110     paf loss 19.03801     hm loss 3354.66309\n",
      "Iteration:    575    step:     9047     combined loss: 3714.84785     paf loss 17.47102     hm loss 3697.37683\n",
      "Iteration:    580    step:     9052     combined loss: 2560.97097     paf loss 13.14675     hm loss 2547.82422\n",
      "Iteration:    585    step:     9057     combined loss: 3166.71624     paf loss 19.35198     hm loss 3147.36426\n",
      "Iteration:    590    step:     9062     combined loss: 4262.74420     paf loss 20.42047     hm loss 4242.32373\n",
      "Iteration:    595    step:     9067     combined loss: 3965.01155     paf loss 24.10713     hm loss 3940.90442\n",
      "Iteration:    600    step:     9072     combined loss: 3857.54882     paf loss 21.13622     hm loss 3836.41260\n",
      "Iteration:    605    step:     9077     combined loss: 5464.56733     paf loss 19.07612     hm loss 5445.49121\n",
      "Iteration:    610    step:     9082     combined loss: 3102.05226     paf loss 19.12135     hm loss 3082.93091\n",
      "Iteration:    615    step:     9087     combined loss: 4067.22558     paf loss 19.63635     hm loss 4047.58923\n",
      "Iteration:    620    step:     9092     combined loss: 3773.98419     paf loss 18.58795     hm loss 3755.39624\n",
      "Iteration:    625    step:     9097     combined loss: 3024.25322     paf loss 24.14360     hm loss 3000.10962\n",
      "Iteration:    630    step:     9102     combined loss: 3715.29970     paf loss 13.40542     hm loss 3701.89429\n",
      "Iteration:    635    step:     9107     combined loss: 3250.60324     paf loss 19.22982     hm loss 3231.37341\n",
      "Iteration:    640    step:     9112     combined loss: 3745.55031     paf loss 17.87331     hm loss 3727.67700\n",
      "Iteration:    645    step:     9117     combined loss: 3761.23905     paf loss 33.83096     hm loss 3727.40808\n",
      "Iteration:    650    step:     9122     combined loss: 3292.57446     paf loss 24.23828     hm loss 3268.33618\n",
      "Iteration:    655    step:     9127     combined loss: 4288.32387     paf loss 21.92250     hm loss 4266.40137\n",
      "Iteration:    660    step:     9132     combined loss: 3433.19066     paf loss 23.46336     hm loss 3409.72729\n",
      "Iteration:    665    step:     9137     combined loss: 2879.99037     paf loss 15.34938     hm loss 2864.64099\n",
      "Iteration:    670    step:     9142     combined loss: 3132.85385     paf loss 16.96970     hm loss 3115.88416\n",
      "Iteration:    675    step:     9147     combined loss: 2740.22551     paf loss 16.42974     hm loss 2723.79578\n",
      "Iteration:    680    step:     9152     combined loss: 3823.82950     paf loss 21.64554     hm loss 3802.18396\n",
      "Iteration:    685    step:     9157     combined loss: 4637.49168     paf loss 21.32371     hm loss 4616.16797\n",
      "Iteration:    690    step:     9162     combined loss: 3762.84564     paf loss 22.60199     hm loss 3740.24365\n",
      "Iteration:    695    step:     9167     combined loss: 3436.16092     paf loss 17.66787     hm loss 3418.49304\n",
      "Iteration:    700    step:     9172     combined loss: 3190.99062     paf loss 17.19167     hm loss 3173.79895\n",
      "Iteration:    705    step:     9177     combined loss: 2702.34697     paf loss 16.30803     hm loss 2686.03894\n",
      "Iteration:    710    step:     9182     combined loss: 4241.57242     paf loss 16.50651     hm loss 4225.06592\n",
      "Iteration:    715    step:     9187     combined loss: 3246.05016     paf loss 17.20300     hm loss 3228.84717\n",
      "Iteration:    720    step:     9192     combined loss: 4792.02112     paf loss 23.47059     hm loss 4768.55054\n",
      "Iteration:    725    step:     9197     combined loss: 3158.58594     paf loss 22.44495     hm loss 3136.14099\n",
      "Iteration:    730    step:     9202     combined loss: 3536.99053     paf loss 19.07500     hm loss 3517.91553\n",
      "Iteration:    735    step:     9207     combined loss: 3308.54874     paf loss 18.28836     hm loss 3290.26038\n",
      "Iteration:    740    step:     9212     combined loss: 4439.14432     paf loss 14.80277     hm loss 4424.34155\n",
      "Iteration:    745    step:     9217     combined loss: 3805.48584     paf loss 17.92566     hm loss 3787.56018\n",
      "Iteration:    750    step:     9222     combined loss: 3330.41643     paf loss 21.28618     hm loss 3309.13025\n",
      "Iteration:    755    step:     9227     combined loss: 3323.87427     paf loss 24.75024     hm loss 3299.12402\n",
      "Iteration:    760    step:     9232     combined loss: 3288.98622     paf loss 15.96486     hm loss 3273.02136\n",
      "Iteration:    765    step:     9237     combined loss: 2639.07002     paf loss 17.75203     hm loss 2621.31799\n",
      "Iteration:    770    step:     9242     combined loss: 4257.69426     paf loss 27.86833     hm loss 4229.82593\n",
      "Iteration:    775    step:     9247     combined loss: 3772.17428     paf loss 13.03817     hm loss 3759.13611\n",
      "Iteration:    780    step:     9252     combined loss: 3692.79227     paf loss 24.46744     hm loss 3668.32483\n",
      "Iteration:    785    step:     9257     combined loss: 2361.43333     paf loss 17.81468     hm loss 2343.61865\n",
      "Iteration:    790    step:     9262     combined loss: 3572.89979     paf loss 17.91419     hm loss 3554.98560\n",
      "Iteration:    795    step:     9267     combined loss: 3803.49829     paf loss 23.12647     hm loss 3780.37183\n",
      "Iteration:    800    step:     9272     combined loss: 2335.21009     paf loss 16.02919     hm loss 2319.18091\n",
      "Iteration:    805    step:     9277     combined loss: 4387.62592     paf loss 27.13129     hm loss 4360.49463\n",
      "Iteration:    810    step:     9282     combined loss: 3606.64599     paf loss 21.49145     hm loss 3585.15454\n",
      "Iteration:    815    step:     9287     combined loss: 4178.81243     paf loss 21.18597     hm loss 4157.62646\n",
      "Iteration:    820    step:     9292     combined loss: 2657.42160     paf loss 18.78305     hm loss 2638.63855\n",
      "Iteration:    825    step:     9297     combined loss: 3203.02035     paf loss 18.09176     hm loss 3184.92859\n",
      "Iteration:    830    step:     9302     combined loss: 3580.43612     paf loss 20.24203     hm loss 3560.19409\n",
      "Iteration:    835    step:     9307     combined loss: 3140.78713     paf loss 14.88637     hm loss 3125.90076\n",
      "Iteration:    840    step:     9312     combined loss: 3557.06487     paf loss 20.70855     hm loss 3536.35632\n",
      "Iteration:    845    step:     9317     combined loss: 4023.00589     paf loss 19.77896     hm loss 4003.22693\n",
      "Iteration:    850    step:     9322     combined loss: 4194.72775     paf loss 20.68234     hm loss 4174.04541\n",
      "Iteration:    855    step:     9327     combined loss: 3917.01975     paf loss 22.54746     hm loss 3894.47229\n",
      "Iteration:    860    step:     9332     combined loss: 4671.58624     paf loss 16.87140     hm loss 4654.71484\n",
      "Iteration:    865    step:     9337     combined loss: 2460.53058     paf loss 16.10968     hm loss 2444.42090\n",
      "Iteration:    870    step:     9342     combined loss: 3324.14937     paf loss 15.06184     hm loss 3309.08752\n",
      "Iteration:    875    step:     9347     combined loss: 4263.04519     paf loss 20.64651     hm loss 4242.39868\n",
      "Iteration:    880    step:     9352     combined loss: 3216.11567     paf loss 24.91718     hm loss 3191.19849\n",
      "Iteration:    885    step:     9357     combined loss: 3903.49452     paf loss 16.73377     hm loss 3886.76074\n",
      "Iteration:    890    step:     9362     combined loss: 4186.41152     paf loss 21.53896     hm loss 4164.87256\n",
      "Iteration:    895    step:     9367     combined loss: 2522.03058     paf loss 18.06976     hm loss 2503.96082\n",
      "Iteration:    900    step:     9372     combined loss: 4324.06549     paf loss 20.67047     hm loss 4303.39502\n",
      "Iteration:    905    step:     9377     combined loss: 3873.34603     paf loss 21.06503     hm loss 3852.28101\n",
      "Iteration:    910    step:     9382     combined loss: 3043.94200     paf loss 25.38633     hm loss 3018.55566\n",
      "Iteration:    915    step:     9387     combined loss: 2715.98753     paf loss 18.40770     hm loss 2697.57983\n",
      "Iteration:    920    step:     9392     combined loss: 3609.16563     paf loss 22.01621     hm loss 3587.14941\n",
      "Iteration:    925    step:     9397     combined loss: 3487.39118     paf loss 20.34089     hm loss 3467.05029\n",
      "Iteration:    930    step:     9402     combined loss: 3657.91247     paf loss 28.46068     hm loss 3629.45178\n",
      "Iteration:    935    step:     9407     combined loss: 3925.49415     paf loss 26.86634     hm loss 3898.62781\n",
      "Iteration:    940    step:     9412     combined loss: 2841.52389     paf loss 18.62252     hm loss 2822.90137\n",
      "Iteration:    945    step:     9417     combined loss: 4807.01960     paf loss 18.46589     hm loss 4788.55371\n",
      "Iteration:    950    step:     9422     combined loss: 3903.98523     paf loss 21.46778     hm loss 3882.51746\n",
      "Iteration:    955    step:     9427     combined loss: 3373.96973     paf loss 18.43347     hm loss 3355.53625\n",
      "Iteration:    960    step:     9432     combined loss: 3767.03019     paf loss 18.90544     hm loss 3748.12476\n",
      "Iteration:    965    step:     9437     combined loss: 3962.90197     paf loss 19.84533     hm loss 3943.05664\n",
      "Iteration:    970    step:     9442     combined loss: 3857.62402     paf loss 15.00610     hm loss 3842.61792\n",
      "Iteration:    975    step:     9447     combined loss: 4122.42285     paf loss 21.13806     hm loss 4101.28479\n",
      "Iteration:    980    step:     9452     combined loss: 4903.08232     paf loss 19.43218     hm loss 4883.65015\n",
      "Iteration:    985    step:     9457     combined loss: 3263.02412     paf loss 20.39009     hm loss 3242.63403\n",
      "Iteration:    990    step:     9462     combined loss: 3388.89050     paf loss 17.62853     hm loss 3371.26196\n",
      "Iteration:    995    step:     9467     combined loss: 5675.88877     paf loss 26.67710     hm loss 5649.21167\n",
      "Iteration:   1000    step:     9472     combined loss: 3158.98099     paf loss 15.37955     hm loss 3143.60144\n",
      "Iteration:   1005    step:     9477     combined loss: 3505.94308     paf loss 20.98849     hm loss 3484.95459\n",
      "Iteration:   1010    step:     9482     combined loss: 3889.40188     paf loss 20.10610     hm loss 3869.29578\n",
      "Iteration:   1015    step:     9487     combined loss: 3342.31203     paf loss 22.17531     hm loss 3320.13672\n",
      "Iteration:   1020    step:     9492     combined loss: 3898.93693     paf loss 21.45256     hm loss 3877.48438\n",
      "Iteration:   1025    step:     9497     combined loss: 3013.46074     paf loss 17.82597     hm loss 2995.63477\n",
      "Iteration:   1030    step:     9502     combined loss: 3328.37164     paf loss 20.85308     hm loss 3307.51855\n",
      "Iteration:   1035    step:     9507     combined loss: 3459.63348     paf loss 24.51238     hm loss 3435.12109\n",
      "Iteration:   1040    step:     9512     combined loss: 4029.76812     paf loss 18.99017     hm loss 4010.77795\n",
      "Iteration:   1045    step:     9517     combined loss: 3226.40922     paf loss 21.78825     hm loss 3204.62097\n",
      "Iteration:   1050    step:     9522     combined loss: 3432.16240     paf loss 21.79668     hm loss 3410.36572\n",
      "Iteration:   1055    step:     9527     combined loss: 3975.93930     paf loss 21.06894     hm loss 3954.87036\n",
      "Iteration:   1060    step:     9532     combined loss: 3170.36714     paf loss 17.44222     hm loss 3152.92493\n",
      "Iteration:   1065    step:     9537     combined loss: 3089.43699     paf loss 15.71556     hm loss 3073.72144\n",
      "Iteration:   1070    step:     9542     combined loss: 2751.72038     paf loss 18.95073     hm loss 2732.76965\n",
      "Iteration:   1075    step:     9547     combined loss: 3372.70626     paf loss 19.96822     hm loss 3352.73804\n",
      "Iteration:   1080    step:     9552     combined loss: 4513.30953     paf loss 18.01876     hm loss 4495.29077\n",
      "Iteration:   1085    step:     9557     combined loss: 2760.52827     paf loss 20.89668     hm loss 2739.63159\n",
      "Iteration:   1090    step:     9562     combined loss: 2555.75661     paf loss 18.06447     hm loss 2537.69214\n",
      "Iteration:   1095    step:     9567     combined loss: 3208.16970     paf loss 18.93691     hm loss 3189.23279\n",
      "Iteration:   1100    step:     9572     combined loss: 4879.81843     paf loss 20.82112     hm loss 4858.99731\n",
      "Iteration:   1105    step:     9577     combined loss: 3973.73360     paf loss 19.73873     hm loss 3953.99487\n",
      "Iteration:   1110    step:     9582     combined loss: 3200.39293     paf loss 20.38427     hm loss 3180.00867\n",
      "Iteration:   1115    step:     9587     combined loss: 4161.67414     paf loss 23.34602     hm loss 4138.32812\n",
      "Iteration:   1120    step:     9592     combined loss: 2951.48633     paf loss 15.13623     hm loss 2936.35010\n",
      "Iteration:   1125    step:     9597     combined loss: 4103.10412     paf loss 23.74389     hm loss 4079.36023\n",
      "Iteration:   1130    step:     9602     combined loss: 3898.96511     paf loss 20.10903     hm loss 3878.85608\n",
      "Iteration:   1135    step:     9607     combined loss: 3810.35304     paf loss 20.36147     hm loss 3789.99158\n",
      "Iteration:   1140    step:     9612     combined loss: 4965.56201     paf loss 19.19849     hm loss 4946.36353\n",
      "Iteration:   1145    step:     9617     combined loss: 3493.81663     paf loss 16.27610     hm loss 3477.54053\n",
      "Iteration:   1150    step:     9622     combined loss: 3396.29637     paf loss 22.29381     hm loss 3374.00256\n",
      "Iteration:   1155    step:     9627     combined loss: 3013.22665     paf loss 18.14755     hm loss 2995.07910\n",
      "Iteration:   1160    step:     9632     combined loss: 5495.43437     paf loss 24.96342     hm loss 5470.47095\n",
      "Iteration:   1165    step:     9637     combined loss: 3460.82527     paf loss 24.01424     hm loss 3436.81104\n",
      "Iteration:   1170    step:     9642     combined loss: 5078.76848     paf loss 23.71135     hm loss 5055.05713\n",
      "Iteration:   1175    step:     9647     combined loss: 4802.78628     paf loss 26.11563     hm loss 4776.67065\n",
      "Iteration:   1180    step:     9652     combined loss: 4387.12044     paf loss 19.37606     hm loss 4367.74438\n",
      "Iteration:   1185    step:     9657     combined loss: 4408.30391     paf loss 20.63984     hm loss 4387.66406\n",
      "Iteration:   1190    step:     9662     combined loss: 3338.00727     paf loss 17.72235     hm loss 3320.28491\n",
      "Iteration:   1195    step:     9667     combined loss: 3820.53522     paf loss 21.43610     hm loss 3799.09912\n",
      "Iteration:   1200    step:     9672     combined loss: 3416.77029     paf loss 20.23953     hm loss 3396.53076\n",
      "Iteration:   1205    step:     9677     combined loss: 2594.03349     paf loss 17.31645     hm loss 2576.71704\n",
      "Iteration:   1210    step:     9682     combined loss: 3892.92499     paf loss 23.57941     hm loss 3869.34558\n",
      "Iteration:   1215    step:     9687     combined loss: 5785.68038     paf loss 26.44796     hm loss 5759.23242\n",
      "Iteration:   1220    step:     9692     combined loss: 2407.18947     paf loss 18.63259     hm loss 2388.55688\n",
      "Iteration:   1225    step:     9697     combined loss: 4579.14410     paf loss 19.37677     hm loss 4559.76733\n",
      "Iteration:   1230    step:     9702     combined loss: 4333.42200     paf loss 15.95569     hm loss 4317.46631\n",
      "Iteration:   1235    step:     9707     combined loss: 3023.82896     paf loss 15.81358     hm loss 3008.01538\n",
      "Iteration:   1240    step:     9712     combined loss: 4106.94061     paf loss 20.46417     hm loss 4086.47644\n",
      "Iteration:   1245    step:     9717     combined loss: 2730.88444     paf loss 18.60013     hm loss 2712.28430\n",
      "Iteration:   1250    step:     9722     combined loss: 3610.55427     paf loss 13.74519     hm loss 3596.80908\n",
      "Iteration:   1255    step:     9727     combined loss: 3411.56331     paf loss 18.35200     hm loss 3393.21130\n",
      "Iteration:   1260    step:     9732     combined loss: 2814.80214     paf loss 15.31190     hm loss 2799.49023\n",
      "Iteration:   1265    step:     9737     combined loss: 2636.20918     paf loss 12.92146     hm loss 2623.28772\n",
      "Iteration:   1270    step:     9742     combined loss: 7073.43780     paf loss 23.78326     hm loss 7049.65454\n",
      "Iteration:   1275    step:     9747     combined loss: 2846.07652     paf loss 21.38914     hm loss 2824.68738\n",
      "Iteration:   1280    step:     9752     combined loss: 3451.19645     paf loss 14.21024     hm loss 3436.98621\n",
      "Iteration:   1285    step:     9757     combined loss: 4161.64066     paf loss 15.17532     hm loss 4146.46533\n",
      "Iteration:   1290    step:     9762     combined loss: 3215.60356     paf loss 20.12724     hm loss 3195.47632\n",
      "Iteration:   1295    step:     9767     combined loss: 5774.15040     paf loss 22.78175     hm loss 5751.36865\n",
      "Iteration:   1300    step:     9772     combined loss: 3048.92572     paf loss 15.38983     hm loss 3033.53589\n",
      "Iteration:   1305    step:     9777     combined loss: 4988.72226     paf loss 25.95883     hm loss 4962.76343\n",
      "Iteration:   1310    step:     9782     combined loss: 4086.06392     paf loss 21.42122     hm loss 4064.64270\n",
      "Iteration:   1315    step:     9787     combined loss: 2812.60576     paf loss 16.61320     hm loss 2795.99255\n",
      "Iteration:   1320    step:     9792     combined loss: 2480.31616     paf loss 15.19617     hm loss 2465.12000\n",
      "Iteration:   1325    step:     9797     combined loss: 4222.44919     paf loss 26.98874     hm loss 4195.46045\n",
      "Iteration:   1330    step:     9802     combined loss: 4303.92477     paf loss 21.60348     hm loss 4282.32129\n",
      "Iteration:   1335    step:     9807     combined loss: 2839.39163     paf loss 18.14932     hm loss 2821.24231\n",
      "Iteration:   1340    step:     9812     combined loss: 3701.71739     paf loss 20.53990     hm loss 3681.17749\n",
      "Iteration:   1345    step:     9817     combined loss: 6021.74082     paf loss 25.31406     hm loss 5996.42676\n",
      "Iteration:   1350    step:     9822     combined loss: 2747.94057     paf loss 21.64797     hm loss 2726.29260\n",
      "Iteration:   1355    step:     9827     combined loss: 3577.24655     paf loss 19.07846     hm loss 3558.16809\n",
      "Iteration:   1360    step:     9832     combined loss: 4061.36229     paf loss 21.95592     hm loss 4039.40637\n",
      "Iteration:   1365    step:     9837     combined loss: 4158.40770     paf loss 18.31884     hm loss 4140.08887\n",
      "Iteration:   1370    step:     9842     combined loss: 2543.60003     paf loss 14.83441     hm loss 2528.76562\n",
      "Iteration:   1375    step:     9847     combined loss: 4357.85025     paf loss 23.54849     hm loss 4334.30176\n",
      "Iteration:   1380    step:     9852     combined loss: 3435.71027     paf loss 22.61750     hm loss 3413.09277\n",
      "Iteration:   1385    step:     9857     combined loss: 3425.37876     paf loss 20.37412     hm loss 3405.00464\n",
      "Iteration:   1390    step:     9862     combined loss: 2875.72987     paf loss 11.68886     hm loss 2864.04102\n",
      "Iteration:   1395    step:     9867     combined loss: 3537.13580     paf loss 23.83746     hm loss 3513.29834\n",
      "Iteration:   1400    step:     9872     combined loss: 4021.92625     paf loss 25.48936     hm loss 3996.43689\n",
      "Iteration:   1405    step:     9877     combined loss: 5122.70109     paf loss 25.70011     hm loss 5097.00098\n",
      "Iteration:   1410    step:     9882     combined loss: 4474.74706     paf loss 28.76537     hm loss 4445.98169\n",
      "Iteration:   1415    step:     9887     combined loss: 4498.32264     paf loss 26.27040     hm loss 4472.05225\n",
      "Iteration:   1420    step:     9892     combined loss: 4535.55260     paf loss 18.26817     hm loss 4517.28442\n",
      "Iteration:   1425    step:     9897     combined loss: 3574.85395     paf loss 21.40339     hm loss 3553.45056\n",
      "Iteration:   1430    step:     9902     combined loss: 4265.15782     paf loss 17.58776     hm loss 4247.57007\n",
      "Iteration:   1435    step:     9907     combined loss: 3418.70383     paf loss 19.26657     hm loss 3399.43726\n",
      "Iteration:   1440    step:     9912     combined loss: 4105.05653     paf loss 21.99134     hm loss 4083.06519\n",
      "Iteration:   1445    step:     9917     combined loss: 3946.13987     paf loss 17.79392     hm loss 3928.34595\n",
      "Iteration:   1450    step:     9922     combined loss: 3156.52720     paf loss 16.95225     hm loss 3139.57495\n",
      "Iteration:   1455    step:     9927     combined loss: 3017.19396     paf loss 19.22460     hm loss 2997.96936\n",
      "Iteration:   1460    step:     9932     combined loss: 3523.74554     paf loss 15.78290     hm loss 3507.96265\n",
      "Iteration:   1465    step:     9937     combined loss: 3114.50796     paf loss 21.23855     hm loss 3093.26941\n",
      "Iteration:   1470    step:     9942     combined loss: 3654.04558     paf loss 22.69634     hm loss 3631.34924\n",
      "Iteration:   1475    step:     9947     combined loss: 4365.91663     paf loss 24.43640     hm loss 4341.48022\n",
      "Iteration:   1480    step:     9952     combined loss: 3696.38788     paf loss 16.74823     hm loss 3679.63965\n",
      "Iteration:   1485    step:     9957     combined loss: 3923.37553     paf loss 27.16972     hm loss 3896.20581\n",
      "Iteration:   1490    step:     9962     combined loss: 4753.10912     paf loss 24.68431     hm loss 4728.42480\n",
      "Iteration:   1495    step:     9967     combined loss: 3345.16635     paf loss 19.68539     hm loss 3325.48096\n",
      "Iteration:   1500    step:     9972     combined loss: 2792.97219     paf loss 15.05337     hm loss 2777.91882\n",
      "Iteration:   1505    step:     9977     combined loss: 2455.59671     paf loss 16.60355     hm loss 2438.99316\n",
      "Iteration:   1510    step:     9982     combined loss: 5004.59296     paf loss 19.84271     hm loss 4984.75024\n",
      "Iteration:   1515    step:     9987     combined loss: 2560.67548     paf loss 20.56672     hm loss 2540.10876\n",
      "Iteration:   1520    step:     9992     combined loss: 2085.72297     paf loss 14.48915     hm loss 2071.23383\n",
      "Iteration:   1525    step:     9997     combined loss: 3357.44345     paf loss 18.15463     hm loss 3339.28882\n",
      "Iteration:   1530    step:    10002     combined loss: 3005.51214     paf loss 12.80901     hm loss 2992.70312\n",
      "Iteration:   1535    step:    10007     combined loss: 3575.76076     paf loss 25.12282     hm loss 3550.63794\n",
      "Iteration:   1540    step:    10012     combined loss: 5670.06740     paf loss 19.27199     hm loss 5650.79541\n",
      "Iteration:   1545    step:    10017     combined loss: 3529.78107     paf loss 17.58820     hm loss 3512.19287\n",
      "Iteration:   1550    step:    10022     combined loss: 4191.52981     paf loss 27.03249     hm loss 4164.49731\n",
      "Iteration:   1555    step:    10027     combined loss: 5568.94162     paf loss 26.34494     hm loss 5542.59668\n",
      "Iteration:   1560    step:    10032     combined loss: 3137.43866     paf loss 19.30816     hm loss 3118.13049\n",
      "Iteration:   1565    step:    10037     combined loss: 4988.56421     paf loss 23.70800     hm loss 4964.85620\n",
      "Iteration:   1570    step:    10042     combined loss: 2167.74134     paf loss 14.61890     hm loss 2153.12244\n",
      "Iteration:   1575    step:    10047     combined loss: 2821.23175     paf loss 17.44562     hm loss 2803.78613\n",
      "Iteration:   1580    step:    10052     combined loss: 4846.68974     paf loss 20.16239     hm loss 4826.52734\n",
      "Iteration:   1585    step:    10057     combined loss: 2708.45053     paf loss 19.58786     hm loss 2688.86267\n",
      "Iteration:   1590    step:    10062     combined loss: 4221.78021     paf loss 20.60833     hm loss 4201.17188\n",
      "Iteration:   1595    step:    10067     combined loss: 5399.10846     paf loss 24.55573     hm loss 5374.55273\n",
      "Iteration:   1600    step:    10072     combined loss: 2643.75979     paf loss 17.08718     hm loss 2626.67261\n",
      "Iteration:   1605    step:    10077     combined loss: 4049.50579     paf loss 25.37616     hm loss 4024.12964\n",
      "Iteration:   1610    step:    10082     combined loss: 3529.20010     paf loss 23.46072     hm loss 3505.73938\n",
      "Iteration:   1615    step:    10087     combined loss: 3202.44001     paf loss 19.85407     hm loss 3182.58594\n",
      "Iteration:   1620    step:    10092     combined loss: 3275.74737     paf loss 14.77898     hm loss 3260.96838\n",
      "Iteration:   1625    step:    10097     combined loss: 4632.83743     paf loss 18.65262     hm loss 4614.18481\n",
      "Iteration:   1630    step:    10102     combined loss: 4090.35270     paf loss 19.67277     hm loss 4070.67993\n",
      "Iteration:   1635    step:    10107     combined loss: 3914.91896     paf loss 20.96986     hm loss 3893.94910\n",
      "Iteration:   1640    step:    10112     combined loss: 2529.15555     paf loss 19.33279     hm loss 2509.82275\n",
      "Iteration:   1645    step:    10117     combined loss: 3732.60205     paf loss 20.21020     hm loss 3712.39185\n",
      "Iteration:   1650    step:    10122     combined loss: 4934.64337     paf loss 19.49079     hm loss 4915.15259\n",
      "Iteration:   1655    step:    10127     combined loss: 5555.61145     paf loss 19.95227     hm loss 5535.65918\n",
      "Iteration:   1660    step:    10132     combined loss: 4045.14866     paf loss 21.53367     hm loss 4023.61499\n",
      "Iteration:   1665    step:    10137     combined loss: 6342.06123     paf loss 20.36372     hm loss 6321.69751\n",
      "Iteration:   1670    step:    10142     combined loss: 4008.77266     paf loss 15.13020     hm loss 3993.64246\n",
      "Iteration:   1675    step:    10147     combined loss: 3495.62702     paf loss 20.13361     hm loss 3475.49341\n",
      "Iteration:   1680    step:    10152     combined loss: 3128.45858     paf loss 17.14767     hm loss 3111.31091\n",
      "Iteration:   1685    step:    10157     combined loss: 3204.57766     paf loss 15.95962     hm loss 3188.61804\n",
      "Iteration:   1690    step:    10162     combined loss: 2805.29990     paf loss 24.29892     hm loss 2781.00098\n",
      "Iteration:   1695    step:    10167     combined loss: 3984.68985     paf loss 19.40957     hm loss 3965.28027\n",
      "Iteration:   1700    step:    10172     combined loss: 4023.76188     paf loss 22.45914     hm loss 4001.30273\n",
      "Iteration:   1705    step:    10177     combined loss: 4946.03240     paf loss 18.70306     hm loss 4927.32935\n",
      "Iteration:   1710    step:    10182     combined loss: 3633.02431     paf loss 22.07644     hm loss 3610.94788\n",
      "Iteration:   1715    step:    10187     combined loss: 3738.94808     paf loss 23.59774     hm loss 3715.35034\n",
      "Iteration:   1720    step:    10192     combined loss: 3113.38511     paf loss 22.81285     hm loss 3090.57227\n",
      "Iteration:   1725    step:    10197     combined loss: 2704.14919     paf loss 16.34621     hm loss 2687.80298\n",
      "Iteration:   1730    step:    10202     combined loss: 2616.49657     paf loss 18.61486     hm loss 2597.88171\n",
      "Iteration:   1735    step:    10207     combined loss: 4980.36659     paf loss 20.73964     hm loss 4959.62695\n",
      "Iteration:   1740    step:    10212     combined loss: 2244.91441     paf loss 21.12364     hm loss 2223.79077\n",
      "Iteration:   1745    step:    10217     combined loss: 4263.25102     paf loss 20.30669     hm loss 4242.94434\n",
      "Iteration:   1750    step:    10222     combined loss: 5601.53921     paf loss 21.86367     hm loss 5579.67554\n",
      "Iteration:   1755    step:    10227     combined loss: 4587.90304     paf loss 23.77462     hm loss 4564.12842\n",
      "Iteration:   1760    step:    10232     combined loss: 4996.66129     paf loss 25.07560     hm loss 4971.58569\n",
      "Iteration:   1765    step:    10237     combined loss: 4262.76205     paf loss 27.87167     hm loss 4234.89038\n",
      "Iteration:   1770    step:    10242     combined loss: 4248.39778     paf loss 22.44905     hm loss 4225.94873\n",
      "Iteration:   1775    step:    10247     combined loss: 2922.06158     paf loss 18.40741     hm loss 2903.65417\n",
      "Iteration:   1780    step:    10252     combined loss: 3905.69114     paf loss 21.37889     hm loss 3884.31226\n",
      "Iteration:   1785    step:    10257     combined loss: 3190.08410     paf loss 19.12878     hm loss 3170.95532\n",
      "Iteration:   1790    step:    10262     combined loss: 6060.23150     paf loss 21.56622     hm loss 6038.66528\n",
      "Iteration:   1795    step:    10267     combined loss: 4774.53329     paf loss 13.64462     hm loss 4760.88867\n",
      "Iteration:   1800    step:    10272     combined loss: 4452.25452     paf loss 20.26404     hm loss 4431.99048\n",
      "Iteration:   1805    step:    10277     combined loss: 3043.82390     paf loss 18.07451     hm loss 3025.74939\n",
      "Iteration:   1810    step:    10282     combined loss: 2716.44009     paf loss 14.01577     hm loss 2702.42432\n",
      "Iteration:   1815    step:    10287     combined loss: 3813.49188     paf loss 20.41937     hm loss 3793.07251\n",
      "Iteration:   1820    step:    10292     combined loss: 3287.64582     paf loss 16.57539     hm loss 3271.07043\n",
      "Iteration:   1825    step:    10297     combined loss: 3333.70372     paf loss 19.57505     hm loss 3314.12866\n",
      "Iteration:   1830    step:    10302     combined loss: 2983.68015     paf loss 20.10007     hm loss 2963.58008\n",
      "Iteration:   1835    step:    10307     combined loss: 2777.59703     paf loss 23.73778     hm loss 2753.85925\n",
      "Iteration:   1840    step:    10312     combined loss: 3009.43164     paf loss 19.19019     hm loss 2990.24146\n",
      "Iteration:   1845    step:    10317     combined loss: 4525.82754     paf loss 23.88930     hm loss 4501.93823\n",
      "Iteration:   1850    step:    10322     combined loss: 4765.90210     paf loss 19.15186     hm loss 4746.75024\n",
      "Iteration:   1855    step:    10327     combined loss: 3340.56583     paf loss 17.11991     hm loss 3323.44592\n",
      "Iteration:   1860    step:    10332     combined loss: 3838.26525     paf loss 20.63585     hm loss 3817.62939\n",
      "Iteration:   1865    step:    10337     combined loss: 4661.42590     paf loss 22.28307     hm loss 4639.14282\n",
      "Iteration:   1870    step:    10342     combined loss: 2678.64411     paf loss 13.99470     hm loss 2664.64941\n",
      "Iteration:   1875    step:    10347     combined loss: 4036.30531     paf loss 23.88319     hm loss 4012.42212\n",
      "Iteration:   1880    step:    10352     combined loss: 3635.57125     paf loss 20.70967     hm loss 3614.86157\n",
      "Iteration:   1885    step:    10357     combined loss: 3103.22220     paf loss 16.82486     hm loss 3086.39734\n",
      "Iteration:   1890    step:    10362     combined loss: 3018.59127     paf loss 18.56661     hm loss 3000.02466\n",
      "Iteration:   1895    step:    10367     combined loss: 2938.88441     paf loss 21.58668     hm loss 2917.29773\n",
      "Iteration:   1900    step:    10372     combined loss: 3118.42497     paf loss 21.05912     hm loss 3097.36584\n",
      "Iteration:   1905    step:    10377     combined loss: 3398.10887     paf loss 19.50438     hm loss 3378.60449\n",
      "Iteration:   1910    step:    10382     combined loss: 2703.98324     paf loss 17.52193     hm loss 2686.46130\n",
      "Iteration:   1915    step:    10387     combined loss: 3447.95598     paf loss 20.60418     hm loss 3427.35181\n",
      "Iteration:   1920    step:    10392     combined loss: 3482.71090     paf loss 19.07723     hm loss 3463.63367\n",
      "Iteration:   1925    step:    10397     combined loss: 3920.75633     paf loss 24.90928     hm loss 3895.84705\n",
      "Iteration:   1930    step:    10402     combined loss: 3146.79662     paf loss 23.26171     hm loss 3123.53491\n",
      "Iteration:   1935    step:    10407     combined loss: 3271.16438     paf loss 21.01069     hm loss 3250.15369\n",
      "Iteration:   1940    step:    10412     combined loss: 4085.70064     paf loss 21.91561     hm loss 4063.78503\n",
      "Iteration:   1945    step:    10417     combined loss: 4203.24877     paf loss 18.98144     hm loss 4184.26733\n",
      "Iteration:   1950    step:    10422     combined loss: 3013.69215     paf loss 19.51673     hm loss 2994.17542\n",
      "Iteration:   1955    step:    10427     combined loss: 2748.12189     paf loss 20.50238     hm loss 2727.61951\n",
      "Iteration:   1960    step:    10432     combined loss: 3175.80940     paf loss 21.40620     hm loss 3154.40320\n",
      "Iteration:   1965    step:    10437     combined loss: 5662.96997     paf loss 23.09229     hm loss 5639.87769\n",
      "Iteration:   1970    step:    10442     combined loss: 3912.84589     paf loss 16.95930     hm loss 3895.88660\n",
      "Iteration:   1975    step:    10447     combined loss: 3895.84368     paf loss 20.03862     hm loss 3875.80505\n",
      "Iteration:   1980    step:    10452     combined loss: 3912.03852     paf loss 24.33197     hm loss 3887.70654\n",
      "Iteration:   1985    step:    10457     combined loss: 4923.33188     paf loss 22.04013     hm loss 4901.29175\n",
      "Iteration:   1990    step:    10462     combined loss: 2485.52513     paf loss 15.03612     hm loss 2470.48901\n",
      "Iteration:   1995    step:    10467     combined loss: 4597.70713     paf loss 21.26206     hm loss 4576.44507\n",
      "Iteration:   2000    step:    10472     combined loss: 3252.91025     paf loss 19.44858     hm loss 3233.46167\n",
      "Iteration:   2005    step:    10477     combined loss: 5643.13080     paf loss 23.03998     hm loss 5620.09082\n",
      "Iteration:   2010    step:    10482     combined loss: 2495.72457     paf loss 14.20565     hm loss 2481.51892\n",
      "Iteration:   2015    step:    10487     combined loss: 4945.55835     paf loss 21.14063     hm loss 4924.41772\n",
      "Iteration:   2020    step:    10492     combined loss: 2285.37183     paf loss 15.73804     hm loss 2269.63379\n",
      "Iteration:   2025    step:    10497     combined loss: 2938.54526     paf loss 16.92770     hm loss 2921.61755\n",
      "Iteration:   2030    step:    10502     combined loss: 4588.41002     paf loss 25.94029     hm loss 4562.46973\n",
      "Iteration:   2035    step:    10507     combined loss: 4224.42566     paf loss 18.30384     hm loss 4206.12183\n",
      "Iteration:   2040    step:    10512     combined loss: 3056.81393     paf loss 15.62143     hm loss 3041.19250\n",
      "Iteration:   2045    step:    10517     combined loss: 2236.70283     paf loss 13.69599     hm loss 2223.00684\n",
      "Iteration:   2050    step:    10522     combined loss: 2884.33436     paf loss 15.26332     hm loss 2869.07104\n",
      "Iteration:   2055    step:    10527     combined loss: 4229.25163     paf loss 18.75846     hm loss 4210.49316\n",
      "Iteration:   2060    step:    10532     combined loss: 2558.92598     paf loss 17.86140     hm loss 2541.06458\n",
      "Iteration:   2065    step:    10537     combined loss: 2916.18371     paf loss 27.32018     hm loss 2888.86353\n",
      "Iteration:   2070    step:    10542     combined loss: 4509.88841     paf loss 17.55785     hm loss 4492.33057\n",
      "Iteration:   2075    step:    10547     combined loss: 5118.75433     paf loss 30.44427     hm loss 5088.31006\n",
      "Iteration:   2080    step:    10552     combined loss: 4051.05968     paf loss 26.27001     hm loss 4024.78967\n",
      "Iteration:   2085    step:    10557     combined loss: 3039.36228     paf loss 14.18039     hm loss 3025.18188\n",
      "Iteration:   2090    step:    10562     combined loss: 3096.32383     paf loss 17.09348     hm loss 3079.23035\n",
      "Iteration:   2095    step:    10567     combined loss: 3668.12110     paf loss 18.04041     hm loss 3650.08069\n",
      "Iteration:   2100    step:    10572     combined loss: 5971.03118     paf loss 18.03631     hm loss 5952.99487\n",
      "Iteration:   2105    step:    10577     combined loss: 4311.89230     paf loss 20.02243     hm loss 4291.86987\n",
      "Iteration:   2110    step:    10582     combined loss: 4430.22594     paf loss 19.79405     hm loss 4410.43188\n",
      "Iteration:   2115    step:    10587     combined loss: 4603.13074     paf loss 21.00305     hm loss 4582.12769\n",
      "Iteration:   2120    step:    10592     combined loss: 4163.04594     paf loss 17.30449     hm loss 4145.74146\n",
      "Iteration:   2125    step:    10597     combined loss: 3440.79253     paf loss 21.06377     hm loss 3419.72876\n",
      "Iteration:   2130    step:    10602     combined loss: 2221.93655     paf loss 15.36148     hm loss 2206.57507\n",
      "Iteration:   2135    step:    10607     combined loss: 3273.39746     paf loss 22.28137     hm loss 3251.11609\n",
      "Iteration:   2140    step:    10612     combined loss: 3845.78183     paf loss 26.03830     hm loss 3819.74353\n",
      "Iteration:   2145    step:    10617     combined loss: 5883.48220     paf loss 32.09182     hm loss 5851.39038\n",
      "Iteration:   2150    step:    10622     combined loss: 5866.40435     paf loss 30.88018     hm loss 5835.52417\n",
      "Iteration:   2155    step:    10627     combined loss: 3863.37311     paf loss 25.92231     hm loss 3837.45081\n",
      "Iteration:   2160    step:    10632     combined loss: 2239.05111     paf loss 13.34433     hm loss 2225.70679\n",
      "Iteration:   2165    step:    10637     combined loss: 2760.60971     paf loss 17.57407     hm loss 2743.03564\n",
      "Iteration:   2170    step:    10642     combined loss: 4683.72381     paf loss 20.09124     hm loss 4663.63257\n",
      "Iteration:   2175    step:    10647     combined loss: 3410.32620     paf loss 18.15603     hm loss 3392.17017\n",
      "Iteration:   2180    step:    10652     combined loss: 3312.43594     paf loss 14.81827     hm loss 3297.61768\n",
      "Iteration:   2185    step:    10657     combined loss: 2725.43687     paf loss 11.78550     hm loss 2713.65137\n",
      "Iteration:   2190    step:    10662     combined loss: 3736.91456     paf loss 24.77406     hm loss 3712.14050\n",
      "Iteration:   2195    step:    10667     combined loss: 2645.45664     paf loss 15.11558     hm loss 2630.34106\n",
      "Iteration:   2200    step:    10672     combined loss: 4772.19952     paf loss 25.68390     hm loss 4746.51562\n",
      "Iteration:   2205    step:    10677     combined loss: 6095.26333     paf loss 22.92300     hm loss 6072.34033\n",
      "Iteration:   2210    step:    10682     combined loss: 4531.03745     paf loss 28.80112     hm loss 4502.23633\n",
      "Iteration:   2215    step:    10687     combined loss: 2784.29107     paf loss 17.98284     hm loss 2766.30823\n",
      "Iteration:   2220    step:    10692     combined loss: 3342.69695     paf loss 15.84661     hm loss 3326.85034\n",
      "Iteration:   2225    step:    10697     combined loss: 4029.61277     paf loss 19.13474     hm loss 4010.47803\n",
      "Iteration:   2230    step:    10702     combined loss: 3688.89161     paf loss 22.64478     hm loss 3666.24683\n",
      "Iteration:   2235    step:    10707     combined loss: 4518.74609     paf loss 18.64648     hm loss 4500.09961\n",
      "Iteration:   2240    step:    10712     combined loss: 2754.81405     paf loss 19.08822     hm loss 2735.72583\n",
      "Iteration:   2245    step:    10717     combined loss: 3526.77300     paf loss 17.68730     hm loss 3509.08569\n",
      "Iteration:   2250    step:    10722     combined loss: 2600.99546     paf loss 15.97105     hm loss 2585.02441\n",
      "Iteration:   2255    step:    10727     combined loss: 3958.35465     paf loss 17.06327     hm loss 3941.29138\n",
      "Iteration:   2260    step:    10732     combined loss: 4845.20359     paf loss 23.47947     hm loss 4821.72412\n",
      "Iteration:   2265    step:    10737     combined loss: 3295.66304     paf loss 20.13069     hm loss 3275.53235\n",
      "Iteration:   2270    step:    10742     combined loss: 3091.50123     paf loss 20.93494     hm loss 3070.56628\n",
      "Iteration:   2275    step:    10747     combined loss: 2925.13722     paf loss 14.48341     hm loss 2910.65381\n",
      "Iteration:   2280    step:    10752     combined loss: 3783.73075     paf loss 16.74528     hm loss 3766.98547\n",
      "Iteration:   2285    step:    10757     combined loss: 2222.48605     paf loss 14.08163     hm loss 2208.40442\n",
      "Iteration:   2290    step:    10762     combined loss: 4507.69014     paf loss 27.53755     hm loss 4480.15259\n",
      "Iteration:   2295    step:    10767     combined loss: 3314.55221     paf loss 17.59603     hm loss 3296.95618\n",
      "Iteration:   2300    step:    10772     combined loss: 3488.62878     paf loss 19.07007     hm loss 3469.55872\n",
      "Iteration:   2305    step:    10777     combined loss: 3815.16254     paf loss 17.10712     hm loss 3798.05542\n",
      "Iteration:   2310    step:    10782     combined loss: 3367.79337     paf loss 19.37833     hm loss 3348.41504\n",
      "Iteration:   2315    step:    10787     combined loss: 3405.16528     paf loss 16.63183     hm loss 3388.53345\n",
      "Iteration:   2320    step:    10792     combined loss: 2359.41561     paf loss 18.33346     hm loss 2341.08215\n",
      "Iteration:   2325    step:    10797     combined loss: 3943.96931     paf loss 26.01985     hm loss 3917.94946\n",
      "Iteration:   2330    step:    10802     combined loss: 3164.66553     paf loss 20.41309     hm loss 3144.25244\n",
      "Iteration:   2335    step:    10807     combined loss: 3280.56939     paf loss 19.62469     hm loss 3260.94470\n",
      "Iteration:   2340    step:    10812     combined loss: 3011.05644     paf loss 16.53728     hm loss 2994.51917\n",
      "Iteration:   2345    step:    10817     combined loss: 3708.48905     paf loss 20.74100     hm loss 3687.74805\n",
      "Iteration:   2350    step:    10822     combined loss: 2563.55708     paf loss 18.07662     hm loss 2545.48047\n",
      "Iteration:   2355    step:    10827     combined loss: 3160.49051     paf loss 19.32096     hm loss 3141.16956\n",
      "Iteration:   2360    step:    10832     combined loss: 3255.44489     paf loss 27.36042     hm loss 3228.08447\n",
      "Iteration:   2365    step:    10837     combined loss: 3806.72401     paf loss 17.79664     hm loss 3788.92737\n",
      "Iteration:   2370    step:    10842     combined loss: 3426.05272     paf loss 18.56822     hm loss 3407.48450\n",
      "Iteration:   2375    step:    10847     combined loss: 3112.77130     paf loss 19.30902     hm loss 3093.46228\n",
      "Iteration:   2380    step:    10852     combined loss: 4518.55840     paf loss 22.44659     hm loss 4496.11182\n",
      "Iteration:   2385    step:    10857     combined loss: 4043.45090     paf loss 18.25351     hm loss 4025.19739\n",
      "Iteration:   2390    step:    10862     combined loss: 4454.38743     paf loss 18.69261     hm loss 4435.69482\n",
      "Iteration:   2395    step:    10867     combined loss: 2378.97179     paf loss 16.92577     hm loss 2362.04602\n",
      "Iteration:   2400    step:    10872     combined loss: 2998.84860     paf loss 16.49203     hm loss 2982.35657\n",
      "Iteration:   2405    step:    10877     combined loss: 5090.36257     paf loss 26.34010     hm loss 5064.02246\n",
      "Iteration:   2410    step:    10882     combined loss: 2503.55700     paf loss 17.87218     hm loss 2485.68481\n",
      "Iteration:   2415    step:    10887     combined loss: 2560.21734     paf loss 15.46966     hm loss 2544.74768\n",
      "Iteration:   2420    step:    10892     combined loss: 4638.38467     paf loss 17.19228     hm loss 4621.19238\n",
      "Iteration:   2425    step:    10897     combined loss: 3990.20310     paf loss 18.82981     hm loss 3971.37329\n",
      "Iteration:   2430    step:    10902     combined loss: 3862.49615     paf loss 22.49579     hm loss 3840.00037\n",
      "Iteration:   2435    step:    10907     combined loss: 3797.84092     paf loss 21.92051     hm loss 3775.92041\n",
      "Iteration:   2440    step:    10912     combined loss: 2847.15904     paf loss 18.47447     hm loss 2828.68457\n",
      "Iteration:   2445    step:    10917     combined loss: 4273.77163     paf loss 19.69131     hm loss 4254.08032\n",
      "Iteration:   2450    step:    10922     combined loss: 2796.10352     paf loss 18.49585     hm loss 2777.60767\n",
      "Iteration:   2455    step:    10927     combined loss: 6905.06377     paf loss 25.55474     hm loss 6879.50903\n",
      "Iteration:   2460    step:    10932     combined loss: 4369.05698     paf loss 20.81918     hm loss 4348.23779\n",
      "Iteration:   2465    step:    10937     combined loss: 3223.14381     paf loss 17.34779     hm loss 3205.79602\n",
      "Iteration:   2470    step:    10942     combined loss: 3287.47243     paf loss 24.30885     hm loss 3263.16357\n",
      "Iteration:   2475    step:    10947     combined loss: 2673.43288     paf loss 17.19838     hm loss 2656.23450\n",
      "Iteration:   2480    step:    10952     combined loss: 3392.60812     paf loss 18.06734     hm loss 3374.54077\n",
      "Iteration:   2485    step:    10957     combined loss: 3076.64343     paf loss 15.00989     hm loss 3061.63354\n",
      "Iteration:   2490    step:    10962     combined loss: 2959.34647     paf loss 17.51005     hm loss 2941.83643\n",
      "Iteration:   2495    step:    10967     combined loss: 3931.63354     paf loss 20.54308     hm loss 3911.09045\n",
      "Iteration:   2500    step:    10972     combined loss: 3478.65852     paf loss 17.96980     hm loss 3460.68872\n",
      "Iteration:   2505    step:    10977     combined loss: 3097.48103     paf loss 15.19441     hm loss 3082.28662\n",
      "Iteration:   2510    step:    10982     combined loss: 4248.32379     paf loss 19.04156     hm loss 4229.28223\n",
      "Iteration:   2515    step:    10987     combined loss: 4034.24869     paf loss 23.63492     hm loss 4010.61377\n",
      "Iteration:   2520    step:    10992     combined loss: 4992.72820     paf loss 26.15080     hm loss 4966.57739\n",
      "Iteration:   2525    step:    10997     combined loss: 3812.68737     paf loss 23.12474     hm loss 3789.56262\n",
      "Iteration:   2530    step:    11002     combined loss: 3005.82308     paf loss 20.35494     hm loss 2985.46814\n",
      "Iteration:   2535    step:    11007     combined loss: 4304.23924     paf loss 18.07957     hm loss 4286.15967\n",
      "Iteration:   2540    step:    11012     combined loss: 4569.54221     paf loss 19.36447     hm loss 4550.17773\n",
      "Iteration:   2545    step:    11017     combined loss: 4192.82995     paf loss 23.75647     hm loss 4169.07349\n",
      "Iteration:   2550    step:    11022     combined loss: 5522.18596     paf loss 26.99821     hm loss 5495.18774\n",
      "Iteration:   2555    step:    11027     combined loss: 5238.97356     paf loss 20.33391     hm loss 5218.63965\n",
      "Iteration:   2560    step:    11032     combined loss: 4095.59930     paf loss 24.96417     hm loss 4070.63513\n",
      "Iteration:   2565    step:    11037     combined loss: 2232.25048     paf loss 15.52538     hm loss 2216.72510\n",
      "Iteration:   2570    step:    11042     combined loss: 2939.06869     paf loss 27.50899     hm loss 2911.55969\n",
      "Iteration:   2575    step:    11047     combined loss: 3556.39289     paf loss 26.04377     hm loss 3530.34912\n",
      "Iteration:   2580    step:    11052     combined loss: 5744.49032     paf loss 23.82235     hm loss 5720.66797\n",
      "Iteration:   2585    step:    11057     combined loss: 3692.67154     paf loss 17.76737     hm loss 3674.90417\n",
      "Iteration:   2590    step:    11062     combined loss: 3643.62092     paf loss 17.86543     hm loss 3625.75549\n",
      "Iteration:   2595    step:    11067     combined loss: 3544.35748     paf loss 20.78497     hm loss 3523.57251\n",
      "Iteration:   2600    step:    11072     combined loss: 3900.64482     paf loss 18.73040     hm loss 3881.91443\n",
      "Iteration:   2605    step:    11077     combined loss: 3360.73716     paf loss 19.55930     hm loss 3341.17786\n",
      "Iteration:   2610    step:    11082     combined loss: 1954.32049     paf loss 14.58697     hm loss 1939.73352\n",
      "Iteration:   2615    step:    11087     combined loss: 4230.84010     paf loss 18.12257     hm loss 4212.71753\n",
      "learning rate change: 6.25e-05 --> 3.125e-05\n",
      "Iteration:   2620    step:    11092     combined loss: 3023.86475     paf loss 21.13465     hm loss 3002.73010\n",
      "Iteration:   2625    step:    11097     combined loss: 3757.72946     paf loss 24.50327     hm loss 3733.22620\n",
      "Iteration:   2630    step:    11102     combined loss: 6235.64367     paf loss 29.57701     hm loss 6206.06665\n",
      "Iteration:   2635    step:    11107     combined loss: 2323.04022     paf loss 12.63214     hm loss 2310.40808\n",
      "Iteration:   2640    step:    11112     combined loss: 2630.52323     paf loss 25.85551     hm loss 2604.66772\n",
      "Iteration:   2645    step:    11117     combined loss: 3112.71740     paf loss 14.60302     hm loss 3098.11438\n",
      "Iteration:   2650    step:    11122     combined loss: 2402.11472     paf loss 20.16709     hm loss 2381.94763\n",
      "Iteration:   2655    step:    11127     combined loss: 5031.23060     paf loss 27.83949     hm loss 5003.39111\n",
      "Iteration:   2660    step:    11132     combined loss: 3880.61466     paf loss 19.45609     hm loss 3861.15857\n",
      "Iteration:   2665    step:    11137     combined loss: 6417.80876     paf loss 22.42350     hm loss 6395.38525\n",
      "Iteration:   2670    step:    11142     combined loss: 3534.62606     paf loss 17.32723     hm loss 3517.29883\n",
      "Iteration:   2675    step:    11147     combined loss: 2945.55622     paf loss 13.53412     hm loss 2932.02209\n",
      "Iteration:   2680    step:    11152     combined loss: 4169.13630     paf loss 22.37141     hm loss 4146.76489\n",
      "Iteration:   2685    step:    11157     combined loss: 2202.88479     paf loss 15.34280     hm loss 2187.54199\n",
      "Iteration:   2690    step:    11162     combined loss: 3220.66296     paf loss 20.71008     hm loss 3199.95288\n",
      "Iteration:   2695    step:    11167     combined loss: 2178.18552     paf loss 14.75547     hm loss 2163.43005\n",
      "Iteration:   2700    step:    11172     combined loss: 4206.07113     paf loss 22.57138     hm loss 4183.49976\n",
      "Iteration:   2705    step:    11177     combined loss: 3975.36625     paf loss 23.29715     hm loss 3952.06909\n",
      "Iteration:   2710    step:    11182     combined loss: 3557.40537     paf loss 20.69480     hm loss 3536.71057\n",
      "Iteration:   2715    step:    11187     combined loss: 2899.91814     paf loss 15.90264     hm loss 2884.01550\n",
      "Iteration:   2720    step:    11192     combined loss: 3974.78241     paf loss 22.26166     hm loss 3952.52075\n",
      "Iteration:   2725    step:    11197     combined loss: 3015.69334     paf loss 19.71410     hm loss 2995.97925\n",
      "Iteration:   2730    step:    11202     combined loss: 3599.88665     paf loss 21.77642     hm loss 3578.11023\n",
      "Iteration:   2735    step:    11207     combined loss: 3125.72182     paf loss 16.67763     hm loss 3109.04419\n",
      "Iteration:   2740    step:    11212     combined loss: 3544.22761     paf loss 20.40473     hm loss 3523.82288\n",
      "Iteration:   2745    step:    11217     combined loss: 2826.93926     paf loss 14.74285     hm loss 2812.19641\n",
      "Iteration:   2750    step:    11222     combined loss: 3486.78725     paf loss 18.21669     hm loss 3468.57056\n",
      "Iteration:   2755    step:    11227     combined loss: 4805.73804     paf loss 24.90674     hm loss 4780.83130\n",
      "Iteration:   2760    step:    11232     combined loss: 2207.32655     paf loss 16.51881     hm loss 2190.80774\n",
      "Iteration:   2765    step:    11237     combined loss: 3698.87504     paf loss 22.58622     hm loss 3676.28882\n",
      "Iteration:   2770    step:    11242     combined loss: 2768.97041     paf loss 17.93989     hm loss 2751.03052\n",
      "Iteration:   2775    step:    11247     combined loss: 3300.21948     paf loss 16.65662     hm loss 3283.56287\n",
      "Iteration:   2780    step:    11252     combined loss: 3733.33795     paf loss 19.72199     hm loss 3713.61597\n",
      "Iteration:   2785    step:    11257     combined loss: 2786.01513     paf loss 19.56945     hm loss 2766.44568\n",
      "Iteration:   2790    step:    11262     combined loss: 4871.48719     paf loss 16.67176     hm loss 4854.81543\n",
      "Iteration:   2795    step:    11267     combined loss: 2442.45436     paf loss 18.55837     hm loss 2423.89600\n",
      "Iteration:   2800    step:    11272     combined loss: 3547.60054     paf loss 23.45344     hm loss 3524.14709\n",
      "Iteration:   2805    step:    11277     combined loss: 4541.80941     paf loss 22.19369     hm loss 4519.61572\n",
      "Iteration:   2810    step:    11282     combined loss: 3679.56800     paf loss 22.66419     hm loss 3656.90381\n",
      "Iteration:   2815    step:    11287     combined loss: 3158.81984     paf loss 20.38893     hm loss 3138.43091\n",
      "Iteration:   2820    step:    11292     combined loss: 4423.46906     paf loss 21.13117     hm loss 4402.33789\n",
      "Train Loss: 3696.4235    PAF Loss:  19.9841    HM Loss:  3676.4394    Acc: NA\n",
      "Val Loss: 3975.5471    PAF Loss:  16.4293    HM Loss:  3959.1178     Acc: NA\n",
      "Epoch 4/9\n",
      "----------\n",
      "Iteration:      0    step:    11296     combined loss: 3953.16516     paf loss 22.56518     hm loss 3930.59998\n",
      "Iteration:      5    step:    11301     combined loss: 3625.23893     paf loss 24.59659     hm loss 3600.64233\n",
      "Iteration:     10    step:    11306     combined loss: 5038.41073     paf loss 20.79062     hm loss 5017.62012\n",
      "Iteration:     15    step:    11311     combined loss: 4068.16987     paf loss 27.19196     hm loss 4040.97791\n",
      "Iteration:     20    step:    11316     combined loss: 5480.01466     paf loss 22.64137     hm loss 5457.37329\n",
      "Iteration:     25    step:    11321     combined loss: 3658.10320     paf loss 20.05571     hm loss 3638.04749\n",
      "Iteration:     30    step:    11326     combined loss: 3534.94459     paf loss 19.24452     hm loss 3515.70007\n",
      "Iteration:     35    step:    11331     combined loss: 3284.74212     paf loss 20.89959     hm loss 3263.84253\n",
      "Iteration:     40    step:    11336     combined loss: 3717.66614     paf loss 16.03418     hm loss 3701.63196\n",
      "Iteration:     45    step:    11341     combined loss: 4603.53109     paf loss 21.29818     hm loss 4582.23291\n",
      "Iteration:     50    step:    11346     combined loss: 3384.64718     paf loss 21.06308     hm loss 3363.58411\n",
      "Iteration:     55    step:    11351     combined loss: 2702.86886     paf loss 15.91159     hm loss 2686.95728\n",
      "Iteration:     60    step:    11356     combined loss: 4120.99010     paf loss 25.54808     hm loss 4095.44202\n",
      "Iteration:     65    step:    11361     combined loss: 2560.61161     paf loss 14.44938     hm loss 2546.16223\n",
      "Iteration:     70    step:    11366     combined loss: 3128.91776     paf loss 16.57291     hm loss 3112.34485\n",
      "Iteration:     75    step:    11371     combined loss: 3451.60751     paf loss 22.13973     hm loss 3429.46777\n",
      "Iteration:     80    step:    11376     combined loss: 3161.20519     paf loss 21.76720     hm loss 3139.43799\n",
      "Iteration:     85    step:    11381     combined loss: 3759.10109     paf loss 18.81056     hm loss 3740.29053\n",
      "Iteration:     90    step:    11386     combined loss: 3788.62104     paf loss 22.09382     hm loss 3766.52722\n",
      "Iteration:     95    step:    11391     combined loss: 3689.21246     paf loss 20.12396     hm loss 3669.08850\n",
      "Iteration:    100    step:    11396     combined loss: 5096.33080     paf loss 30.18261     hm loss 5066.14819\n",
      "Iteration:    105    step:    11401     combined loss: 2691.80642     paf loss 18.68765     hm loss 2673.11877\n",
      "Iteration:    110    step:    11406     combined loss: 2693.49572     paf loss 14.78783     hm loss 2678.70789\n",
      "Iteration:    115    step:    11411     combined loss: 4399.49542     paf loss 21.93658     hm loss 4377.55884\n",
      "Iteration:    120    step:    11416     combined loss: 4868.41402     paf loss 25.89400     hm loss 4842.52002\n",
      "Iteration:    125    step:    11421     combined loss: 2929.79646     paf loss 19.43037     hm loss 2910.36609\n",
      "Iteration:    130    step:    11426     combined loss: 2924.19881     paf loss 17.64851     hm loss 2906.55029\n",
      "Iteration:    135    step:    11431     combined loss: 3000.34607     paf loss 15.84644     hm loss 2984.49963\n",
      "Iteration:    140    step:    11436     combined loss: 3072.09280     paf loss 13.77481     hm loss 3058.31799\n",
      "Iteration:    145    step:    11441     combined loss: 3413.13331     paf loss 21.60914     hm loss 3391.52417\n",
      "Iteration:    150    step:    11446     combined loss: 2657.15802     paf loss 13.82685     hm loss 2643.33118\n",
      "Iteration:    155    step:    11451     combined loss: 3547.61796     paf loss 19.70536     hm loss 3527.91260\n",
      "Iteration:    160    step:    11456     combined loss: 5024.03859     paf loss 22.15431     hm loss 5001.88428\n",
      "Iteration:    165    step:    11461     combined loss: 3403.71708     paf loss 19.22551     hm loss 3384.49158\n",
      "Iteration:    170    step:    11466     combined loss: 2468.95085     paf loss 14.30851     hm loss 2454.64233\n",
      "Iteration:    175    step:    11471     combined loss: 2432.92341     paf loss 16.13154     hm loss 2416.79187\n",
      "Iteration:    180    step:    11476     combined loss: 2991.65456     paf loss 15.05129     hm loss 2976.60327\n",
      "Iteration:    185    step:    11481     combined loss: 3689.80486     paf loss 22.74871     hm loss 3667.05615\n",
      "Iteration:    190    step:    11486     combined loss: 2563.26205     paf loss 17.47995     hm loss 2545.78210\n",
      "Iteration:    195    step:    11491     combined loss: 3608.31646     paf loss 21.29534     hm loss 3587.02112\n",
      "Iteration:    200    step:    11496     combined loss: 3138.28779     paf loss 17.83638     hm loss 3120.45142\n",
      "Iteration:    205    step:    11501     combined loss: 4234.12659     paf loss 20.36341     hm loss 4213.76318\n",
      "Iteration:    210    step:    11506     combined loss: 4458.77750     paf loss 22.60098     hm loss 4436.17651\n",
      "Iteration:    215    step:    11511     combined loss: 3058.16993     paf loss 21.01173     hm loss 3037.15820\n",
      "Iteration:    220    step:    11516     combined loss: 3758.98898     paf loss 24.42685     hm loss 3734.56213\n",
      "Iteration:    225    step:    11521     combined loss: 4550.61351     paf loss 18.46727     hm loss 4532.14624\n",
      "Iteration:    230    step:    11526     combined loss: 2807.24616     paf loss 15.38483     hm loss 2791.86133\n",
      "Iteration:    235    step:    11531     combined loss: 3011.10541     paf loss 24.95502     hm loss 2986.15039\n",
      "Iteration:    240    step:    11536     combined loss: 4335.46139     paf loss 23.99605     hm loss 4311.46533\n",
      "Iteration:    245    step:    11541     combined loss: 2011.35197     paf loss 15.63254     hm loss 1995.71942\n",
      "Iteration:    250    step:    11546     combined loss: 4375.35931     paf loss 21.45623     hm loss 4353.90308\n",
      "Iteration:    255    step:    11551     combined loss: 4006.27898     paf loss 17.84490     hm loss 3988.43408\n",
      "Iteration:    260    step:    11556     combined loss: 4082.45297     paf loss 15.46274     hm loss 4066.99023\n",
      "Iteration:    265    step:    11561     combined loss: 3414.84100     paf loss 19.56659     hm loss 3395.27441\n",
      "Iteration:    270    step:    11566     combined loss: 2833.57824     paf loss 19.75818     hm loss 2813.82007\n",
      "Iteration:    275    step:    11571     combined loss: 2805.63502     paf loss 14.75795     hm loss 2790.87708\n",
      "Iteration:    280    step:    11576     combined loss: 2876.83494     paf loss 16.86106     hm loss 2859.97388\n",
      "Iteration:    285    step:    11581     combined loss: 2617.54674     paf loss 21.34594     hm loss 2596.20081\n",
      "Iteration:    290    step:    11586     combined loss: 2908.80899     paf loss 17.30972     hm loss 2891.49927\n",
      "Iteration:    295    step:    11591     combined loss: 2525.23649     paf loss 16.23796     hm loss 2508.99854\n",
      "Iteration:    300    step:    11596     combined loss: 4251.62892     paf loss 26.04323     hm loss 4225.58569\n",
      "Iteration:    305    step:    11601     combined loss: 2530.49079     paf loss 17.31794     hm loss 2513.17285\n",
      "Iteration:    310    step:    11606     combined loss: 2122.41537     paf loss 18.20016     hm loss 2104.21521\n",
      "Iteration:    315    step:    11611     combined loss: 2553.80959     paf loss 18.58437     hm loss 2535.22522\n",
      "Iteration:    320    step:    11616     combined loss: 3232.49319     paf loss 19.67141     hm loss 3212.82178\n",
      "Iteration:    325    step:    11621     combined loss: 2520.01118     paf loss 17.90925     hm loss 2502.10193\n",
      "Iteration:    330    step:    11626     combined loss: 3347.13905     paf loss 23.68068     hm loss 3323.45837\n",
      "Iteration:    335    step:    11631     combined loss: 3210.83452     paf loss 19.35625     hm loss 3191.47827\n",
      "Iteration:    340    step:    11636     combined loss: 5937.05474     paf loss 30.10747     hm loss 5906.94727\n",
      "Iteration:    345    step:    11641     combined loss: 3012.33573     paf loss 17.06767     hm loss 2995.26807\n",
      "Iteration:    350    step:    11646     combined loss: 3710.08565     paf loss 21.15767     hm loss 3688.92798\n",
      "Iteration:    355    step:    11651     combined loss: 3980.34631     paf loss 25.56323     hm loss 3954.78308\n",
      "Iteration:    360    step:    11656     combined loss: 3592.29094     paf loss 19.40239     hm loss 3572.88855\n",
      "Iteration:    365    step:    11661     combined loss: 5079.84444     paf loss 27.03072     hm loss 5052.81372\n",
      "Iteration:    370    step:    11666     combined loss: 3194.78149     paf loss 15.90735     hm loss 3178.87415\n",
      "Iteration:    375    step:    11671     combined loss: 4209.78781     paf loss 17.13619     hm loss 4192.65161\n",
      "Iteration:    380    step:    11676     combined loss: 4762.57995     paf loss 27.78918     hm loss 4734.79077\n",
      "Iteration:    385    step:    11681     combined loss: 3239.92815     paf loss 24.20659     hm loss 3215.72156\n",
      "Iteration:    390    step:    11686     combined loss: 4766.76493     paf loss 30.90361     hm loss 4735.86133\n",
      "Iteration:    395    step:    11691     combined loss: 3945.02319     paf loss 18.97437     hm loss 3926.04883\n",
      "Iteration:    400    step:    11696     combined loss: 3623.58678     paf loss 21.33824     hm loss 3602.24854\n",
      "Iteration:    405    step:    11701     combined loss: 4703.18797     paf loss 20.82322     hm loss 4682.36475\n",
      "Iteration:    410    step:    11706     combined loss: 3810.01987     paf loss 23.79745     hm loss 3786.22241\n",
      "Iteration:    415    step:    11711     combined loss: 3154.88218     paf loss 20.80027     hm loss 3134.08191\n",
      "Iteration:    420    step:    11716     combined loss: 3207.12078     paf loss 23.00885     hm loss 3184.11194\n",
      "Iteration:    425    step:    11721     combined loss: 4134.73716     paf loss 28.17759     hm loss 4106.55957\n",
      "Iteration:    430    step:    11726     combined loss: 3382.11021     paf loss 23.08567     hm loss 3359.02454\n",
      "Iteration:    435    step:    11731     combined loss: 3937.89043     paf loss 28.21624     hm loss 3909.67419\n",
      "Iteration:    440    step:    11736     combined loss: 4200.96615     paf loss 22.24008     hm loss 4178.72607\n",
      "Iteration:    445    step:    11741     combined loss: 2729.89447     paf loss 14.13751     hm loss 2715.75696\n",
      "Iteration:    450    step:    11746     combined loss: 4248.14526     paf loss 28.41845     hm loss 4219.72681\n",
      "Iteration:    455    step:    11751     combined loss: 5064.33791     paf loss 23.86110     hm loss 5040.47681\n",
      "Iteration:    460    step:    11756     combined loss: 2864.76917     paf loss 20.54858     hm loss 2844.22058\n",
      "Iteration:    465    step:    11761     combined loss: 3718.48245     paf loss 24.23025     hm loss 3694.25220\n",
      "Iteration:    470    step:    11766     combined loss: 2322.28981     paf loss 18.63978     hm loss 2303.65002\n",
      "Iteration:    475    step:    11771     combined loss: 4463.10424     paf loss 23.10326     hm loss 4440.00098\n",
      "Iteration:    480    step:    11776     combined loss: 3904.00981     paf loss 18.89446     hm loss 3885.11536\n",
      "Iteration:    485    step:    11781     combined loss: 2889.89778     paf loss 13.56099     hm loss 2876.33679\n",
      "Iteration:    490    step:    11786     combined loss: 3154.10283     paf loss 14.95744     hm loss 3139.14539\n",
      "Iteration:    495    step:    11791     combined loss: 4459.77287     paf loss 16.15642     hm loss 4443.61646\n",
      "Iteration:    500    step:    11796     combined loss: 3565.52448     paf loss 17.72455     hm loss 3547.79993\n",
      "Iteration:    505    step:    11801     combined loss: 2435.93056     paf loss 18.65138     hm loss 2417.27917\n",
      "Iteration:    510    step:    11806     combined loss: 5339.41257     paf loss 19.01975     hm loss 5320.39282\n",
      "Iteration:    515    step:    11811     combined loss: 3696.81555     paf loss 21.00562     hm loss 3675.80994\n",
      "Iteration:    520    step:    11816     combined loss: 3344.45180     paf loss 14.19277     hm loss 3330.25903\n",
      "Iteration:    525    step:    11821     combined loss: 2674.29854     paf loss 19.13582     hm loss 2655.16272\n",
      "Iteration:    530    step:    11826     combined loss: 3714.56603     paf loss 17.14733     hm loss 3697.41870\n",
      "Iteration:    535    step:    11831     combined loss: 2192.85114     paf loss 12.37617     hm loss 2180.47498\n",
      "Iteration:    540    step:    11836     combined loss: 3228.53222     paf loss 17.78576     hm loss 3210.74646\n",
      "Iteration:    545    step:    11841     combined loss: 3257.49864     paf loss 20.28453     hm loss 3237.21411\n",
      "Iteration:    550    step:    11846     combined loss: 4097.68874     paf loss 16.65932     hm loss 4081.02942\n",
      "Iteration:    555    step:    11851     combined loss: 4971.56497     paf loss 26.88431     hm loss 4944.68066\n",
      "Iteration:    560    step:    11856     combined loss: 3965.30111     paf loss 20.48886     hm loss 3944.81226\n",
      "Iteration:    565    step:    11861     combined loss: 2821.74950     paf loss 15.59361     hm loss 2806.15588\n",
      "Iteration:    570    step:    11866     combined loss: 3107.97851     paf loss 20.67174     hm loss 3087.30676\n",
      "Iteration:    575    step:    11871     combined loss: 3958.73829     paf loss 20.82032     hm loss 3937.91797\n",
      "Iteration:    580    step:    11876     combined loss: 4054.80132     paf loss 18.99175     hm loss 4035.80957\n",
      "Iteration:    585    step:    11881     combined loss: 3661.26016     paf loss 20.50321     hm loss 3640.75696\n",
      "Iteration:    590    step:    11886     combined loss: 2873.48401     paf loss 13.32605     hm loss 2860.15796\n",
      "Iteration:    595    step:    11891     combined loss: 3113.32655     paf loss 15.44813     hm loss 3097.87842\n",
      "Iteration:    600    step:    11896     combined loss: 2984.75965     paf loss 16.43190     hm loss 2968.32776\n",
      "Iteration:    605    step:    11901     combined loss: 4229.75258     paf loss 18.53359     hm loss 4211.21899\n",
      "Iteration:    610    step:    11906     combined loss: 2534.64479     paf loss 16.47657     hm loss 2518.16821\n",
      "Iteration:    615    step:    11911     combined loss: 3395.55777     paf loss 22.74673     hm loss 3372.81104\n",
      "Iteration:    620    step:    11916     combined loss: 6217.69020     paf loss 23.10035     hm loss 6194.58984\n",
      "Iteration:    625    step:    11921     combined loss: 4464.90736     paf loss 24.72181     hm loss 4440.18555\n",
      "Iteration:    630    step:    11926     combined loss: 4197.73457     paf loss 23.50679     hm loss 4174.22778\n",
      "Iteration:    635    step:    11931     combined loss: 2637.99604     paf loss 15.08271     hm loss 2622.91333\n",
      "Iteration:    640    step:    11936     combined loss: 4074.91196     paf loss 19.66391     hm loss 4055.24805\n",
      "Iteration:    645    step:    11941     combined loss: 4605.65102     paf loss 22.65492     hm loss 4582.99609\n",
      "Iteration:    650    step:    11946     combined loss: 2776.30137     paf loss 20.22666     hm loss 2756.07471\n",
      "Iteration:    655    step:    11951     combined loss: 3860.86192     paf loss 17.59629     hm loss 3843.26562\n",
      "Iteration:    660    step:    11956     combined loss: 3173.47448     paf loss 18.84826     hm loss 3154.62622\n",
      "Iteration:    665    step:    11961     combined loss: 6524.87976     paf loss 23.32898     hm loss 6501.55078\n",
      "Iteration:    670    step:    11966     combined loss: 4611.68671     paf loss 25.23944     hm loss 4586.44727\n",
      "Iteration:    675    step:    11971     combined loss: 2746.57865     paf loss 12.46549     hm loss 2734.11316\n",
      "Iteration:    680    step:    11976     combined loss: 3930.43467     paf loss 24.72459     hm loss 3905.71008\n",
      "Iteration:    685    step:    11981     combined loss: 2431.66850     paf loss 14.54289     hm loss 2417.12561\n",
      "Iteration:    690    step:    11986     combined loss: 3323.21573     paf loss 22.67411     hm loss 3300.54163\n",
      "Iteration:    695    step:    11991     combined loss: 3872.83063     paf loss 22.91901     hm loss 3849.91162\n",
      "Iteration:    700    step:    11996     combined loss: 4008.06370     paf loss 25.85826     hm loss 3982.20544\n",
      "Iteration:    705    step:    12001     combined loss: 3144.20373     paf loss 20.38208     hm loss 3123.82166\n",
      "Iteration:    710    step:    12006     combined loss: 3946.99047     paf loss 20.82250     hm loss 3926.16797\n",
      "Iteration:    715    step:    12011     combined loss: 4222.23344     paf loss 21.79423     hm loss 4200.43921\n",
      "Iteration:    720    step:    12016     combined loss: 4475.35193     paf loss 26.25159     hm loss 4449.10034\n",
      "Iteration:    725    step:    12021     combined loss: 3083.48906     paf loss 19.34502     hm loss 3064.14404\n",
      "Iteration:    730    step:    12026     combined loss: 3948.76807     paf loss 20.86865     hm loss 3927.89941\n",
      "Iteration:    735    step:    12031     combined loss: 2897.06411     paf loss 13.53982     hm loss 2883.52429\n",
      "Iteration:    740    step:    12036     combined loss: 2739.81588     paf loss 17.36251     hm loss 2722.45337\n",
      "Iteration:    745    step:    12041     combined loss: 4424.47372     paf loss 23.02914     hm loss 4401.44458\n",
      "Iteration:    750    step:    12046     combined loss: 4222.75103     paf loss 20.09185     hm loss 4202.65918\n",
      "Iteration:    755    step:    12051     combined loss: 4304.55237     paf loss 18.58118     hm loss 4285.97119\n",
      "Iteration:    760    step:    12056     combined loss: 4271.80831     paf loss 25.22531     hm loss 4246.58301\n",
      "Iteration:    765    step:    12061     combined loss: 4417.66870     paf loss 30.95166     hm loss 4386.71704\n",
      "Iteration:    770    step:    12066     combined loss: 3467.80342     paf loss 17.49214     hm loss 3450.31128\n",
      "Iteration:    775    step:    12071     combined loss: 3614.85472     paf loss 17.78783     hm loss 3597.06689\n",
      "Iteration:    780    step:    12076     combined loss: 2834.78504     paf loss 17.98719     hm loss 2816.79785\n",
      "Iteration:    785    step:    12081     combined loss: 3333.71313     paf loss 17.24048     hm loss 3316.47266\n",
      "Iteration:    790    step:    12086     combined loss: 2534.91791     paf loss 12.35785     hm loss 2522.56006\n",
      "Iteration:    795    step:    12091     combined loss: 2405.84122     paf loss 19.52176     hm loss 2386.31946\n",
      "Iteration:    800    step:    12096     combined loss: 3789.71896     paf loss 20.19040     hm loss 3769.52856\n",
      "Iteration:    805    step:    12101     combined loss: 2182.39070     paf loss 16.02401     hm loss 2166.36670\n",
      "Iteration:    810    step:    12106     combined loss: 1945.10133     paf loss 13.07435     hm loss 1932.02698\n",
      "Iteration:    815    step:    12111     combined loss: 3075.26758     paf loss 25.30726     hm loss 3049.96033\n",
      "Iteration:    820    step:    12116     combined loss: 3664.75787     paf loss 21.90313     hm loss 3642.85474\n",
      "Iteration:    825    step:    12121     combined loss: 3183.14444     paf loss 19.79922     hm loss 3163.34521\n",
      "Iteration:    830    step:    12126     combined loss: 2655.00003     paf loss 14.33523     hm loss 2640.66479\n",
      "Iteration:    835    step:    12131     combined loss: 4232.15182     paf loss 16.79513     hm loss 4215.35669\n",
      "Iteration:    840    step:    12136     combined loss: 3353.72415     paf loss 17.88406     hm loss 3335.84009\n",
      "Iteration:    845    step:    12141     combined loss: 4352.12171     paf loss 15.92127     hm loss 4336.20044\n",
      "Iteration:    850    step:    12146     combined loss: 2500.87091     paf loss 21.04095     hm loss 2479.82996\n",
      "Iteration:    855    step:    12151     combined loss: 2851.76516     paf loss 17.59109     hm loss 2834.17407\n",
      "Iteration:    860    step:    12156     combined loss: 4947.63717     paf loss 21.06296     hm loss 4926.57422\n",
      "Iteration:    865    step:    12161     combined loss: 3766.70584     paf loss 17.95950     hm loss 3748.74634\n",
      "Iteration:    870    step:    12166     combined loss: 3948.06976     paf loss 20.04193     hm loss 3928.02783\n",
      "Iteration:    875    step:    12171     combined loss: 3152.27605     paf loss 21.24407     hm loss 3131.03198\n",
      "Iteration:    880    step:    12176     combined loss: 2432.57992     paf loss 17.73910     hm loss 2414.84082\n",
      "Iteration:    885    step:    12181     combined loss: 4226.74428     paf loss 22.51430     hm loss 4204.22998\n",
      "Iteration:    890    step:    12186     combined loss: 3983.05780     paf loss 23.11041     hm loss 3959.94739\n",
      "Iteration:    895    step:    12191     combined loss: 2809.10190     paf loss 20.66574     hm loss 2788.43616\n",
      "Iteration:    900    step:    12196     combined loss: 2766.41835     paf loss 18.88942     hm loss 2747.52893\n",
      "Iteration:    905    step:    12201     combined loss: 2724.30119     paf loss 20.06987     hm loss 2704.23132\n",
      "Iteration:    910    step:    12206     combined loss: 3737.10408     paf loss 22.67415     hm loss 3714.42993\n",
      "Iteration:    915    step:    12211     combined loss: 4753.70449     paf loss 22.00234     hm loss 4731.70215\n",
      "Iteration:    920    step:    12216     combined loss: 3613.35318     paf loss 16.64834     hm loss 3596.70483\n",
      "Iteration:    925    step:    12221     combined loss: 3011.16480     paf loss 17.03382     hm loss 2994.13098\n",
      "Iteration:    930    step:    12226     combined loss: 3180.00401     paf loss 19.25817     hm loss 3160.74585\n",
      "Iteration:    935    step:    12231     combined loss: 3121.71048     paf loss 15.93960     hm loss 3105.77087\n",
      "Iteration:    940    step:    12236     combined loss: 2628.53981     paf loss 16.54359     hm loss 2611.99622\n",
      "Iteration:    945    step:    12241     combined loss: 3386.93151     paf loss 20.93029     hm loss 3366.00122\n",
      "Iteration:    950    step:    12246     combined loss: 2184.04419     paf loss 16.48084     hm loss 2167.56335\n",
      "Iteration:    955    step:    12251     combined loss: 4173.98086     paf loss 21.73184     hm loss 4152.24902\n",
      "Iteration:    960    step:    12256     combined loss: 3591.68744     paf loss 18.50055     hm loss 3573.18689\n",
      "Iteration:    965    step:    12261     combined loss: 2980.72977     paf loss 19.64628     hm loss 2961.08350\n",
      "Iteration:    970    step:    12266     combined loss: 2410.39252     paf loss 15.62824     hm loss 2394.76428\n",
      "Iteration:    975    step:    12271     combined loss: 3366.63750     paf loss 15.25969     hm loss 3351.37781\n",
      "Iteration:    980    step:    12276     combined loss: 4735.16246     paf loss 16.81310     hm loss 4718.34937\n",
      "Iteration:    985    step:    12281     combined loss: 2426.70394     paf loss 14.89986     hm loss 2411.80408\n",
      "Iteration:    990    step:    12286     combined loss: 3416.25010     paf loss 18.94920     hm loss 3397.30090\n",
      "Iteration:    995    step:    12291     combined loss: 3816.14930     paf loss 23.03309     hm loss 3793.11621\n",
      "Iteration:   1000    step:    12296     combined loss: 4533.36922     paf loss 19.56648     hm loss 4513.80273\n",
      "Iteration:   1005    step:    12301     combined loss: 2917.43967     paf loss 16.59373     hm loss 2900.84595\n",
      "Iteration:   1010    step:    12306     combined loss: 2883.37949     paf loss 16.46848     hm loss 2866.91101\n",
      "Iteration:   1015    step:    12311     combined loss: 2477.24441     paf loss 14.92971     hm loss 2462.31470\n",
      "Iteration:   1020    step:    12316     combined loss: 4181.06649     paf loss 16.76522     hm loss 4164.30127\n",
      "Iteration:   1025    step:    12321     combined loss: 3799.16671     paf loss 24.68551     hm loss 3774.48120\n",
      "Iteration:   1030    step:    12326     combined loss: 4373.89805     paf loss 17.90000     hm loss 4355.99805\n",
      "Iteration:   1035    step:    12331     combined loss: 3240.85496     paf loss 17.97092     hm loss 3222.88403\n",
      "Iteration:   1040    step:    12336     combined loss: 3067.33843     paf loss 14.25188     hm loss 3053.08655\n",
      "Iteration:   1045    step:    12341     combined loss: 3422.74643     paf loss 19.66965     hm loss 3403.07678\n",
      "Iteration:   1050    step:    12346     combined loss: 3821.94389     paf loss 20.72465     hm loss 3801.21924\n",
      "Iteration:   1055    step:    12351     combined loss: 3216.33831     paf loss 24.46929     hm loss 3191.86902\n",
      "Iteration:   1060    step:    12356     combined loss: 3086.79151     paf loss 24.28675     hm loss 3062.50476\n",
      "Iteration:   1065    step:    12361     combined loss: 3079.96812     paf loss 24.54308     hm loss 3055.42505\n",
      "Iteration:   1070    step:    12366     combined loss: 4225.05969     paf loss 29.94274     hm loss 4195.11694\n",
      "Iteration:   1075    step:    12371     combined loss: 3887.46682     paf loss 19.54470     hm loss 3867.92212\n",
      "Iteration:   1080    step:    12376     combined loss: 3060.70384     paf loss 24.19066     hm loss 3036.51318\n",
      "Iteration:   1085    step:    12381     combined loss: 4322.28532     paf loss 24.42570     hm loss 4297.85962\n",
      "Iteration:   1090    step:    12386     combined loss: 4198.12475     paf loss 21.79125     hm loss 4176.33350\n",
      "Iteration:   1095    step:    12391     combined loss: 4113.52424     paf loss 20.42476     hm loss 4093.09949\n",
      "Iteration:   1100    step:    12396     combined loss: 4541.77403     paf loss 17.48766     hm loss 4524.28638\n",
      "Iteration:   1105    step:    12401     combined loss: 2968.88819     paf loss 17.56812     hm loss 2951.32007\n",
      "Iteration:   1110    step:    12406     combined loss: 3409.88667     paf loss 17.44807     hm loss 3392.43860\n",
      "Iteration:   1115    step:    12411     combined loss: 4004.66702     paf loss 29.47683     hm loss 3975.19019\n",
      "Iteration:   1120    step:    12416     combined loss: 4561.12140     paf loss 25.45490     hm loss 4535.66650\n",
      "Iteration:   1125    step:    12421     combined loss: 4203.26392     paf loss 20.79248     hm loss 4182.47144\n",
      "Iteration:   1130    step:    12426     combined loss: 2973.73825     paf loss 23.03756     hm loss 2950.70068\n",
      "Iteration:   1135    step:    12431     combined loss: 2445.21841     paf loss 17.91555     hm loss 2427.30286\n",
      "Iteration:   1140    step:    12436     combined loss: 3073.37800     paf loss 24.14118     hm loss 3049.23682\n",
      "Iteration:   1145    step:    12441     combined loss: 2818.76794     paf loss 16.14623     hm loss 2802.62170\n",
      "Iteration:   1150    step:    12446     combined loss: 3369.10488     paf loss 19.04299     hm loss 3350.06189\n",
      "Iteration:   1155    step:    12451     combined loss: 2750.76282     paf loss 18.28187     hm loss 2732.48096\n",
      "Iteration:   1160    step:    12456     combined loss: 3852.21690     paf loss 15.97007     hm loss 3836.24683\n",
      "Iteration:   1165    step:    12461     combined loss: 3142.19134     paf loss 24.94122     hm loss 3117.25012\n",
      "Iteration:   1170    step:    12466     combined loss: 3144.58810     paf loss 19.15939     hm loss 3125.42871\n",
      "Iteration:   1175    step:    12471     combined loss: 3489.66396     paf loss 19.48024     hm loss 3470.18372\n",
      "Iteration:   1180    step:    12476     combined loss: 3027.65883     paf loss 16.02040     hm loss 3011.63843\n",
      "Iteration:   1185    step:    12481     combined loss: 1779.79073     paf loss 11.03231     hm loss 1768.75842\n",
      "Iteration:   1190    step:    12486     combined loss: 5286.52655     paf loss 17.70624     hm loss 5268.82031\n",
      "Iteration:   1195    step:    12491     combined loss: 3233.45128     paf loss 18.47985     hm loss 3214.97144\n",
      "Iteration:   1200    step:    12496     combined loss: 6247.69995     paf loss 23.67652     hm loss 6224.02344\n",
      "Iteration:   1205    step:    12501     combined loss: 2681.49931     paf loss 16.20890     hm loss 2665.29041\n",
      "Iteration:   1210    step:    12506     combined loss: 5327.61199     paf loss 25.76287     hm loss 5301.84912\n",
      "Iteration:   1215    step:    12511     combined loss: 3421.81986     paf loss 20.32999     hm loss 3401.48987\n",
      "Iteration:   1220    step:    12516     combined loss: 3331.18502     paf loss 21.05099     hm loss 3310.13403\n",
      "Iteration:   1225    step:    12521     combined loss: 3439.80532     paf loss 18.68361     hm loss 3421.12170\n",
      "Iteration:   1230    step:    12526     combined loss: 3008.43347     paf loss 21.38110     hm loss 2987.05237\n",
      "Iteration:   1235    step:    12531     combined loss: 3917.32257     paf loss 21.51020     hm loss 3895.81238\n",
      "Iteration:   1240    step:    12536     combined loss: 3500.95763     paf loss 23.75622     hm loss 3477.20142\n",
      "Iteration:   1245    step:    12541     combined loss: 5898.44797     paf loss 21.83273     hm loss 5876.61523\n",
      "Iteration:   1250    step:    12546     combined loss: 4405.17297     paf loss 25.09386     hm loss 4380.07910\n",
      "Iteration:   1255    step:    12551     combined loss: 2529.17228     paf loss 16.42459     hm loss 2512.74768\n",
      "Iteration:   1260    step:    12556     combined loss: 2860.58701     paf loss 12.97995     hm loss 2847.60706\n",
      "Iteration:   1265    step:    12561     combined loss: 2975.38703     paf loss 12.72211     hm loss 2962.66492\n",
      "Iteration:   1270    step:    12566     combined loss: 2705.77833     paf loss 19.29347     hm loss 2686.48486\n",
      "Iteration:   1275    step:    12571     combined loss: 3711.03631     paf loss 19.00433     hm loss 3692.03198\n",
      "Iteration:   1280    step:    12576     combined loss: 2688.43402     paf loss 16.23138     hm loss 2672.20264\n",
      "Iteration:   1285    step:    12581     combined loss: 3482.44754     paf loss 17.39627     hm loss 3465.05127\n",
      "Iteration:   1290    step:    12586     combined loss: 3170.30029     paf loss 16.23352     hm loss 3154.06677\n",
      "Iteration:   1295    step:    12591     combined loss: 3799.06094     paf loss 20.93533     hm loss 3778.12561\n",
      "Iteration:   1300    step:    12596     combined loss: 4195.50424     paf loss 27.31479     hm loss 4168.18945\n",
      "Iteration:   1305    step:    12601     combined loss: 2124.01595     paf loss 15.88497     hm loss 2108.13098\n",
      "Iteration:   1310    step:    12606     combined loss: 2980.17213     paf loss 15.07912     hm loss 2965.09302\n",
      "Iteration:   1315    step:    12611     combined loss: 2534.14019     paf loss 15.92510     hm loss 2518.21509\n",
      "Iteration:   1320    step:    12616     combined loss: 4156.87902     paf loss 25.12487     hm loss 4131.75415\n",
      "Iteration:   1325    step:    12621     combined loss: 4142.87342     paf loss 22.14320     hm loss 4120.73022\n",
      "Iteration:   1330    step:    12626     combined loss: 2834.76403     paf loss 20.27184     hm loss 2814.49219\n",
      "Iteration:   1335    step:    12631     combined loss: 3933.90653     paf loss 21.36808     hm loss 3912.53845\n",
      "Iteration:   1340    step:    12636     combined loss: 3258.07844     paf loss 19.71577     hm loss 3238.36267\n",
      "Iteration:   1345    step:    12641     combined loss: 6178.99840     paf loss 29.89440     hm loss 6149.10400\n",
      "Iteration:   1350    step:    12646     combined loss: 3341.72061     paf loss 16.21768     hm loss 3325.50293\n",
      "Iteration:   1355    step:    12651     combined loss: 2674.32821     paf loss 15.16720     hm loss 2659.16101\n",
      "Iteration:   1360    step:    12656     combined loss: 4107.70414     paf loss 22.86674     hm loss 4084.83740\n",
      "Iteration:   1365    step:    12661     combined loss: 2725.69924     paf loss 17.08999     hm loss 2708.60925\n",
      "Iteration:   1370    step:    12666     combined loss: 3144.02900     paf loss 16.94172     hm loss 3127.08728\n",
      "Iteration:   1375    step:    12671     combined loss: 2800.05222     paf loss 18.59861     hm loss 2781.45361\n",
      "Iteration:   1380    step:    12676     combined loss: 3904.28281     paf loss 22.75082     hm loss 3881.53198\n",
      "Iteration:   1385    step:    12681     combined loss: 3732.35367     paf loss 21.58181     hm loss 3710.77185\n",
      "Iteration:   1390    step:    12686     combined loss: 3002.48204     paf loss 22.38316     hm loss 2980.09888\n",
      "Iteration:   1395    step:    12691     combined loss: 4649.93274     paf loss 21.66272     hm loss 4628.27002\n",
      "Iteration:   1400    step:    12696     combined loss: 4500.18655     paf loss 24.41604     hm loss 4475.77051\n",
      "Iteration:   1405    step:    12701     combined loss: 3154.72034     paf loss 18.93127     hm loss 3135.78906\n",
      "Iteration:   1410    step:    12706     combined loss: 2754.03612     paf loss 14.08727     hm loss 2739.94885\n",
      "Iteration:   1415    step:    12711     combined loss: 2486.36773     paf loss 14.94622     hm loss 2471.42151\n",
      "Iteration:   1420    step:    12716     combined loss: 4787.02446     paf loss 18.52958     hm loss 4768.49487\n",
      "Iteration:   1425    step:    12721     combined loss: 3838.24610     paf loss 24.20313     hm loss 3814.04297\n",
      "Iteration:   1430    step:    12726     combined loss: 3230.98342     paf loss 17.19558     hm loss 3213.78784\n",
      "Iteration:   1435    step:    12731     combined loss: 3513.87622     paf loss 22.36402     hm loss 3491.51221\n",
      "Iteration:   1440    step:    12736     combined loss: 3856.51544     paf loss 24.21002     hm loss 3832.30542\n",
      "Iteration:   1445    step:    12741     combined loss: 2259.35128     paf loss 18.42647     hm loss 2240.92480\n",
      "Iteration:   1450    step:    12746     combined loss: 4130.02611     paf loss 23.72948     hm loss 4106.29663\n",
      "Iteration:   1455    step:    12751     combined loss: 2947.09347     paf loss 17.71615     hm loss 2929.37732\n",
      "Iteration:   1460    step:    12756     combined loss: 4285.63809     paf loss 22.90103     hm loss 4262.73706\n",
      "Iteration:   1465    step:    12761     combined loss: 3085.00794     paf loss 19.18299     hm loss 3065.82495\n",
      "Iteration:   1470    step:    12766     combined loss: 2367.30616     paf loss 17.21375     hm loss 2350.09241\n",
      "Iteration:   1475    step:    12771     combined loss: 4476.49866     paf loss 22.64905     hm loss 4453.84961\n",
      "Iteration:   1480    step:    12776     combined loss: 4550.75962     paf loss 20.10801     hm loss 4530.65161\n",
      "Iteration:   1485    step:    12781     combined loss: 3852.70269     paf loss 21.62005     hm loss 3831.08264\n",
      "Iteration:   1490    step:    12786     combined loss: 2679.31879     paf loss 17.14825     hm loss 2662.17053\n",
      "Iteration:   1495    step:    12791     combined loss: 4245.77513     paf loss 21.91136     hm loss 4223.86377\n",
      "Iteration:   1500    step:    12796     combined loss: 3157.43499     paf loss 20.67364     hm loss 3136.76135\n",
      "Iteration:   1505    step:    12801     combined loss: 3964.88600     paf loss 17.13844     hm loss 3947.74756\n",
      "Iteration:   1510    step:    12806     combined loss: 3389.10073     paf loss 18.96596     hm loss 3370.13477\n",
      "Iteration:   1515    step:    12811     combined loss: 4307.41147     paf loss 18.74521     hm loss 4288.66626\n",
      "Iteration:   1520    step:    12816     combined loss: 2056.05024     paf loss 21.07905     hm loss 2034.97119\n",
      "Iteration:   1525    step:    12821     combined loss: 3645.24230     paf loss 15.99987     hm loss 3629.24243\n",
      "Iteration:   1530    step:    12826     combined loss: 3215.25020     paf loss 23.90438     hm loss 3191.34583\n",
      "Iteration:   1535    step:    12831     combined loss: 2569.55280     paf loss 15.03693     hm loss 2554.51587\n",
      "Iteration:   1540    step:    12836     combined loss: 2663.87313     paf loss 20.79708     hm loss 2643.07605\n",
      "Iteration:   1545    step:    12841     combined loss: 3661.14858     paf loss 22.47793     hm loss 3638.67065\n",
      "Iteration:   1550    step:    12846     combined loss: 2954.58387     paf loss 20.03456     hm loss 2934.54932\n",
      "Iteration:   1555    step:    12851     combined loss: 4180.30334     paf loss 23.66613     hm loss 4156.63721\n",
      "Iteration:   1560    step:    12856     combined loss: 3185.71021     paf loss 16.56470     hm loss 3169.14551\n",
      "Iteration:   1565    step:    12861     combined loss: 5245.90365     paf loss 20.57601     hm loss 5225.32764\n",
      "Iteration:   1570    step:    12866     combined loss: 3632.80192     paf loss 20.17460     hm loss 3612.62732\n",
      "Iteration:   1575    step:    12871     combined loss: 2665.72330     paf loss 14.51541     hm loss 2651.20789\n",
      "Iteration:   1580    step:    12876     combined loss: 5535.33834     paf loss 24.86666     hm loss 5510.47168\n",
      "Iteration:   1585    step:    12881     combined loss: 2214.68870     paf loss 14.96080     hm loss 2199.72791\n",
      "Iteration:   1590    step:    12886     combined loss: 3455.17683     paf loss 20.80452     hm loss 3434.37231\n",
      "Iteration:   1595    step:    12891     combined loss: 3446.45284     paf loss 19.98189     hm loss 3426.47095\n",
      "Iteration:   1600    step:    12896     combined loss: 5015.24501     paf loss 22.32314     hm loss 4992.92188\n",
      "Iteration:   1605    step:    12901     combined loss: 2836.44992     paf loss 15.56637     hm loss 2820.88354\n",
      "Iteration:   1610    step:    12906     combined loss: 3037.20768     paf loss 19.42717     hm loss 3017.78052\n",
      "Iteration:   1615    step:    12911     combined loss: 3130.62789     paf loss 17.13229     hm loss 3113.49561\n",
      "Iteration:   1620    step:    12916     combined loss: 3835.39196     paf loss 23.90478     hm loss 3811.48718\n",
      "Iteration:   1625    step:    12921     combined loss: 3842.25541     paf loss 16.64151     hm loss 3825.61389\n",
      "Iteration:   1630    step:    12926     combined loss: 3444.68858     paf loss 15.70530     hm loss 3428.98328\n",
      "Iteration:   1635    step:    12931     combined loss: 3353.29038     paf loss 18.11277     hm loss 3335.17761\n",
      "Iteration:   1640    step:    12936     combined loss: 3549.18897     paf loss 16.87219     hm loss 3532.31677\n",
      "Iteration:   1645    step:    12941     combined loss: 3714.08725     paf loss 19.02756     hm loss 3695.05969\n",
      "Iteration:   1650    step:    12946     combined loss: 2244.92103     paf loss 14.96425     hm loss 2229.95679\n",
      "Iteration:   1655    step:    12951     combined loss: 3597.88656     paf loss 25.20163     hm loss 3572.68494\n",
      "Iteration:   1660    step:    12956     combined loss: 4193.56383     paf loss 19.42149     hm loss 4174.14233\n",
      "Iteration:   1665    step:    12961     combined loss: 2925.30986     paf loss 14.83891     hm loss 2910.47095\n",
      "Iteration:   1670    step:    12966     combined loss: 3407.66593     paf loss 21.74821     hm loss 3385.91772\n",
      "Iteration:   1675    step:    12971     combined loss: 2956.72497     paf loss 16.58813     hm loss 2940.13684\n",
      "Iteration:   1680    step:    12976     combined loss: 4768.20488     paf loss 20.70561     hm loss 4747.49927\n",
      "Iteration:   1685    step:    12981     combined loss: 3244.50254     paf loss 20.66136     hm loss 3223.84119\n",
      "Iteration:   1690    step:    12986     combined loss: 3001.61258     paf loss 20.18656     hm loss 2981.42603\n",
      "Iteration:   1695    step:    12991     combined loss: 3785.85751     paf loss 14.87216     hm loss 3770.98535\n",
      "Iteration:   1700    step:    12996     combined loss: 2856.28627     paf loss 21.34266     hm loss 2834.94360\n",
      "Iteration:   1705    step:    13001     combined loss: 3757.30357     paf loss 17.85704     hm loss 3739.44653\n",
      "Iteration:   1710    step:    13006     combined loss: 7398.80573     paf loss 26.82916     hm loss 7371.97656\n",
      "Iteration:   1715    step:    13011     combined loss: 3927.37945     paf loss 21.95819     hm loss 3905.42126\n",
      "Iteration:   1720    step:    13016     combined loss: 4771.67345     paf loss 27.01794     hm loss 4744.65552\n",
      "Iteration:   1725    step:    13021     combined loss: 4090.47980     paf loss 16.38813     hm loss 4074.09167\n",
      "Iteration:   1730    step:    13026     combined loss: 5162.27103     paf loss 19.99466     hm loss 5142.27637\n",
      "Iteration:   1735    step:    13031     combined loss: 3991.68161     paf loss 17.00326     hm loss 3974.67834\n",
      "Iteration:   1740    step:    13036     combined loss: 6481.63465     paf loss 23.54383     hm loss 6458.09082\n",
      "Iteration:   1745    step:    13041     combined loss: 3195.71023     paf loss 19.73257     hm loss 3175.97766\n",
      "Iteration:   1750    step:    13046     combined loss: 3321.03055     paf loss 18.24368     hm loss 3302.78687\n",
      "Iteration:   1755    step:    13051     combined loss: 3053.98046     paf loss 21.66796     hm loss 3032.31250\n",
      "Iteration:   1760    step:    13056     combined loss: 2309.55024     paf loss 13.24482     hm loss 2296.30542\n",
      "Iteration:   1765    step:    13061     combined loss: 3296.28955     paf loss 21.35754     hm loss 3274.93201\n",
      "Iteration:   1770    step:    13066     combined loss: 3223.36425     paf loss 19.79614     hm loss 3203.56812\n",
      "Iteration:   1775    step:    13071     combined loss: 2289.58001     paf loss 17.07000     hm loss 2272.51001\n",
      "Iteration:   1780    step:    13076     combined loss: 3679.14748     paf loss 23.83107     hm loss 3655.31641\n",
      "Iteration:   1785    step:    13081     combined loss: 2851.03023     paf loss 15.99752     hm loss 2835.03271\n",
      "Iteration:   1790    step:    13086     combined loss: 4701.52984     paf loss 20.91656     hm loss 4680.61328\n",
      "Iteration:   1795    step:    13091     combined loss: 4448.43282     paf loss 22.60884     hm loss 4425.82397\n",
      "Iteration:   1800    step:    13096     combined loss: 3384.18352     paf loss 18.55828     hm loss 3365.62524\n",
      "Iteration:   1805    step:    13101     combined loss: 4379.41781     paf loss 27.80672     hm loss 4351.61108\n",
      "Iteration:   1810    step:    13106     combined loss: 3372.67693     paf loss 18.60661     hm loss 3354.07031\n",
      "Iteration:   1815    step:    13111     combined loss: 3927.83656     paf loss 23.02491     hm loss 3904.81165\n",
      "Iteration:   1820    step:    13116     combined loss: 2792.36557     paf loss 15.64767     hm loss 2776.71790\n",
      "Iteration:   1825    step:    13121     combined loss: 2623.47916     paf loss 12.88090     hm loss 2610.59827\n",
      "Iteration:   1830    step:    13126     combined loss: 2740.84999     paf loss 15.86391     hm loss 2724.98608\n",
      "Iteration:   1835    step:    13131     combined loss: 4233.32080     paf loss 21.69629     hm loss 4211.62451\n",
      "Iteration:   1840    step:    13136     combined loss: 2887.90688     paf loss 19.47072     hm loss 2868.43616\n",
      "Iteration:   1845    step:    13141     combined loss: 3370.32209     paf loss 18.78217     hm loss 3351.53992\n",
      "Iteration:   1850    step:    13146     combined loss: 3166.38590     paf loss 19.14737     hm loss 3147.23853\n",
      "Iteration:   1855    step:    13151     combined loss: 4936.27611     paf loss 24.00463     hm loss 4912.27148\n",
      "Iteration:   1860    step:    13156     combined loss: 3412.32824     paf loss 21.96740     hm loss 3390.36084\n",
      "Iteration:   1865    step:    13161     combined loss: 3273.10358     paf loss 18.54047     hm loss 3254.56311\n",
      "Iteration:   1870    step:    13166     combined loss: 3020.25699     paf loss 18.74954     hm loss 3001.50745\n",
      "Iteration:   1875    step:    13171     combined loss: 2609.51853     paf loss 21.59617     hm loss 2587.92236\n",
      "Iteration:   1880    step:    13176     combined loss: 2626.31224     paf loss 14.82335     hm loss 2611.48889\n",
      "Iteration:   1885    step:    13181     combined loss: 2498.43778     paf loss 16.98038     hm loss 2481.45740\n",
      "Iteration:   1890    step:    13186     combined loss: 2178.53040     paf loss 19.01367     hm loss 2159.51672\n",
      "Iteration:   1895    step:    13191     combined loss: 3815.73804     paf loss 18.52710     hm loss 3797.21094\n",
      "Iteration:   1900    step:    13196     combined loss: 4509.83742     paf loss 18.20778     hm loss 4491.62964\n",
      "Iteration:   1905    step:    13201     combined loss: 3391.02456     paf loss 21.11904     hm loss 3369.90552\n",
      "Iteration:   1910    step:    13206     combined loss: 3497.36925     paf loss 21.32555     hm loss 3476.04370\n",
      "Iteration:   1915    step:    13211     combined loss: 4668.36489     paf loss 27.76088     hm loss 4640.60400\n",
      "Iteration:   1920    step:    13216     combined loss: 4001.39706     paf loss 24.88851     hm loss 3976.50854\n",
      "Iteration:   1925    step:    13221     combined loss: 3400.99531     paf loss 23.64094     hm loss 3377.35437\n",
      "Iteration:   1930    step:    13226     combined loss: 3785.51066     paf loss 15.17863     hm loss 3770.33203\n",
      "Iteration:   1935    step:    13231     combined loss: 3621.31684     paf loss 17.72223     hm loss 3603.59460\n",
      "Iteration:   1940    step:    13236     combined loss: 3557.26174     paf loss 20.88686     hm loss 3536.37488\n",
      "Iteration:   1945    step:    13241     combined loss: 5301.12628     paf loss 21.38971     hm loss 5279.73657\n",
      "Iteration:   1950    step:    13246     combined loss: 4573.23889     paf loss 16.39660     hm loss 4556.84229\n",
      "Iteration:   1955    step:    13251     combined loss: 3614.49916     paf loss 22.69472     hm loss 3591.80444\n",
      "Iteration:   1960    step:    13256     combined loss: 3231.84773     paf loss 15.45918     hm loss 3216.38855\n",
      "Iteration:   1965    step:    13261     combined loss: 4098.67348     paf loss 20.94679     hm loss 4077.72668\n",
      "Iteration:   1970    step:    13266     combined loss: 3590.84639     paf loss 19.57259     hm loss 3571.27380\n",
      "Iteration:   1975    step:    13271     combined loss: 3326.77195     paf loss 18.55332     hm loss 3308.21863\n",
      "Iteration:   1980    step:    13276     combined loss: 4353.31421     paf loss 24.97900     hm loss 4328.33521\n",
      "Iteration:   1985    step:    13281     combined loss: 6595.31163     paf loss 27.39073     hm loss 6567.92090\n",
      "Iteration:   1990    step:    13286     combined loss: 2597.39245     paf loss 19.36902     hm loss 2578.02344\n",
      "Iteration:   1995    step:    13291     combined loss: 2688.30931     paf loss 13.97765     hm loss 2674.33167\n",
      "Iteration:   2000    step:    13296     combined loss: 2443.34097     paf loss 14.41751     hm loss 2428.92346\n",
      "Iteration:   2005    step:    13301     combined loss: 4097.85778     paf loss 20.86511     hm loss 4076.99268\n",
      "Iteration:   2010    step:    13306     combined loss: 3758.30390     paf loss 14.44623     hm loss 3743.85767\n",
      "Iteration:   2015    step:    13311     combined loss: 3909.19800     paf loss 20.63453     hm loss 3888.56348\n",
      "Iteration:   2020    step:    13316     combined loss: 3392.48579     paf loss 19.10139     hm loss 3373.38440\n",
      "Iteration:   2025    step:    13321     combined loss: 4091.68081     paf loss 21.09134     hm loss 4070.58948\n",
      "Iteration:   2030    step:    13326     combined loss: 3738.41017     paf loss 25.79396     hm loss 3712.61621\n",
      "Iteration:   2035    step:    13331     combined loss: 4076.27866     paf loss 20.51791     hm loss 4055.76074\n",
      "Iteration:   2040    step:    13336     combined loss: 2872.91327     paf loss 17.61688     hm loss 2855.29639\n",
      "Iteration:   2045    step:    13341     combined loss: 3996.46724     paf loss 20.35396     hm loss 3976.11328\n",
      "Iteration:   2050    step:    13346     combined loss: 4739.35537     paf loss 20.35610     hm loss 4718.99927\n",
      "Iteration:   2055    step:    13351     combined loss: 3699.95279     paf loss 19.33048     hm loss 3680.62231\n",
      "Iteration:   2060    step:    13356     combined loss: 3762.43694     paf loss 14.92802     hm loss 3747.50891\n",
      "Iteration:   2065    step:    13361     combined loss: 2707.24906     paf loss 15.04093     hm loss 2692.20813\n",
      "Iteration:   2070    step:    13366     combined loss: 5918.75798     paf loss 18.65227     hm loss 5900.10571\n",
      "Iteration:   2075    step:    13371     combined loss: 2296.03512     paf loss 13.51828     hm loss 2282.51685\n",
      "Iteration:   2080    step:    13376     combined loss: 3478.70180     paf loss 18.47865     hm loss 3460.22314\n",
      "Iteration:   2085    step:    13381     combined loss: 4101.38121     paf loss 19.27672     hm loss 4082.10449\n",
      "Iteration:   2090    step:    13386     combined loss: 3219.11545     paf loss 16.38743     hm loss 3202.72803\n",
      "Iteration:   2095    step:    13391     combined loss: 3189.78179     paf loss 15.34820     hm loss 3174.43359\n",
      "Iteration:   2100    step:    13396     combined loss: 3854.76102     paf loss 19.28275     hm loss 3835.47827\n",
      "Iteration:   2105    step:    13401     combined loss: 3186.11125     paf loss 24.14836     hm loss 3161.96289\n",
      "Iteration:   2110    step:    13406     combined loss: 3423.46908     paf loss 23.03121     hm loss 3400.43787\n",
      "Iteration:   2115    step:    13411     combined loss: 2639.11277     paf loss 18.34251     hm loss 2620.77026\n",
      "Iteration:   2120    step:    13416     combined loss: 4546.28525     paf loss 21.17417     hm loss 4525.11108\n",
      "Iteration:   2125    step:    13421     combined loss: 3807.60965     paf loss 19.81717     hm loss 3787.79248\n",
      "Iteration:   2130    step:    13426     combined loss: 3922.61826     paf loss 20.02255     hm loss 3902.59570\n",
      "Iteration:   2135    step:    13431     combined loss: 2667.66371     paf loss 15.94752     hm loss 2651.71619\n",
      "Iteration:   2140    step:    13436     combined loss: 4037.64628     paf loss 23.15116     hm loss 4014.49512\n",
      "Iteration:   2145    step:    13441     combined loss: 3558.67633     paf loss 14.46857     hm loss 3544.20776\n",
      "Iteration:   2150    step:    13446     combined loss: 2578.69799     paf loss 18.06188     hm loss 2560.63611\n",
      "Iteration:   2155    step:    13451     combined loss: 3557.42243     paf loss 20.57477     hm loss 3536.84766\n",
      "Iteration:   2160    step:    13456     combined loss: 3278.63789     paf loss 18.77693     hm loss 3259.86096\n",
      "Iteration:   2165    step:    13461     combined loss: 2943.64961     paf loss 16.98066     hm loss 2926.66895\n",
      "Iteration:   2170    step:    13466     combined loss: 3498.67517     paf loss 18.03723     hm loss 3480.63794\n",
      "Iteration:   2175    step:    13471     combined loss: 3151.76353     paf loss 18.96982     hm loss 3132.79370\n",
      "Iteration:   2180    step:    13476     combined loss: 3238.99702     paf loss 17.68696     hm loss 3221.31006\n",
      "Iteration:   2185    step:    13481     combined loss: 3105.32571     paf loss 22.67447     hm loss 3082.65125\n",
      "Iteration:   2190    step:    13486     combined loss: 3539.75026     paf loss 17.23085     hm loss 3522.51941\n",
      "Iteration:   2195    step:    13491     combined loss: 3491.89818     paf loss 22.38792     hm loss 3469.51025\n",
      "Iteration:   2200    step:    13496     combined loss: 4287.36456     paf loss 23.22638     hm loss 4264.13818\n",
      "Iteration:   2205    step:    13501     combined loss: 3620.10550     paf loss 14.94119     hm loss 3605.16431\n",
      "Iteration:   2210    step:    13506     combined loss: 3635.86190     paf loss 26.75509     hm loss 3609.10681\n",
      "Iteration:   2215    step:    13511     combined loss: 3528.44394     paf loss 20.87179     hm loss 3507.57214\n",
      "Iteration:   2220    step:    13516     combined loss: 3542.71960     paf loss 21.94421     hm loss 3520.77539\n",
      "Iteration:   2225    step:    13521     combined loss: 5151.51702     paf loss 25.88250     hm loss 5125.63452\n",
      "Iteration:   2230    step:    13526     combined loss: 3541.85279     paf loss 16.63953     hm loss 3525.21326\n",
      "Iteration:   2235    step:    13531     combined loss: 3912.25348     paf loss 20.51301     hm loss 3891.74048\n",
      "Iteration:   2240    step:    13536     combined loss: 2327.90357     paf loss 18.79163     hm loss 2309.11194\n",
      "Iteration:   2245    step:    13541     combined loss: 2635.38097     paf loss 18.20152     hm loss 2617.17944\n",
      "Iteration:   2250    step:    13546     combined loss: 4750.39400     paf loss 25.05879     hm loss 4725.33521\n",
      "Iteration:   2255    step:    13551     combined loss: 2989.42650     paf loss 19.19798     hm loss 2970.22852\n",
      "Iteration:   2260    step:    13556     combined loss: 3366.77517     paf loss 18.73855     hm loss 3348.03662\n",
      "Iteration:   2265    step:    13561     combined loss: 4171.40815     paf loss 23.24408     hm loss 4148.16406\n",
      "Iteration:   2270    step:    13566     combined loss: 4310.02823     paf loss 20.50992     hm loss 4289.51831\n",
      "Iteration:   2275    step:    13571     combined loss: 4311.86953     paf loss 21.48843     hm loss 4290.38110\n",
      "Iteration:   2280    step:    13576     combined loss: 2528.49589     paf loss 13.98076     hm loss 2514.51514\n",
      "Iteration:   2285    step:    13581     combined loss: 3249.01188     paf loss 19.46378     hm loss 3229.54810\n",
      "Iteration:   2290    step:    13586     combined loss: 3953.86971     paf loss 21.69930     hm loss 3932.17041\n",
      "Iteration:   2295    step:    13591     combined loss: 2765.34548     paf loss 19.48977     hm loss 2745.85571\n",
      "Iteration:   2300    step:    13596     combined loss: 3311.62377     paf loss 19.56420     hm loss 3292.05957\n",
      "Iteration:   2305    step:    13601     combined loss: 5505.96554     paf loss 21.83200     hm loss 5484.13354\n",
      "Iteration:   2310    step:    13606     combined loss: 3114.23748     paf loss 20.75908     hm loss 3093.47839\n",
      "Iteration:   2315    step:    13611     combined loss: 3638.10341     paf loss 22.62587     hm loss 3615.47754\n",
      "Iteration:   2320    step:    13616     combined loss: 3957.20309     paf loss 21.84713     hm loss 3935.35596\n",
      "Iteration:   2325    step:    13621     combined loss: 3910.26955     paf loss 19.23537     hm loss 3891.03418\n",
      "Iteration:   2330    step:    13626     combined loss: 3303.00588     paf loss 21.31728     hm loss 3281.68860\n",
      "Iteration:   2335    step:    13631     combined loss: 2698.76713     paf loss 22.44242     hm loss 2676.32471\n",
      "Iteration:   2340    step:    13636     combined loss: 2617.72677     paf loss 19.46822     hm loss 2598.25854\n",
      "Iteration:   2345    step:    13641     combined loss: 2744.58344     paf loss 19.78144     hm loss 2724.80200\n",
      "Iteration:   2350    step:    13646     combined loss: 4425.03825     paf loss 19.15129     hm loss 4405.88696\n",
      "Iteration:   2355    step:    13651     combined loss: 3045.47993     paf loss 20.29671     hm loss 3025.18323\n",
      "Iteration:   2360    step:    13656     combined loss: 5297.06162     paf loss 18.82139     hm loss 5278.24023\n",
      "Iteration:   2365    step:    13661     combined loss: 4475.12775     paf loss 25.79523     hm loss 4449.33252\n",
      "Iteration:   2370    step:    13666     combined loss: 2750.07301     paf loss 16.05910     hm loss 2734.01392\n",
      "Iteration:   2375    step:    13671     combined loss: 5259.28501     paf loss 23.45615     hm loss 5235.82886\n",
      "Iteration:   2380    step:    13676     combined loss: 3064.98535     paf loss 19.21802     hm loss 3045.76733\n",
      "Iteration:   2385    step:    13681     combined loss: 3267.15008     paf loss 19.24871     hm loss 3247.90137\n",
      "Iteration:   2390    step:    13686     combined loss: 3568.03179     paf loss 27.21599     hm loss 3540.81580\n",
      "Iteration:   2395    step:    13691     combined loss: 3731.94408     paf loss 19.42979     hm loss 3712.51428\n",
      "Iteration:   2400    step:    13696     combined loss: 2879.92741     paf loss 16.81157     hm loss 2863.11584\n",
      "Iteration:   2405    step:    13701     combined loss: 3258.10180     paf loss 20.23791     hm loss 3237.86389\n",
      "Iteration:   2410    step:    13706     combined loss: 3542.38544     paf loss 17.76997     hm loss 3524.61548\n",
      "Iteration:   2415    step:    13711     combined loss: 2713.91652     paf loss 23.69228     hm loss 2690.22424\n",
      "Iteration:   2420    step:    13716     combined loss: 2963.24930     paf loss 20.11515     hm loss 2943.13416\n",
      "Iteration:   2425    step:    13721     combined loss: 4384.24827     paf loss 25.71507     hm loss 4358.53320\n",
      "Iteration:   2430    step:    13726     combined loss: 3925.94684     paf loss 19.75141     hm loss 3906.19543\n",
      "Iteration:   2435    step:    13731     combined loss: 4112.51766     paf loss 19.00545     hm loss 4093.51221\n",
      "Iteration:   2440    step:    13736     combined loss: 3798.24866     paf loss 20.76855     hm loss 3777.48010\n",
      "Iteration:   2445    step:    13741     combined loss: 3157.47982     paf loss 17.42282     hm loss 3140.05701\n",
      "Iteration:   2450    step:    13746     combined loss: 4714.53632     paf loss 23.36029     hm loss 4691.17603\n",
      "Iteration:   2455    step:    13751     combined loss: 3719.40304     paf loss 24.88192     hm loss 3694.52112\n",
      "Iteration:   2460    step:    13756     combined loss: 3529.68260     paf loss 18.26475     hm loss 3511.41785\n",
      "Iteration:   2465    step:    13761     combined loss: 3197.49697     paf loss 18.69631     hm loss 3178.80066\n",
      "Iteration:   2470    step:    13766     combined loss: 5147.13102     paf loss 23.51872     hm loss 5123.61230\n",
      "Iteration:   2475    step:    13771     combined loss: 4267.08726     paf loss 25.87608     hm loss 4241.21118\n",
      "Iteration:   2480    step:    13776     combined loss: 3177.54691     paf loss 22.10367     hm loss 3155.44324\n",
      "Iteration:   2485    step:    13781     combined loss: 4371.03464     paf loss 21.78049     hm loss 4349.25415\n",
      "Iteration:   2490    step:    13786     combined loss: 5172.76004     paf loss 22.10501     hm loss 5150.65503\n",
      "Iteration:   2495    step:    13791     combined loss: 3088.28895     paf loss 23.48622     hm loss 3064.80273\n",
      "Iteration:   2500    step:    13796     combined loss: 2949.93778     paf loss 18.83146     hm loss 2931.10632\n",
      "Iteration:   2505    step:    13801     combined loss: 5851.49975     paf loss 19.71264     hm loss 5831.78711\n",
      "Iteration:   2510    step:    13806     combined loss: 3857.99171     paf loss 19.44544     hm loss 3838.54626\n",
      "Iteration:   2515    step:    13811     combined loss: 5226.46551     paf loss 21.18060     hm loss 5205.28491\n",
      "Iteration:   2520    step:    13816     combined loss: 3495.93155     paf loss 24.17227     hm loss 3471.75928\n",
      "Iteration:   2525    step:    13821     combined loss: 2413.02216     paf loss 14.74493     hm loss 2398.27722\n",
      "Iteration:   2530    step:    13826     combined loss: 3138.41066     paf loss 12.75661     hm loss 3125.65405\n",
      "Iteration:   2535    step:    13831     combined loss: 4571.68837     paf loss 22.58143     hm loss 4549.10693\n",
      "Iteration:   2540    step:    13836     combined loss: 2615.10847     paf loss 16.12385     hm loss 2598.98462\n",
      "Iteration:   2545    step:    13841     combined loss: 2988.89290     paf loss 21.63716     hm loss 2967.25574\n",
      "Iteration:   2550    step:    13846     combined loss: 2257.11572     paf loss 14.51050     hm loss 2242.60522\n",
      "Iteration:   2555    step:    13851     combined loss: 3287.70514     paf loss 21.60737     hm loss 3266.09778\n",
      "Iteration:   2560    step:    13856     combined loss: 2710.13095     paf loss 19.73056     hm loss 2690.40039\n",
      "Iteration:   2565    step:    13861     combined loss: 5151.34817     paf loss 24.95656     hm loss 5126.39160\n",
      "Iteration:   2570    step:    13866     combined loss: 2225.83622     paf loss 13.46598     hm loss 2212.37024\n",
      "Iteration:   2575    step:    13871     combined loss: 3648.01444     paf loss 16.33829     hm loss 3631.67615\n",
      "Iteration:   2580    step:    13876     combined loss: 3342.90679     paf loss 19.81340     hm loss 3323.09338\n",
      "Iteration:   2585    step:    13881     combined loss: 5500.14713     paf loss 33.78775     hm loss 5466.35938\n",
      "Iteration:   2590    step:    13886     combined loss: 4469.10553     paf loss 27.25250     hm loss 4441.85303\n",
      "Iteration:   2595    step:    13891     combined loss: 5190.59062     paf loss 23.21269     hm loss 5167.37793\n",
      "Iteration:   2600    step:    13896     combined loss: 2190.06888     paf loss 13.39152     hm loss 2176.67737\n",
      "Iteration:   2605    step:    13901     combined loss: 2577.93547     paf loss 14.23515     hm loss 2563.70032\n",
      "Iteration:   2610    step:    13906     combined loss: 3586.60110     paf loss 17.54971     hm loss 3569.05139\n",
      "Iteration:   2615    step:    13911     combined loss: 3310.93666     paf loss 21.23329     hm loss 3289.70337\n",
      "Iteration:   2620    step:    13916     combined loss: 3860.27531     paf loss 21.99223     hm loss 3838.28308\n",
      "Iteration:   2625    step:    13921     combined loss: 3918.32944     paf loss 22.46799     hm loss 3895.86145\n",
      "Iteration:   2630    step:    13926     combined loss: 2435.83527     paf loss 15.90741     hm loss 2419.92786\n",
      "Iteration:   2635    step:    13931     combined loss: 2773.62040     paf loss 16.11992     hm loss 2757.50049\n",
      "Iteration:   2640    step:    13936     combined loss: 2542.03220     paf loss 17.25034     hm loss 2524.78186\n",
      "Iteration:   2645    step:    13941     combined loss: 6171.90907     paf loss 22.99159     hm loss 6148.91748\n",
      "Iteration:   2650    step:    13946     combined loss: 2782.32319     paf loss 21.82710     hm loss 2760.49609\n",
      "Iteration:   2655    step:    13951     combined loss: 3051.52492     paf loss 19.64907     hm loss 3031.87585\n",
      "Iteration:   2660    step:    13956     combined loss: 4234.51225     paf loss 19.61723     hm loss 4214.89502\n",
      "Iteration:   2665    step:    13961     combined loss: 3250.58264     paf loss 17.59680     hm loss 3232.98584\n",
      "Iteration:   2670    step:    13966     combined loss: 2903.83973     paf loss 16.08180     hm loss 2887.75793\n",
      "Iteration:   2675    step:    13971     combined loss: 2305.72677     paf loss 17.44125     hm loss 2288.28552\n",
      "Iteration:   2680    step:    13976     combined loss: 2619.77553     paf loss 15.66506     hm loss 2604.11047\n",
      "Iteration:   2685    step:    13981     combined loss: 4048.16447     paf loss 23.92484     hm loss 4024.23962\n",
      "Iteration:   2690    step:    13986     combined loss: 3158.08866     paf loss 15.87565     hm loss 3142.21301\n",
      "Iteration:   2695    step:    13991     combined loss: 3049.84263     paf loss 17.59178     hm loss 3032.25085\n",
      "Iteration:   2700    step:    13996     combined loss: 3644.10594     paf loss 23.74925     hm loss 3620.35669\n",
      "Iteration:   2705    step:    14001     combined loss: 3565.12597     paf loss 23.24364     hm loss 3541.88232\n",
      "Iteration:   2710    step:    14006     combined loss: 3047.92593     paf loss 21.80215     hm loss 3026.12378\n",
      "Iteration:   2715    step:    14011     combined loss: 3940.04878     paf loss 24.76033     hm loss 3915.28845\n",
      "Iteration:   2720    step:    14016     combined loss: 4017.72286     paf loss 26.51070     hm loss 3991.21216\n",
      "Iteration:   2725    step:    14021     combined loss: 3036.74300     paf loss 15.89424     hm loss 3020.84875\n",
      "Iteration:   2730    step:    14026     combined loss: 3365.90352     paf loss 16.99300     hm loss 3348.91052\n",
      "Iteration:   2735    step:    14031     combined loss: 2709.51000     paf loss 17.13671     hm loss 2692.37329\n",
      "Iteration:   2740    step:    14036     combined loss: 3862.75981     paf loss 16.46086     hm loss 3846.29895\n",
      "Iteration:   2745    step:    14041     combined loss: 2386.47308     paf loss 9.94744     hm loss 2376.52563\n",
      "Iteration:   2750    step:    14046     combined loss: 2725.03071     paf loss 15.29829     hm loss 2709.73242\n",
      "Iteration:   2755    step:    14051     combined loss: 2702.29883     paf loss 15.00562     hm loss 2687.29321\n",
      "Iteration:   2760    step:    14056     combined loss: 4492.42174     paf loss 31.05577     hm loss 4461.36597\n",
      "Iteration:   2765    step:    14061     combined loss: 2822.03314     paf loss 20.71392     hm loss 2801.31921\n",
      "Iteration:   2770    step:    14066     combined loss: 3820.96349     paf loss 15.90709     hm loss 3805.05640\n",
      "Iteration:   2775    step:    14071     combined loss: 3765.89622     paf loss 17.16135     hm loss 3748.73486\n",
      "Iteration:   2780    step:    14076     combined loss: 2923.31453     paf loss 16.37789     hm loss 2906.93665\n",
      "Iteration:   2785    step:    14081     combined loss: 3979.34336     paf loss 25.39268     hm loss 3953.95068\n",
      "Iteration:   2790    step:    14086     combined loss: 3239.87006     paf loss 16.54792     hm loss 3223.32214\n",
      "Iteration:   2795    step:    14091     combined loss: 5046.33095     paf loss 21.55458     hm loss 5024.77637\n",
      "Iteration:   2800    step:    14096     combined loss: 2796.69645     paf loss 15.61625     hm loss 2781.08020\n",
      "Iteration:   2805    step:    14101     combined loss: 3619.85117     paf loss 17.24607     hm loss 3602.60510\n",
      "Iteration:   2810    step:    14106     combined loss: 2816.24464     paf loss 20.24806     hm loss 2795.99658\n",
      "Iteration:   2815    step:    14111     combined loss: 3382.28766     paf loss 23.05609     hm loss 3359.23157\n",
      "Iteration:   2820    step:    14116     combined loss: 3128.01442     paf loss 17.91273     hm loss 3110.10168\n",
      "Train Loss: 3623.9376    PAF Loss:  19.8805    HM Loss:  3604.0571    Acc: NA\n",
      "Val Loss: 3912.8311    PAF Loss:  16.5855    HM Loss:  3896.2456     Acc: NA\n",
      "Epoch 5/9\n",
      "----------\n",
      "Iteration:      0    step:    14120     combined loss: 5379.14082     paf loss 26.50557     hm loss 5352.63525\n",
      "Iteration:      5    step:    14125     combined loss: 4776.69535     paf loss 23.63041     hm loss 4753.06494\n",
      "Iteration:     10    step:    14130     combined loss: 5163.05957     paf loss 20.96411     hm loss 5142.09546\n",
      "Iteration:     15    step:    14135     combined loss: 4907.71964     paf loss 20.63297     hm loss 4887.08667\n",
      "Iteration:     20    step:    14140     combined loss: 2619.81425     paf loss 15.87357     hm loss 2603.94067\n",
      "Iteration:     25    step:    14145     combined loss: 3469.50435     paf loss 17.21871     hm loss 3452.28564\n",
      "Iteration:     30    step:    14150     combined loss: 3363.22423     paf loss 20.15269     hm loss 3343.07153\n",
      "Iteration:     35    step:    14155     combined loss: 3781.66968     paf loss 20.24805     hm loss 3761.42163\n",
      "Iteration:     40    step:    14160     combined loss: 3329.37207     paf loss 19.18116     hm loss 3310.19092\n",
      "Iteration:     45    step:    14165     combined loss: 3308.69749     paf loss 17.59141     hm loss 3291.10608\n",
      "Iteration:     50    step:    14170     combined loss: 5573.78844     paf loss 20.12560     hm loss 5553.66284\n",
      "Iteration:     55    step:    14175     combined loss: 3240.16702     paf loss 20.04641     hm loss 3220.12061\n",
      "Iteration:     60    step:    14180     combined loss: 3870.99545     paf loss 22.05404     hm loss 3848.94141\n",
      "Iteration:     65    step:    14185     combined loss: 3260.07585     paf loss 23.64616     hm loss 3236.42969\n",
      "Iteration:     70    step:    14190     combined loss: 2543.03170     paf loss 20.29122     hm loss 2522.74048\n",
      "Iteration:     75    step:    14195     combined loss: 3640.18085     paf loss 18.73993     hm loss 3621.44092\n",
      "Iteration:     80    step:    14200     combined loss: 3093.96600     paf loss 23.22076     hm loss 3070.74524\n",
      "Iteration:     85    step:    14205     combined loss: 3407.55271     paf loss 22.76401     hm loss 3384.78870\n",
      "Iteration:     90    step:    14210     combined loss: 3450.27934     paf loss 18.97526     hm loss 3431.30408\n",
      "Iteration:     95    step:    14215     combined loss: 2989.66454     paf loss 16.31127     hm loss 2973.35327\n",
      "Iteration:    100    step:    14220     combined loss: 3886.00612     paf loss 18.65199     hm loss 3867.35413\n",
      "Iteration:    105    step:    14225     combined loss: 2681.57186     paf loss 15.66561     hm loss 2665.90625\n",
      "Iteration:    110    step:    14230     combined loss: 3020.31979     paf loss 19.93942     hm loss 3000.38037\n",
      "Iteration:    115    step:    14235     combined loss: 3010.00320     paf loss 16.09305     hm loss 2993.91016\n",
      "Iteration:    120    step:    14240     combined loss: 3254.78891     paf loss 19.61886     hm loss 3235.17004\n",
      "Iteration:    125    step:    14245     combined loss: 4709.68271     paf loss 22.30551     hm loss 4687.37720\n",
      "Iteration:    130    step:    14250     combined loss: 3940.16454     paf loss 18.87084     hm loss 3921.29370\n",
      "Iteration:    135    step:    14255     combined loss: 4315.52426     paf loss 22.23984     hm loss 4293.28442\n",
      "Iteration:    140    step:    14260     combined loss: 4089.70362     paf loss 19.44031     hm loss 4070.26331\n",
      "Iteration:    145    step:    14265     combined loss: 4013.75362     paf loss 27.32443     hm loss 3986.42920\n",
      "Iteration:    150    step:    14270     combined loss: 5712.05720     paf loss 21.89387     hm loss 5690.16333\n",
      "Iteration:    155    step:    14275     combined loss: 2804.42075     paf loss 18.21689     hm loss 2786.20386\n",
      "Iteration:    160    step:    14280     combined loss: 3716.51358     paf loss 21.37576     hm loss 3695.13782\n",
      "Iteration:    165    step:    14285     combined loss: 5450.06325     paf loss 25.01369     hm loss 5425.04956\n",
      "Iteration:    170    step:    14290     combined loss: 3039.02357     paf loss 18.86158     hm loss 3020.16199\n",
      "Iteration:    175    step:    14295     combined loss: 3136.34698     paf loss 16.13763     hm loss 3120.20935\n",
      "Iteration:    180    step:    14300     combined loss: 3995.65615     paf loss 23.28298     hm loss 3972.37317\n",
      "Iteration:    185    step:    14305     combined loss: 3105.27326     paf loss 19.31977     hm loss 3085.95349\n",
      "Iteration:    190    step:    14310     combined loss: 3362.17149     paf loss 24.09972     hm loss 3338.07178\n",
      "Iteration:    195    step:    14315     combined loss: 3375.67597     paf loss 17.39765     hm loss 3358.27832\n",
      "Iteration:    200    step:    14320     combined loss: 3075.80489     paf loss 18.37093     hm loss 3057.43396\n",
      "Iteration:    205    step:    14325     combined loss: 3702.53330     paf loss 19.11032     hm loss 3683.42297\n",
      "Iteration:    210    step:    14330     combined loss: 5142.22977     paf loss 23.91678     hm loss 5118.31299\n",
      "Iteration:    215    step:    14335     combined loss: 3180.89694     paf loss 17.30160     hm loss 3163.59534\n",
      "Iteration:    220    step:    14340     combined loss: 2931.64214     paf loss 21.70464     hm loss 2909.93750\n",
      "Iteration:    225    step:    14345     combined loss: 3138.30440     paf loss 19.64156     hm loss 3118.66284\n",
      "Iteration:    230    step:    14350     combined loss: 3229.26469     paf loss 18.27800     hm loss 3210.98669\n",
      "Iteration:    235    step:    14355     combined loss: 4674.83568     paf loss 18.56761     hm loss 4656.26807\n",
      "Iteration:    240    step:    14360     combined loss: 3492.70060     paf loss 12.40604     hm loss 3480.29456\n",
      "Iteration:    245    step:    14365     combined loss: 2969.91296     paf loss 18.09729     hm loss 2951.81567\n",
      "Iteration:    250    step:    14370     combined loss: 2775.78353     paf loss 19.44613     hm loss 2756.33740\n",
      "Iteration:    255    step:    14375     combined loss: 2377.93849     paf loss 15.43177     hm loss 2362.50671\n",
      "Iteration:    260    step:    14380     combined loss: 2505.07212     paf loss 21.28233     hm loss 2483.78979\n",
      "Iteration:    265    step:    14385     combined loss: 3179.33134     paf loss 24.76078     hm loss 3154.57056\n",
      "Iteration:    270    step:    14390     combined loss: 3452.99665     paf loss 21.46699     hm loss 3431.52966\n",
      "Iteration:    275    step:    14395     combined loss: 3569.65827     paf loss 20.56647     hm loss 3549.09180\n",
      "Iteration:    280    step:    14400     combined loss: 2559.04042     paf loss 16.56545     hm loss 2542.47498\n",
      "Iteration:    285    step:    14405     combined loss: 3738.04224     paf loss 26.57752     hm loss 3711.46472\n",
      "Iteration:    290    step:    14410     combined loss: 4122.80442     paf loss 21.27146     hm loss 4101.53296\n",
      "Iteration:    295    step:    14415     combined loss: 5224.27793     paf loss 21.76109     hm loss 5202.51685\n",
      "Iteration:    300    step:    14420     combined loss: 3555.10935     paf loss 21.75840     hm loss 3533.35095\n",
      "Iteration:    305    step:    14425     combined loss: 2868.11931     paf loss 16.44780     hm loss 2851.67151\n",
      "Iteration:    310    step:    14430     combined loss: 2281.07463     paf loss 19.42363     hm loss 2261.65100\n",
      "Iteration:    315    step:    14435     combined loss: 3159.36231     paf loss 18.19495     hm loss 3141.16736\n",
      "Iteration:    320    step:    14440     combined loss: 3549.98697     paf loss 20.69205     hm loss 3529.29492\n",
      "Iteration:    325    step:    14445     combined loss: 3564.97457     paf loss 16.86739     hm loss 3548.10718\n",
      "Iteration:    330    step:    14450     combined loss: 3010.07443     paf loss 12.52121     hm loss 2997.55322\n",
      "Iteration:    335    step:    14455     combined loss: 4011.81205     paf loss 18.53654     hm loss 3993.27551\n",
      "Iteration:    340    step:    14460     combined loss: 2489.14039     paf loss 16.84889     hm loss 2472.29150\n",
      "Iteration:    345    step:    14465     combined loss: 5781.81276     paf loss 28.76393     hm loss 5753.04883\n",
      "Iteration:    350    step:    14470     combined loss: 3952.08524     paf loss 20.37113     hm loss 3931.71411\n",
      "Iteration:    355    step:    14475     combined loss: 2938.28694     paf loss 17.63337     hm loss 2920.65356\n",
      "Iteration:    360    step:    14480     combined loss: 2701.33425     paf loss 14.98709     hm loss 2686.34717\n",
      "learning rate change: 3.125e-05 --> 1.5625e-05\n",
      "Iteration:    365    step:    14485     combined loss: 3218.90121     paf loss 22.85897     hm loss 3196.04224\n",
      "Iteration:    370    step:    14490     combined loss: 2838.43090     paf loss 16.39977     hm loss 2822.03113\n",
      "Iteration:    375    step:    14495     combined loss: 4287.12054     paf loss 15.87299     hm loss 4271.24756\n",
      "Iteration:    380    step:    14500     combined loss: 2986.22395     paf loss 19.22322     hm loss 2967.00073\n",
      "Iteration:    385    step:    14505     combined loss: 5426.65075     paf loss 21.93982     hm loss 5404.71094\n",
      "Iteration:    390    step:    14510     combined loss: 4048.95872     paf loss 20.50291     hm loss 4028.45581\n",
      "Iteration:    395    step:    14515     combined loss: 4428.88974     paf loss 20.67783     hm loss 4408.21191\n",
      "Iteration:    400    step:    14520     combined loss: 3345.80960     paf loss 26.52420     hm loss 3319.28540\n",
      "Iteration:    405    step:    14525     combined loss: 2957.37734     paf loss 19.03774     hm loss 2938.33960\n",
      "Iteration:    410    step:    14530     combined loss: 3082.47177     paf loss 19.78598     hm loss 3062.68579\n",
      "Iteration:    415    step:    14535     combined loss: 3437.11492     paf loss 19.03094     hm loss 3418.08398\n",
      "Iteration:    420    step:    14540     combined loss: 3257.42251     paf loss 18.40408     hm loss 3239.01843\n",
      "Iteration:    425    step:    14545     combined loss: 3687.18484     paf loss 18.24941     hm loss 3668.93542\n",
      "Iteration:    430    step:    14550     combined loss: 3788.51326     paf loss 21.74214     hm loss 3766.77112\n",
      "Iteration:    435    step:    14555     combined loss: 4581.90899     paf loss 29.10772     hm loss 4552.80127\n",
      "Iteration:    440    step:    14560     combined loss: 3954.13179     paf loss 19.24751     hm loss 3934.88428\n",
      "Iteration:    445    step:    14565     combined loss: 3092.89343     paf loss 15.57031     hm loss 3077.32312\n",
      "Iteration:    450    step:    14570     combined loss: 2368.44100     paf loss 17.52535     hm loss 2350.91565\n",
      "Iteration:    455    step:    14575     combined loss: 6276.24960     paf loss 25.11239     hm loss 6251.13721\n",
      "Iteration:    460    step:    14580     combined loss: 2860.17136     paf loss 20.45261     hm loss 2839.71875\n",
      "Iteration:    465    step:    14585     combined loss: 3060.42664     paf loss 23.01319     hm loss 3037.41345\n",
      "Iteration:    470    step:    14590     combined loss: 4176.31940     paf loss 20.48761     hm loss 4155.83179\n",
      "Iteration:    475    step:    14595     combined loss: 3586.29665     paf loss 19.15163     hm loss 3567.14502\n",
      "Iteration:    480    step:    14600     combined loss: 3989.64712     paf loss 16.62356     hm loss 3973.02356\n",
      "Iteration:    485    step:    14605     combined loss: 3479.79340     paf loss 16.32038     hm loss 3463.47302\n",
      "Iteration:    490    step:    14610     combined loss: 4281.19850     paf loss 25.18849     hm loss 4256.01001\n",
      "Iteration:    495    step:    14615     combined loss: 4809.58628     paf loss 23.11802     hm loss 4786.46826\n",
      "Iteration:    500    step:    14620     combined loss: 5158.47596     paf loss 21.92615     hm loss 5136.54980\n",
      "Iteration:    505    step:    14625     combined loss: 3808.90056     paf loss 22.75517     hm loss 3786.14539\n",
      "Iteration:    510    step:    14630     combined loss: 3301.85455     paf loss 20.89886     hm loss 3280.95569\n",
      "Iteration:    515    step:    14635     combined loss: 2329.07718     paf loss 19.11698     hm loss 2309.96021\n",
      "Iteration:    520    step:    14640     combined loss: 4066.12561     paf loss 20.09314     hm loss 4046.03247\n",
      "Iteration:    525    step:    14645     combined loss: 3336.13040     paf loss 21.00919     hm loss 3315.12122\n",
      "Iteration:    530    step:    14650     combined loss: 4321.40126     paf loss 22.15834     hm loss 4299.24292\n",
      "Iteration:    535    step:    14655     combined loss: 4042.74105     paf loss 22.18075     hm loss 4020.56030\n",
      "Iteration:    540    step:    14660     combined loss: 4378.16349     paf loss 24.85514     hm loss 4353.30835\n",
      "Iteration:    545    step:    14665     combined loss: 4882.31938     paf loss 16.78398     hm loss 4865.53540\n",
      "Iteration:    550    step:    14670     combined loss: 3492.96952     paf loss 15.03849     hm loss 3477.93103\n",
      "Iteration:    555    step:    14675     combined loss: 2301.21341     paf loss 17.53507     hm loss 2283.67834\n",
      "Iteration:    560    step:    14680     combined loss: 3298.89401     paf loss 14.89059     hm loss 3284.00342\n",
      "Iteration:    565    step:    14685     combined loss: 2763.61657     paf loss 17.95519     hm loss 2745.66138\n",
      "Iteration:    570    step:    14690     combined loss: 3598.73240     paf loss 21.37120     hm loss 3577.36121\n",
      "Iteration:    575    step:    14695     combined loss: 2270.33790     paf loss 15.48475     hm loss 2254.85315\n",
      "Iteration:    580    step:    14700     combined loss: 2727.35035     paf loss 18.76185     hm loss 2708.58850\n",
      "Iteration:    585    step:    14705     combined loss: 3802.00083     paf loss 24.68809     hm loss 3777.31274\n",
      "Iteration:    590    step:    14710     combined loss: 3016.60741     paf loss 19.54601     hm loss 2997.06140\n",
      "Iteration:    595    step:    14715     combined loss: 3164.78607     paf loss 19.18145     hm loss 3145.60461\n",
      "Iteration:    600    step:    14720     combined loss: 3276.73138     paf loss 20.25433     hm loss 3256.47705\n",
      "Iteration:    605    step:    14725     combined loss: 2071.86172     paf loss 12.33047     hm loss 2059.53125\n",
      "Iteration:    610    step:    14730     combined loss: 2739.14741     paf loss 18.40730     hm loss 2720.74011\n",
      "Iteration:    615    step:    14735     combined loss: 3764.60427     paf loss 13.87209     hm loss 3750.73218\n",
      "Iteration:    620    step:    14740     combined loss: 3183.47432     paf loss 17.03731     hm loss 3166.43701\n",
      "Iteration:    625    step:    14745     combined loss: 3863.22257     paf loss 26.14799     hm loss 3837.07458\n",
      "Iteration:    630    step:    14750     combined loss: 3500.34782     paf loss 21.46366     hm loss 3478.88416\n",
      "Iteration:    635    step:    14755     combined loss: 4327.22584     paf loss 19.45069     hm loss 4307.77515\n",
      "Iteration:    640    step:    14760     combined loss: 3650.51004     paf loss 17.81485     hm loss 3632.69519\n",
      "Iteration:    645    step:    14765     combined loss: 3362.26036     paf loss 22.44627     hm loss 3339.81409\n",
      "Iteration:    650    step:    14770     combined loss: 2596.31149     paf loss 19.85458     hm loss 2576.45691\n",
      "Iteration:    655    step:    14775     combined loss: 2638.85399     paf loss 21.07432     hm loss 2617.77966\n",
      "Iteration:    660    step:    14780     combined loss: 3963.21170     paf loss 21.91873     hm loss 3941.29297\n",
      "Iteration:    665    step:    14785     combined loss: 4338.47344     paf loss 13.93389     hm loss 4324.53955\n",
      "Iteration:    670    step:    14790     combined loss: 3738.87032     paf loss 18.63998     hm loss 3720.23035\n",
      "Iteration:    675    step:    14795     combined loss: 6197.46554     paf loss 23.90670     hm loss 6173.55884\n",
      "Iteration:    680    step:    14800     combined loss: 4139.54009     paf loss 26.20440     hm loss 4113.33569\n",
      "Iteration:    685    step:    14805     combined loss: 3102.80666     paf loss 19.37343     hm loss 3083.43323\n",
      "Iteration:    690    step:    14810     combined loss: 4231.02423     paf loss 20.30474     hm loss 4210.71948\n",
      "Iteration:    695    step:    14815     combined loss: 4760.80835     paf loss 25.42578     hm loss 4735.38257\n",
      "Iteration:    700    step:    14820     combined loss: 3430.50747     paf loss 20.77871     hm loss 3409.72876\n",
      "Iteration:    705    step:    14825     combined loss: 3663.37146     paf loss 22.42920     hm loss 3640.94226\n",
      "Iteration:    710    step:    14830     combined loss: 3355.11788     paf loss 19.02462     hm loss 3336.09326\n",
      "Iteration:    715    step:    14835     combined loss: 3519.12160     paf loss 18.45070     hm loss 3500.67090\n",
      "Iteration:    720    step:    14840     combined loss: 3733.12254     paf loss 23.09056     hm loss 3710.03198\n",
      "Iteration:    725    step:    14845     combined loss: 2470.94181     paf loss 19.72721     hm loss 2451.21460\n",
      "Iteration:    730    step:    14850     combined loss: 4158.72217     paf loss 19.91528     hm loss 4138.80688\n",
      "Iteration:    735    step:    14855     combined loss: 3299.57090     paf loss 22.39317     hm loss 3277.17773\n",
      "Iteration:    740    step:    14860     combined loss: 2419.34088     paf loss 23.00104     hm loss 2396.33984\n",
      "Iteration:    745    step:    14865     combined loss: 3525.96705     paf loss 19.28773     hm loss 3506.67932\n",
      "Iteration:    750    step:    14870     combined loss: 2758.87351     paf loss 13.56724     hm loss 2745.30627\n",
      "Iteration:    755    step:    14875     combined loss: 3180.02752     paf loss 22.60650     hm loss 3157.42102\n",
      "Iteration:    760    step:    14880     combined loss: 4882.81915     paf loss 20.24004     hm loss 4862.57910\n",
      "Iteration:    765    step:    14885     combined loss: 4184.46309     paf loss 20.28560     hm loss 4164.17749\n",
      "Iteration:    770    step:    14890     combined loss: 3178.77387     paf loss 16.05500     hm loss 3162.71887\n",
      "Iteration:    775    step:    14895     combined loss: 3838.39558     paf loss 27.03242     hm loss 3811.36316\n",
      "Iteration:    780    step:    14900     combined loss: 3694.75281     paf loss 17.13807     hm loss 3677.61475\n",
      "Iteration:    785    step:    14905     combined loss: 4592.20821     paf loss 18.48555     hm loss 4573.72266\n",
      "Iteration:    790    step:    14910     combined loss: 4321.28247     paf loss 19.10425     hm loss 4302.17822\n",
      "Iteration:    795    step:    14915     combined loss: 2877.84009     paf loss 23.78186     hm loss 2854.05823\n",
      "Iteration:    800    step:    14920     combined loss: 4246.55344     paf loss 26.16453     hm loss 4220.38892\n",
      "Iteration:    805    step:    14925     combined loss: 3498.36833     paf loss 15.84574     hm loss 3482.52258\n",
      "Iteration:    810    step:    14930     combined loss: 4185.23859     paf loss 17.01447     hm loss 4168.22412\n",
      "Iteration:    815    step:    14935     combined loss: 3144.98143     paf loss 17.02757     hm loss 3127.95386\n",
      "Iteration:    820    step:    14940     combined loss: 4562.80648     paf loss 20.22005     hm loss 4542.58643\n",
      "Iteration:    825    step:    14945     combined loss: 2774.08973     paf loss 19.68971     hm loss 2754.40002\n",
      "Iteration:    830    step:    14950     combined loss: 3543.46835     paf loss 22.47909     hm loss 3520.98926\n",
      "Iteration:    835    step:    14955     combined loss: 4654.92667     paf loss 29.16153     hm loss 4625.76514\n",
      "Iteration:    840    step:    14960     combined loss: 1959.64320     paf loss 14.50954     hm loss 1945.13367\n",
      "Iteration:    845    step:    14965     combined loss: 4491.49770     paf loss 23.54165     hm loss 4467.95605\n",
      "Iteration:    850    step:    14970     combined loss: 4171.97792     paf loss 19.78847     hm loss 4152.18945\n",
      "Iteration:    855    step:    14975     combined loss: 3107.70109     paf loss 21.44181     hm loss 3086.25928\n",
      "Iteration:    860    step:    14980     combined loss: 2719.77416     paf loss 18.23412     hm loss 2701.54004\n",
      "Iteration:    865    step:    14985     combined loss: 3679.07947     paf loss 23.25379     hm loss 3655.82568\n",
      "Iteration:    870    step:    14990     combined loss: 3549.67785     paf loss 20.44250     hm loss 3529.23535\n",
      "Iteration:    875    step:    14995     combined loss: 2723.23590     paf loss 19.94672     hm loss 2703.28918\n",
      "Iteration:    880    step:    15000     combined loss: 2493.64219     paf loss 18.79331     hm loss 2474.84888\n",
      "Iteration:    885    step:    15005     combined loss: 2619.02906     paf loss 16.42615     hm loss 2602.60291\n",
      "Iteration:    890    step:    15010     combined loss: 7682.49911     paf loss 30.95736     hm loss 7651.54175\n",
      "Iteration:    895    step:    15015     combined loss: 2495.61351     paf loss 16.51561     hm loss 2479.09790\n",
      "Iteration:    900    step:    15020     combined loss: 3096.64242     paf loss 19.76815     hm loss 3076.87427\n",
      "Iteration:    905    step:    15025     combined loss: 3136.85540     paf loss 22.24383     hm loss 3114.61157\n",
      "Iteration:    910    step:    15030     combined loss: 2983.15489     paf loss 20.18150     hm loss 2962.97339\n",
      "Iteration:    915    step:    15035     combined loss: 2787.74800     paf loss 22.09773     hm loss 2765.65027\n",
      "Iteration:    920    step:    15040     combined loss: 1952.00074     paf loss 12.35602     hm loss 1939.64471\n",
      "Iteration:    925    step:    15045     combined loss: 4546.55073     paf loss 22.77241     hm loss 4523.77832\n",
      "Iteration:    930    step:    15050     combined loss: 3399.74006     paf loss 25.18550     hm loss 3374.55457\n",
      "Iteration:    935    step:    15055     combined loss: 1992.29330     paf loss 15.42080     hm loss 1976.87250\n",
      "Iteration:    940    step:    15060     combined loss: 2015.23088     paf loss 10.64195     hm loss 2004.58893\n",
      "Iteration:    945    step:    15065     combined loss: 4171.98478     paf loss 19.99138     hm loss 4151.99341\n",
      "Iteration:    950    step:    15070     combined loss: 4305.16393     paf loss 18.06627     hm loss 4287.09766\n",
      "Iteration:    955    step:    15075     combined loss: 4709.29153     paf loss 24.16336     hm loss 4685.12817\n",
      "Iteration:    960    step:    15080     combined loss: 3907.07362     paf loss 19.04469     hm loss 3888.02893\n",
      "Iteration:    965    step:    15085     combined loss: 4378.80641     paf loss 20.24123     hm loss 4358.56519\n",
      "Iteration:    970    step:    15090     combined loss: 5610.48172     paf loss 24.23294     hm loss 5586.24878\n",
      "Iteration:    975    step:    15095     combined loss: 2797.43306     paf loss 14.77888     hm loss 2782.65417\n",
      "Iteration:    980    step:    15100     combined loss: 3871.18830     paf loss 22.01362     hm loss 3849.17468\n",
      "Iteration:    985    step:    15105     combined loss: 2513.10274     paf loss 22.25899     hm loss 2490.84375\n",
      "Iteration:    990    step:    15110     combined loss: 2912.85310     paf loss 18.61787     hm loss 2894.23523\n",
      "Iteration:    995    step:    15115     combined loss: 3500.12281     paf loss 22.06874     hm loss 3478.05408\n",
      "Iteration:   1000    step:    15120     combined loss: 2994.89404     paf loss 20.13830     hm loss 2974.75574\n",
      "Iteration:   1005    step:    15125     combined loss: 5133.80712     paf loss 19.40209     hm loss 5114.40503\n",
      "Iteration:   1010    step:    15130     combined loss: 3371.48174     paf loss 23.41545     hm loss 3348.06628\n",
      "Iteration:   1015    step:    15135     combined loss: 3251.86705     paf loss 24.61669     hm loss 3227.25037\n",
      "Iteration:   1020    step:    15140     combined loss: 3693.26247     paf loss 18.61452     hm loss 3674.64795\n",
      "Iteration:   1025    step:    15145     combined loss: 4467.54219     paf loss 21.48653     hm loss 4446.05566\n",
      "Iteration:   1030    step:    15150     combined loss: 5485.47099     paf loss 25.60551     hm loss 5459.86548\n",
      "Iteration:   1035    step:    15155     combined loss: 3578.60858     paf loss 21.59162     hm loss 3557.01697\n",
      "Iteration:   1040    step:    15160     combined loss: 2953.69737     paf loss 20.59581     hm loss 2933.10156\n",
      "Iteration:   1045    step:    15165     combined loss: 3212.34908     paf loss 23.57332     hm loss 3188.77576\n",
      "Iteration:   1050    step:    15170     combined loss: 3165.12710     paf loss 18.58206     hm loss 3146.54504\n",
      "Iteration:   1055    step:    15175     combined loss: 5193.12882     paf loss 23.34880     hm loss 5169.78003\n",
      "Iteration:   1060    step:    15180     combined loss: 4928.16261     paf loss 19.26442     hm loss 4908.89819\n",
      "Iteration:   1065    step:    15185     combined loss: 3792.24944     paf loss 21.01494     hm loss 3771.23450\n",
      "Iteration:   1070    step:    15190     combined loss: 3715.46636     paf loss 22.72271     hm loss 3692.74365\n",
      "Iteration:   1075    step:    15195     combined loss: 3771.51187     paf loss 22.46841     hm loss 3749.04346\n",
      "Iteration:   1080    step:    15200     combined loss: 3899.22119     paf loss 20.05908     hm loss 3879.16211\n",
      "Iteration:   1085    step:    15205     combined loss: 4823.08033     paf loss 24.28443     hm loss 4798.79590\n",
      "Iteration:   1090    step:    15210     combined loss: 2716.98000     paf loss 19.38589     hm loss 2697.59412\n",
      "Iteration:   1095    step:    15215     combined loss: 4136.70980     paf loss 22.19417     hm loss 4114.51562\n",
      "Iteration:   1100    step:    15220     combined loss: 3675.53110     paf loss 19.38339     hm loss 3656.14771\n",
      "Iteration:   1105    step:    15225     combined loss: 3553.78042     paf loss 21.31313     hm loss 3532.46729\n",
      "Iteration:   1110    step:    15230     combined loss: 3363.53787     paf loss 17.09232     hm loss 3346.44556\n",
      "Iteration:   1115    step:    15235     combined loss: 3138.49188     paf loss 12.99359     hm loss 3125.49829\n",
      "Iteration:   1120    step:    15240     combined loss: 3150.26401     paf loss 20.08505     hm loss 3130.17896\n",
      "Iteration:   1125    step:    15245     combined loss: 5705.06579     paf loss 29.39856     hm loss 5675.66724\n",
      "Iteration:   1130    step:    15250     combined loss: 4744.27991     paf loss 24.21253     hm loss 4720.06738\n",
      "Iteration:   1135    step:    15255     combined loss: 3717.58303     paf loss 25.34206     hm loss 3692.24097\n",
      "Iteration:   1140    step:    15260     combined loss: 4005.35630     paf loss 23.76939     hm loss 3981.58691\n",
      "Iteration:   1145    step:    15265     combined loss: 3270.48779     paf loss 15.79284     hm loss 3254.69495\n",
      "Iteration:   1150    step:    15270     combined loss: 2944.79038     paf loss 21.40000     hm loss 2923.39038\n",
      "Iteration:   1155    step:    15275     combined loss: 2607.54560     paf loss 19.83027     hm loss 2587.71533\n",
      "Iteration:   1160    step:    15280     combined loss: 3029.31006     paf loss 17.86328     hm loss 3011.44678\n",
      "Iteration:   1165    step:    15285     combined loss: 3176.29591     paf loss 14.25245     hm loss 3162.04346\n",
      "Iteration:   1170    step:    15290     combined loss: 2816.13192     paf loss 19.66634     hm loss 2796.46558\n",
      "Iteration:   1175    step:    15295     combined loss: 3052.22019     paf loss 17.50681     hm loss 3034.71338\n",
      "Iteration:   1180    step:    15300     combined loss: 5013.02595     paf loss 20.93220     hm loss 4992.09375\n",
      "Iteration:   1185    step:    15305     combined loss: 3423.01380     paf loss 23.90674     hm loss 3399.10706\n",
      "Iteration:   1190    step:    15310     combined loss: 4562.76565     paf loss 18.08694     hm loss 4544.67871\n",
      "Iteration:   1195    step:    15315     combined loss: 3885.73788     paf loss 14.83200     hm loss 3870.90588\n",
      "Iteration:   1200    step:    15320     combined loss: 2898.33308     paf loss 23.96199     hm loss 2874.37109\n",
      "Iteration:   1205    step:    15325     combined loss: 3909.58131     paf loss 20.13124     hm loss 3889.45007\n",
      "Iteration:   1210    step:    15330     combined loss: 3296.32692     paf loss 17.84230     hm loss 3278.48462\n",
      "Iteration:   1215    step:    15335     combined loss: 3718.78772     paf loss 17.83386     hm loss 3700.95386\n",
      "Iteration:   1220    step:    15340     combined loss: 3316.46019     paf loss 16.96996     hm loss 3299.49023\n",
      "Iteration:   1225    step:    15345     combined loss: 2292.80275     paf loss 15.22328     hm loss 2277.57947\n",
      "Iteration:   1230    step:    15350     combined loss: 3047.57170     paf loss 21.93596     hm loss 3025.63574\n",
      "Iteration:   1235    step:    15355     combined loss: 1838.07519     paf loss 14.64147     hm loss 1823.43372\n",
      "Iteration:   1240    step:    15360     combined loss: 3407.17039     paf loss 17.88243     hm loss 3389.28796\n",
      "Iteration:   1245    step:    15365     combined loss: 2040.55112     paf loss 17.02298     hm loss 2023.52814\n",
      "Iteration:   1250    step:    15370     combined loss: 3377.03419     paf loss 17.93263     hm loss 3359.10156\n",
      "Iteration:   1255    step:    15375     combined loss: 3777.50710     paf loss 22.67690     hm loss 3754.83020\n",
      "Iteration:   1260    step:    15380     combined loss: 4265.41133     paf loss 25.75630     hm loss 4239.65503\n",
      "Iteration:   1265    step:    15385     combined loss: 3741.09043     paf loss 20.45652     hm loss 3720.63391\n",
      "Iteration:   1270    step:    15390     combined loss: 3883.20203     paf loss 15.93677     hm loss 3867.26526\n",
      "Iteration:   1275    step:    15395     combined loss: 5080.39715     paf loss 18.67547     hm loss 5061.72168\n",
      "Iteration:   1280    step:    15400     combined loss: 4538.39786     paf loss 16.94302     hm loss 4521.45483\n",
      "Iteration:   1285    step:    15405     combined loss: 4388.85042     paf loss 18.06380     hm loss 4370.78662\n",
      "Iteration:   1290    step:    15410     combined loss: 6820.80528     paf loss 31.03550     hm loss 6789.76978\n",
      "Iteration:   1295    step:    15415     combined loss: 4801.45330     paf loss 21.28753     hm loss 4780.16577\n",
      "Iteration:   1300    step:    15420     combined loss: 2548.48020     paf loss 20.36960     hm loss 2528.11060\n",
      "Iteration:   1305    step:    15425     combined loss: 3452.50879     paf loss 18.65368     hm loss 3433.85510\n",
      "Iteration:   1310    step:    15430     combined loss: 3045.12458     paf loss 19.88300     hm loss 3025.24158\n",
      "Iteration:   1315    step:    15435     combined loss: 3651.48085     paf loss 19.80592     hm loss 3631.67493\n",
      "Iteration:   1320    step:    15440     combined loss: 4784.87953     paf loss 28.90126     hm loss 4755.97827\n",
      "Iteration:   1325    step:    15445     combined loss: 4008.66988     paf loss 16.72640     hm loss 3991.94348\n",
      "Iteration:   1330    step:    15450     combined loss: 5743.18953     paf loss 29.31258     hm loss 5713.87695\n",
      "Iteration:   1335    step:    15455     combined loss: 3257.33498     paf loss 22.35415     hm loss 3234.98083\n",
      "Iteration:   1340    step:    15460     combined loss: 3065.91600     paf loss 25.14842     hm loss 3040.76758\n",
      "Iteration:   1345    step:    15465     combined loss: 3773.61818     paf loss 22.50466     hm loss 3751.11353\n",
      "Iteration:   1350    step:    15470     combined loss: 3456.89594     paf loss 19.17817     hm loss 3437.71777\n",
      "Iteration:   1355    step:    15475     combined loss: 2577.85781     paf loss 16.60171     hm loss 2561.25610\n",
      "Iteration:   1360    step:    15480     combined loss: 2896.40135     paf loss 17.59959     hm loss 2878.80176\n",
      "Iteration:   1365    step:    15485     combined loss: 2585.80048     paf loss 20.53401     hm loss 2565.26648\n",
      "Iteration:   1370    step:    15490     combined loss: 2447.87113     paf loss 18.07670     hm loss 2429.79443\n",
      "Iteration:   1375    step:    15495     combined loss: 3975.29033     paf loss 18.42082     hm loss 3956.86951\n",
      "Iteration:   1380    step:    15500     combined loss: 2069.26793     paf loss 12.09916     hm loss 2057.16876\n",
      "Iteration:   1385    step:    15505     combined loss: 3485.00934     paf loss 20.84919     hm loss 3464.16016\n",
      "Iteration:   1390    step:    15510     combined loss: 5458.97697     paf loss 24.12296     hm loss 5434.85400\n",
      "Iteration:   1395    step:    15515     combined loss: 3295.51191     paf loss 15.64399     hm loss 3279.86792\n",
      "Iteration:   1400    step:    15520     combined loss: 3803.43437     paf loss 18.48905     hm loss 3784.94531\n",
      "Iteration:   1405    step:    15525     combined loss: 4086.81938     paf loss 22.06633     hm loss 4064.75305\n",
      "Iteration:   1410    step:    15530     combined loss: 4207.14111     paf loss 20.03979     hm loss 4187.10132\n",
      "Iteration:   1415    step:    15535     combined loss: 3754.48277     paf loss 23.70958     hm loss 3730.77319\n",
      "Iteration:   1420    step:    15540     combined loss: 4302.27997     paf loss 22.75238     hm loss 4279.52759\n",
      "Iteration:   1425    step:    15545     combined loss: 3010.95960     paf loss 18.60230     hm loss 2992.35730\n",
      "Iteration:   1430    step:    15550     combined loss: 3116.57957     paf loss 17.33665     hm loss 3099.24292\n",
      "Iteration:   1435    step:    15555     combined loss: 3600.87819     paf loss 10.59389     hm loss 3590.28430\n",
      "Iteration:   1440    step:    15560     combined loss: 2339.55309     paf loss 11.29296     hm loss 2328.26013\n",
      "Iteration:   1445    step:    15565     combined loss: 5187.71689     paf loss 22.40415     hm loss 5165.31274\n",
      "Iteration:   1450    step:    15570     combined loss: 3177.50795     paf loss 22.02199     hm loss 3155.48596\n",
      "Iteration:   1455    step:    15575     combined loss: 3181.66148     paf loss 19.60862     hm loss 3162.05286\n",
      "Iteration:   1460    step:    15580     combined loss: 4924.63260     paf loss 26.51639     hm loss 4898.11621\n",
      "Iteration:   1465    step:    15585     combined loss: 3846.00198     paf loss 21.55264     hm loss 3824.44934\n",
      "Iteration:   1470    step:    15590     combined loss: 3220.29005     paf loss 21.72340     hm loss 3198.56665\n",
      "Iteration:   1475    step:    15595     combined loss: 3743.46422     paf loss 19.04527     hm loss 3724.41895\n",
      "Iteration:   1480    step:    15600     combined loss: 3621.27293     paf loss 20.26865     hm loss 3601.00427\n",
      "Iteration:   1485    step:    15605     combined loss: 2567.81440     paf loss 14.11787     hm loss 2553.69653\n",
      "Iteration:   1490    step:    15610     combined loss: 4172.70512     paf loss 24.90361     hm loss 4147.80151\n",
      "Iteration:   1495    step:    15615     combined loss: 2722.62906     paf loss 20.10806     hm loss 2702.52100\n",
      "Iteration:   1500    step:    15620     combined loss: 3364.41575     paf loss 19.11717     hm loss 3345.29858\n",
      "Iteration:   1505    step:    15625     combined loss: 5085.59391     paf loss 21.09831     hm loss 5064.49561\n",
      "Iteration:   1510    step:    15630     combined loss: 3116.33080     paf loss 13.18041     hm loss 3103.15039\n",
      "Iteration:   1515    step:    15635     combined loss: 3441.81973     paf loss 24.33548     hm loss 3417.48425\n",
      "Iteration:   1520    step:    15640     combined loss: 4205.15359     paf loss 22.08426     hm loss 4183.06934\n",
      "Iteration:   1525    step:    15645     combined loss: 6331.69520     paf loss 25.13221     hm loss 6306.56299\n",
      "Iteration:   1530    step:    15650     combined loss: 3102.80377     paf loss 19.71808     hm loss 3083.08569\n",
      "Iteration:   1535    step:    15655     combined loss: 3814.15660     paf loss 20.47667     hm loss 3793.67993\n",
      "Iteration:   1540    step:    15660     combined loss: 2887.10283     paf loss 19.53533     hm loss 2867.56750\n",
      "Iteration:   1545    step:    15665     combined loss: 3977.70046     paf loss 19.17471     hm loss 3958.52576\n",
      "Iteration:   1550    step:    15670     combined loss: 3616.83332     paf loss 21.55537     hm loss 3595.27795\n",
      "Iteration:   1555    step:    15675     combined loss: 3964.73235     paf loss 21.34099     hm loss 3943.39136\n",
      "Iteration:   1560    step:    15680     combined loss: 2696.62660     paf loss 17.41517     hm loss 2679.21143\n",
      "Iteration:   1565    step:    15685     combined loss: 3931.23620     paf loss 18.46472     hm loss 3912.77148\n",
      "Iteration:   1570    step:    15690     combined loss: 3388.26976     paf loss 24.20433     hm loss 3364.06543\n",
      "Iteration:   1575    step:    15695     combined loss: 3651.90602     paf loss 21.32509     hm loss 3630.58093\n",
      "Iteration:   1580    step:    15700     combined loss: 4030.38818     paf loss 25.40478     hm loss 4004.98340\n",
      "Iteration:   1585    step:    15705     combined loss: 2449.80878     paf loss 18.56476     hm loss 2431.24402\n",
      "Iteration:   1590    step:    15710     combined loss: 2801.24897     paf loss 19.77985     hm loss 2781.46912\n",
      "Iteration:   1595    step:    15715     combined loss: 3392.38944     paf loss 24.05924     hm loss 3368.33020\n",
      "Iteration:   1600    step:    15720     combined loss: 4038.46019     paf loss 22.33848     hm loss 4016.12170\n",
      "Iteration:   1605    step:    15725     combined loss: 3755.31176     paf loss 21.16222     hm loss 3734.14954\n",
      "Iteration:   1610    step:    15730     combined loss: 2383.15786     paf loss 16.74661     hm loss 2366.41125\n",
      "Iteration:   1615    step:    15735     combined loss: 4896.08003     paf loss 23.12910     hm loss 4872.95093\n",
      "Iteration:   1620    step:    15740     combined loss: 3576.78579     paf loss 16.42568     hm loss 3560.36011\n",
      "Iteration:   1625    step:    15745     combined loss: 3740.47409     paf loss 16.61423     hm loss 3723.85986\n",
      "Iteration:   1630    step:    15750     combined loss: 3511.47083     paf loss 16.80689     hm loss 3494.66394\n",
      "Iteration:   1635    step:    15755     combined loss: 3850.40211     paf loss 18.95484     hm loss 3831.44727\n",
      "Iteration:   1640    step:    15760     combined loss: 4019.83014     paf loss 24.47736     hm loss 3995.35278\n",
      "Iteration:   1645    step:    15765     combined loss: 5200.94907     paf loss 18.86533     hm loss 5182.08374\n",
      "Iteration:   1650    step:    15770     combined loss: 4124.37838     paf loss 19.17830     hm loss 4105.20007\n",
      "Iteration:   1655    step:    15775     combined loss: 3142.95480     paf loss 18.22165     hm loss 3124.73315\n",
      "Iteration:   1660    step:    15780     combined loss: 4684.89672     paf loss 19.19531     hm loss 4665.70142\n",
      "Iteration:   1665    step:    15785     combined loss: 3945.40718     paf loss 18.91927     hm loss 3926.48792\n",
      "Iteration:   1670    step:    15790     combined loss: 3157.34682     paf loss 15.42690     hm loss 3141.91992\n",
      "Iteration:   1675    step:    15795     combined loss: 3943.37127     paf loss 16.30425     hm loss 3927.06702\n",
      "Iteration:   1680    step:    15800     combined loss: 4457.61081     paf loss 22.30001     hm loss 4435.31079\n",
      "Iteration:   1685    step:    15805     combined loss: 3186.75297     paf loss 18.27470     hm loss 3168.47827\n",
      "Iteration:   1690    step:    15810     combined loss: 2231.39790     paf loss 17.52021     hm loss 2213.87769\n",
      "Iteration:   1695    step:    15815     combined loss: 3089.04789     paf loss 14.28800     hm loss 3074.75989\n",
      "Iteration:   1700    step:    15820     combined loss: 4494.90379     paf loss 27.10545     hm loss 4467.79834\n",
      "Iteration:   1705    step:    15825     combined loss: 3528.04062     paf loss 20.80478     hm loss 3507.23584\n",
      "Iteration:   1710    step:    15830     combined loss: 4297.65567     paf loss 24.08414     hm loss 4273.57153\n",
      "Iteration:   1715    step:    15835     combined loss: 4397.44218     paf loss 21.56498     hm loss 4375.87720\n",
      "Iteration:   1720    step:    15840     combined loss: 3829.63588     paf loss 22.11879     hm loss 3807.51709\n",
      "Iteration:   1725    step:    15845     combined loss: 2833.57682     paf loss 19.56534     hm loss 2814.01147\n",
      "Iteration:   1730    step:    15850     combined loss: 5465.31044     paf loss 22.40175     hm loss 5442.90869\n",
      "Iteration:   1735    step:    15855     combined loss: 3156.94896     paf loss 17.39903     hm loss 3139.54993\n",
      "Iteration:   1740    step:    15860     combined loss: 2906.90968     paf loss 20.27577     hm loss 2886.63391\n",
      "Iteration:   1745    step:    15865     combined loss: 3010.87157     paf loss 11.03173     hm loss 2999.83984\n",
      "Iteration:   1750    step:    15870     combined loss: 2709.99195     paf loss 17.30335     hm loss 2692.68860\n",
      "Iteration:   1755    step:    15875     combined loss: 2258.55267     paf loss 11.38715     hm loss 2247.16553\n",
      "Iteration:   1760    step:    15880     combined loss: 3224.01621     paf loss 18.67514     hm loss 3205.34106\n",
      "Iteration:   1765    step:    15885     combined loss: 3504.22689     paf loss 21.85311     hm loss 3482.37378\n",
      "Iteration:   1770    step:    15890     combined loss: 2869.66745     paf loss 18.67966     hm loss 2850.98779\n",
      "Iteration:   1775    step:    15895     combined loss: 3820.04061     paf loss 21.84164     hm loss 3798.19897\n",
      "Iteration:   1780    step:    15900     combined loss: 3700.08416     paf loss 18.31890     hm loss 3681.76526\n",
      "Iteration:   1785    step:    15905     combined loss: 3510.64232     paf loss 17.33373     hm loss 3493.30859\n",
      "Iteration:   1790    step:    15910     combined loss: 4652.15929     paf loss 25.70568     hm loss 4626.45361\n",
      "Iteration:   1795    step:    15915     combined loss: 2310.57735     paf loss 17.20943     hm loss 2293.36792\n",
      "Iteration:   1800    step:    15920     combined loss: 2864.18288     paf loss 16.17067     hm loss 2848.01221\n",
      "Iteration:   1805    step:    15925     combined loss: 4448.14821     paf loss 18.02297     hm loss 4430.12524\n",
      "Iteration:   1810    step:    15930     combined loss: 3767.18758     paf loss 23.80013     hm loss 3743.38745\n",
      "Iteration:   1815    step:    15935     combined loss: 4047.54891     paf loss 23.08456     hm loss 4024.46436\n",
      "Iteration:   1820    step:    15940     combined loss: 2774.53817     paf loss 20.39559     hm loss 2754.14258\n",
      "Iteration:   1825    step:    15945     combined loss: 2819.91491     paf loss 21.84594     hm loss 2798.06897\n",
      "Iteration:   1830    step:    15950     combined loss: 4744.08665     paf loss 21.53733     hm loss 4722.54932\n",
      "Iteration:   1835    step:    15955     combined loss: 4696.83305     paf loss 25.89506     hm loss 4670.93799\n",
      "Iteration:   1840    step:    15960     combined loss: 3498.81926     paf loss 14.67522     hm loss 3484.14404\n",
      "Iteration:   1845    step:    15965     combined loss: 3032.66844     paf loss 16.50413     hm loss 3016.16431\n",
      "Iteration:   1850    step:    15970     combined loss: 4458.16014     paf loss 24.96800     hm loss 4433.19214\n",
      "Iteration:   1855    step:    15975     combined loss: 8075.51226     paf loss 30.15655     hm loss 8045.35571\n",
      "Iteration:   1860    step:    15980     combined loss: 3401.77808     paf loss 21.35658     hm loss 3380.42151\n",
      "Iteration:   1865    step:    15985     combined loss: 3261.25378     paf loss 20.02490     hm loss 3241.22888\n",
      "Iteration:   1870    step:    15990     combined loss: 3811.79120     paf loss 25.03509     hm loss 3786.75610\n",
      "Iteration:   1875    step:    15995     combined loss: 2706.02037     paf loss 16.54869     hm loss 2689.47168\n",
      "Iteration:   1880    step:    16000     combined loss: 3469.47508     paf loss 19.34849     hm loss 3450.12659\n",
      "Iteration:   1885    step:    16005     combined loss: 3019.67541     paf loss 21.30114     hm loss 2998.37427\n",
      "Iteration:   1890    step:    16010     combined loss: 6557.41014     paf loss 28.59935     hm loss 6528.81079\n",
      "Iteration:   1895    step:    16015     combined loss: 3849.90697     paf loss 22.44737     hm loss 3827.45959\n",
      "Iteration:   1900    step:    16020     combined loss: 4972.73570     paf loss 27.04210     hm loss 4945.69360\n",
      "Iteration:   1905    step:    16025     combined loss: 3299.93545     paf loss 18.49771     hm loss 3281.43774\n",
      "Iteration:   1910    step:    16030     combined loss: 5397.72899     paf loss 22.46727     hm loss 5375.26172\n",
      "Iteration:   1915    step:    16035     combined loss: 3826.22655     paf loss 24.22240     hm loss 3802.00415\n",
      "Iteration:   1920    step:    16040     combined loss: 3827.99722     paf loss 19.13980     hm loss 3808.85742\n",
      "Iteration:   1925    step:    16045     combined loss: 2747.39743     paf loss 19.22165     hm loss 2728.17578\n",
      "Iteration:   1930    step:    16050     combined loss: 2575.91635     paf loss 16.16146     hm loss 2559.75488\n",
      "Iteration:   1935    step:    16055     combined loss: 3047.12378     paf loss 14.35474     hm loss 3032.76904\n",
      "Iteration:   1940    step:    16060     combined loss: 3179.99193     paf loss 20.15062     hm loss 3159.84131\n",
      "Iteration:   1945    step:    16065     combined loss: 3446.09793     paf loss 16.27274     hm loss 3429.82520\n",
      "Iteration:   1950    step:    16070     combined loss: 1777.91121     paf loss 11.15169     hm loss 1766.75952\n",
      "Iteration:   1955    step:    16075     combined loss: 3020.59929     paf loss 14.27250     hm loss 3006.32678\n",
      "Iteration:   1960    step:    16080     combined loss: 3697.13587     paf loss 17.90686     hm loss 3679.22900\n",
      "Iteration:   1965    step:    16085     combined loss: 3432.90443     paf loss 15.78822     hm loss 3417.11621\n",
      "Iteration:   1970    step:    16090     combined loss: 3184.08783     paf loss 19.58002     hm loss 3164.50781\n",
      "Iteration:   1975    step:    16095     combined loss: 4874.42122     paf loss 22.88875     hm loss 4851.53247\n",
      "Iteration:   1980    step:    16100     combined loss: 4099.97865     paf loss 27.72853     hm loss 4072.25012\n",
      "Iteration:   1985    step:    16105     combined loss: 1992.25857     paf loss 16.62954     hm loss 1975.62903\n",
      "Iteration:   1990    step:    16110     combined loss: 3596.73652     paf loss 22.03645     hm loss 3574.70007\n",
      "Iteration:   1995    step:    16115     combined loss: 4512.35855     paf loss 21.69863     hm loss 4490.65991\n",
      "Iteration:   2000    step:    16120     combined loss: 2203.34128     paf loss 16.15439     hm loss 2187.18689\n",
      "Iteration:   2005    step:    16125     combined loss: 3002.89290     paf loss 20.12251     hm loss 2982.77039\n",
      "Iteration:   2010    step:    16130     combined loss: 2756.19774     paf loss 18.56041     hm loss 2737.63733\n",
      "Iteration:   2015    step:    16135     combined loss: 3326.09468     paf loss 24.82979     hm loss 3301.26489\n",
      "Iteration:   2020    step:    16140     combined loss: 3377.75101     paf loss 20.93424     hm loss 3356.81677\n",
      "Iteration:   2025    step:    16145     combined loss: 3307.98226     paf loss 18.57589     hm loss 3289.40637\n",
      "Iteration:   2030    step:    16150     combined loss: 3795.14353     paf loss 19.27903     hm loss 3775.86450\n",
      "Iteration:   2035    step:    16155     combined loss: 3540.49235     paf loss 19.13945     hm loss 3521.35291\n",
      "Iteration:   2040    step:    16160     combined loss: 4905.35603     paf loss 19.85383     hm loss 4885.50220\n",
      "Iteration:   2045    step:    16165     combined loss: 3642.19681     paf loss 18.94120     hm loss 3623.25562\n",
      "Iteration:   2050    step:    16170     combined loss: 2455.75345     paf loss 11.69143     hm loss 2444.06201\n",
      "Iteration:   2055    step:    16175     combined loss: 3231.97263     paf loss 23.02597     hm loss 3208.94666\n",
      "Iteration:   2060    step:    16180     combined loss: 4105.63690     paf loss 19.20807     hm loss 4086.42883\n",
      "Iteration:   2065    step:    16185     combined loss: 3880.58066     paf loss 19.29135     hm loss 3861.28931\n",
      "Iteration:   2070    step:    16190     combined loss: 2423.49396     paf loss 16.96771     hm loss 2406.52625\n",
      "Iteration:   2075    step:    16195     combined loss: 3763.74890     paf loss 18.71900     hm loss 3745.02991\n",
      "Iteration:   2080    step:    16200     combined loss: 3540.74233     paf loss 23.42898     hm loss 3517.31335\n",
      "Iteration:   2085    step:    16205     combined loss: 3058.11686     paf loss 20.18876     hm loss 3037.92810\n",
      "Iteration:   2090    step:    16210     combined loss: 5038.45842     paf loss 26.64202     hm loss 5011.81641\n",
      "Iteration:   2095    step:    16215     combined loss: 2637.17864     paf loss 22.70952     hm loss 2614.46912\n",
      "Iteration:   2100    step:    16220     combined loss: 3746.75741     paf loss 18.94650     hm loss 3727.81091\n",
      "Iteration:   2105    step:    16225     combined loss: 2366.78951     paf loss 14.55953     hm loss 2352.22998\n",
      "Iteration:   2110    step:    16230     combined loss: 2787.99308     paf loss 20.91606     hm loss 2767.07703\n",
      "Iteration:   2115    step:    16235     combined loss: 2504.82930     paf loss 22.92171     hm loss 2481.90759\n",
      "Iteration:   2120    step:    16240     combined loss: 2824.05974     paf loss 15.57378     hm loss 2808.48596\n",
      "Iteration:   2125    step:    16245     combined loss: 2807.86546     paf loss 18.95506     hm loss 2788.91040\n",
      "Iteration:   2130    step:    16250     combined loss: 4055.71880     paf loss 17.22832     hm loss 4038.49048\n",
      "Iteration:   2135    step:    16255     combined loss: 4562.16193     paf loss 20.04621     hm loss 4542.11572\n",
      "Iteration:   2140    step:    16260     combined loss: 3063.54193     paf loss 15.35797     hm loss 3048.18396\n",
      "Iteration:   2145    step:    16265     combined loss: 3482.10847     paf loss 20.23262     hm loss 3461.87585\n",
      "Iteration:   2150    step:    16270     combined loss: 2226.09102     paf loss 13.09334     hm loss 2212.99768\n",
      "Iteration:   2155    step:    16275     combined loss: 2629.04337     paf loss 20.97171     hm loss 2608.07166\n",
      "Iteration:   2160    step:    16280     combined loss: 3462.66741     paf loss 18.22869     hm loss 3444.43872\n",
      "Iteration:   2165    step:    16285     combined loss: 2510.60863     paf loss 16.16247     hm loss 2494.44617\n",
      "Iteration:   2170    step:    16290     combined loss: 3492.49351     paf loss 17.15257     hm loss 3475.34094\n",
      "Iteration:   2175    step:    16295     combined loss: 3392.94945     paf loss 22.05541     hm loss 3370.89404\n",
      "Iteration:   2180    step:    16300     combined loss: 2513.24510     paf loss 17.20030     hm loss 2496.04480\n",
      "Iteration:   2185    step:    16305     combined loss: 3825.66095     paf loss 24.14532     hm loss 3801.51562\n",
      "Iteration:   2190    step:    16310     combined loss: 2917.80997     paf loss 21.08158     hm loss 2896.72839\n",
      "Iteration:   2195    step:    16315     combined loss: 2086.90688     paf loss 15.80398     hm loss 2071.10291\n",
      "Iteration:   2200    step:    16320     combined loss: 4512.16426     paf loss 22.83809     hm loss 4489.32617\n",
      "Iteration:   2205    step:    16325     combined loss: 4528.55061     paf loss 19.00935     hm loss 4509.54126\n",
      "Iteration:   2210    step:    16330     combined loss: 2990.10078     paf loss 19.39851     hm loss 2970.70227\n",
      "Iteration:   2215    step:    16335     combined loss: 2738.34824     paf loss 15.90598     hm loss 2722.44226\n",
      "Iteration:   2220    step:    16340     combined loss: 3446.29757     paf loss 21.16658     hm loss 3425.13098\n",
      "Iteration:   2225    step:    16345     combined loss: 5139.59769     paf loss 21.40116     hm loss 5118.19653\n",
      "Iteration:   2230    step:    16350     combined loss: 2583.10079     paf loss 16.70528     hm loss 2566.39551\n",
      "Iteration:   2235    step:    16355     combined loss: 3046.69570     paf loss 19.83437     hm loss 3026.86133\n",
      "Iteration:   2240    step:    16360     combined loss: 3867.12985     paf loss 20.86910     hm loss 3846.26074\n",
      "Iteration:   2245    step:    16365     combined loss: 3783.26710     paf loss 19.93946     hm loss 3763.32764\n",
      "Iteration:   2250    step:    16370     combined loss: 2943.00257     paf loss 15.31092     hm loss 2927.69165\n",
      "Iteration:   2255    step:    16375     combined loss: 3443.91669     paf loss 25.53314     hm loss 3418.38354\n",
      "Iteration:   2260    step:    16380     combined loss: 2236.12789     paf loss 13.55685     hm loss 2222.57104\n",
      "Iteration:   2265    step:    16385     combined loss: 3789.00046     paf loss 19.64292     hm loss 3769.35754\n",
      "Iteration:   2270    step:    16390     combined loss: 2417.55435     paf loss 15.43253     hm loss 2402.12183\n",
      "Iteration:   2275    step:    16395     combined loss: 2232.92404     paf loss 18.73642     hm loss 2214.18762\n",
      "Iteration:   2280    step:    16400     combined loss: 3421.00392     paf loss 23.59560     hm loss 3397.40833\n",
      "Iteration:   2285    step:    16405     combined loss: 3400.70554     paf loss 23.76413     hm loss 3376.94141\n",
      "Iteration:   2290    step:    16410     combined loss: 2850.92501     paf loss 18.71517     hm loss 2832.20984\n",
      "Iteration:   2295    step:    16415     combined loss: 3188.37369     paf loss 26.04093     hm loss 3162.33276\n",
      "Iteration:   2300    step:    16420     combined loss: 4664.69315     paf loss 22.75614     hm loss 4641.93701\n",
      "Iteration:   2305    step:    16425     combined loss: 3980.96609     paf loss 20.49710     hm loss 3960.46899\n",
      "Iteration:   2310    step:    16430     combined loss: 2943.57068     paf loss 17.50623     hm loss 2926.06445\n",
      "Iteration:   2315    step:    16435     combined loss: 3184.95848     paf loss 16.75084     hm loss 3168.20764\n",
      "Iteration:   2320    step:    16440     combined loss: 3145.32454     paf loss 19.77230     hm loss 3125.55225\n",
      "Iteration:   2325    step:    16445     combined loss: 2785.41747     paf loss 22.53160     hm loss 2762.88586\n",
      "Iteration:   2330    step:    16450     combined loss: 4584.95574     paf loss 29.61615     hm loss 4555.33960\n",
      "Iteration:   2335    step:    16455     combined loss: 3434.60053     paf loss 26.38263     hm loss 3408.21790\n",
      "Iteration:   2340    step:    16460     combined loss: 4407.22608     paf loss 23.79590     hm loss 4383.43018\n",
      "Iteration:   2345    step:    16465     combined loss: 3497.74502     paf loss 24.91018     hm loss 3472.83484\n",
      "Iteration:   2350    step:    16470     combined loss: 3576.31792     paf loss 16.79582     hm loss 3559.52209\n",
      "Iteration:   2355    step:    16475     combined loss: 3838.22228     paf loss 26.50878     hm loss 3811.71350\n",
      "Iteration:   2360    step:    16480     combined loss: 3592.57710     paf loss 19.64961     hm loss 3572.92749\n",
      "Iteration:   2365    step:    16485     combined loss: 3668.63142     paf loss 23.26960     hm loss 3645.36182\n",
      "Iteration:   2370    step:    16490     combined loss: 2660.88375     paf loss 16.82882     hm loss 2644.05493\n",
      "Iteration:   2375    step:    16495     combined loss: 2648.80187     paf loss 17.33812     hm loss 2631.46375\n",
      "Iteration:   2380    step:    16500     combined loss: 3055.05054     paf loss 23.46765     hm loss 3031.58289\n",
      "Iteration:   2385    step:    16505     combined loss: 2905.18614     paf loss 17.96495     hm loss 2887.22119\n",
      "Iteration:   2390    step:    16510     combined loss: 4171.37602     paf loss 20.58672     hm loss 4150.78931\n",
      "Iteration:   2395    step:    16515     combined loss: 3425.64317     paf loss 16.39524     hm loss 3409.24792\n",
      "Iteration:   2400    step:    16520     combined loss: 4130.06090     paf loss 22.09703     hm loss 4107.96387\n",
      "Iteration:   2405    step:    16525     combined loss: 5881.44635     paf loss 33.20660     hm loss 5848.23975\n",
      "Iteration:   2410    step:    16530     combined loss: 2522.69305     paf loss 19.33197     hm loss 2503.36108\n",
      "Iteration:   2415    step:    16535     combined loss: 3508.17856     paf loss 19.49704     hm loss 3488.68152\n",
      "Iteration:   2420    step:    16540     combined loss: 2937.79558     paf loss 16.05766     hm loss 2921.73792\n",
      "Iteration:   2425    step:    16545     combined loss: 3186.02742     paf loss 21.57076     hm loss 3164.45667\n",
      "Iteration:   2430    step:    16550     combined loss: 3826.64468     paf loss 16.73208     hm loss 3809.91260\n",
      "Iteration:   2435    step:    16555     combined loss: 3220.35592     paf loss 20.31661     hm loss 3200.03931\n",
      "Iteration:   2440    step:    16560     combined loss: 2899.41561     paf loss 16.25667     hm loss 2883.15894\n",
      "Iteration:   2445    step:    16565     combined loss: 3010.64606     paf loss 20.95087     hm loss 2989.69519\n",
      "Iteration:   2450    step:    16570     combined loss: 3964.98079     paf loss 18.19771     hm loss 3946.78308\n",
      "Iteration:   2455    step:    16575     combined loss: 3721.26956     paf loss 22.18289     hm loss 3699.08667\n",
      "Iteration:   2460    step:    16580     combined loss: 2476.53323     paf loss 18.63577     hm loss 2457.89746\n",
      "Iteration:   2465    step:    16585     combined loss: 3605.98979     paf loss 22.12700     hm loss 3583.86279\n",
      "Iteration:   2470    step:    16590     combined loss: 2704.06326     paf loss 20.57510     hm loss 2683.48816\n",
      "Iteration:   2475    step:    16595     combined loss: 2861.64441     paf loss 15.12268     hm loss 2846.52173\n",
      "Iteration:   2480    step:    16600     combined loss: 3728.40067     paf loss 21.42082     hm loss 3706.97986\n",
      "Iteration:   2485    step:    16605     combined loss: 2318.99270     paf loss 16.15761     hm loss 2302.83508\n",
      "Iteration:   2490    step:    16610     combined loss: 3694.44703     paf loss 19.50367     hm loss 3674.94336\n",
      "Iteration:   2495    step:    16615     combined loss: 3945.11350     paf loss 21.92612     hm loss 3923.18738\n",
      "Iteration:   2500    step:    16620     combined loss: 2203.61613     paf loss 19.04655     hm loss 2184.56958\n",
      "Iteration:   2505    step:    16625     combined loss: 4024.02244     paf loss 22.21568     hm loss 4001.80676\n",
      "Iteration:   2510    step:    16630     combined loss: 2510.13980     paf loss 17.83865     hm loss 2492.30115\n",
      "Iteration:   2515    step:    16635     combined loss: 4280.03728     paf loss 24.91472     hm loss 4255.12256\n",
      "Iteration:   2520    step:    16640     combined loss: 3983.08143     paf loss 23.89320     hm loss 3959.18823\n",
      "Iteration:   2525    step:    16645     combined loss: 3904.55015     paf loss 21.81052     hm loss 3882.73962\n",
      "Iteration:   2530    step:    16650     combined loss: 2671.11573     paf loss 20.18311     hm loss 2650.93262\n",
      "Iteration:   2535    step:    16655     combined loss: 2826.03368     paf loss 22.80162     hm loss 2803.23206\n",
      "Iteration:   2540    step:    16660     combined loss: 4297.35906     paf loss 16.82781     hm loss 4280.53125\n",
      "Iteration:   2545    step:    16665     combined loss: 3749.97207     paf loss 26.64797     hm loss 3723.32410\n",
      "Iteration:   2550    step:    16670     combined loss: 4991.28930     paf loss 20.36181     hm loss 4970.92749\n",
      "Iteration:   2555    step:    16675     combined loss: 2598.39216     paf loss 19.21736     hm loss 2579.17480\n",
      "Iteration:   2560    step:    16680     combined loss: 3608.27365     paf loss 17.71372     hm loss 3590.55994\n",
      "Iteration:   2565    step:    16685     combined loss: 3531.33446     paf loss 18.39806     hm loss 3512.93640\n",
      "Iteration:   2570    step:    16690     combined loss: 4833.54298     paf loss 23.92042     hm loss 4809.62256\n",
      "Iteration:   2575    step:    16695     combined loss: 2836.58473     paf loss 19.99306     hm loss 2816.59167\n",
      "Iteration:   2580    step:    16700     combined loss: 3080.53694     paf loss 25.94502     hm loss 3054.59192\n",
      "Iteration:   2585    step:    16705     combined loss: 3174.78556     paf loss 18.75126     hm loss 3156.03430\n",
      "Iteration:   2590    step:    16710     combined loss: 3333.59478     paf loss 17.95562     hm loss 3315.63916\n",
      "Iteration:   2595    step:    16715     combined loss: 3186.08404     paf loss 16.41253     hm loss 3169.67151\n",
      "Iteration:   2600    step:    16720     combined loss: 3169.77683     paf loss 20.20578     hm loss 3149.57104\n",
      "Iteration:   2605    step:    16725     combined loss: 3088.89287     paf loss 17.34514     hm loss 3071.54773\n",
      "Iteration:   2610    step:    16730     combined loss: 3262.46180     paf loss 17.97315     hm loss 3244.48865\n",
      "Iteration:   2615    step:    16735     combined loss: 5379.61522     paf loss 22.04564     hm loss 5357.56958\n",
      "Iteration:   2620    step:    16740     combined loss: 3727.77911     paf loss 18.39812     hm loss 3709.38098\n",
      "Iteration:   2625    step:    16745     combined loss: 4256.05337     paf loss 25.22378     hm loss 4230.82959\n",
      "Iteration:   2630    step:    16750     combined loss: 3094.70796     paf loss 21.25593     hm loss 3073.45203\n",
      "Iteration:   2635    step:    16755     combined loss: 2325.18246     paf loss 17.85799     hm loss 2307.32446\n",
      "Iteration:   2640    step:    16760     combined loss: 3170.23470     paf loss 18.93880     hm loss 3151.29590\n",
      "Iteration:   2645    step:    16765     combined loss: 3870.57832     paf loss 21.61457     hm loss 3848.96375\n",
      "Iteration:   2650    step:    16770     combined loss: 4608.43131     paf loss 20.94303     hm loss 4587.48828\n",
      "Iteration:   2655    step:    16775     combined loss: 3191.89731     paf loss 16.66208     hm loss 3175.23523\n",
      "Iteration:   2660    step:    16780     combined loss: 3293.88328     paf loss 14.32090     hm loss 3279.56238\n",
      "Iteration:   2665    step:    16785     combined loss: 5146.51636     paf loss 23.22046     hm loss 5123.29590\n",
      "Iteration:   2670    step:    16790     combined loss: 5243.99395     paf loss 26.85771     hm loss 5217.13623\n",
      "Iteration:   2675    step:    16795     combined loss: 3411.09003     paf loss 23.76850     hm loss 3387.32153\n",
      "Iteration:   2680    step:    16800     combined loss: 4685.08220     paf loss 22.01603     hm loss 4663.06616\n",
      "Iteration:   2685    step:    16805     combined loss: 2532.03875     paf loss 13.93402     hm loss 2518.10474\n",
      "Iteration:   2690    step:    16810     combined loss: 3808.13265     paf loss 19.08065     hm loss 3789.05200\n",
      "Iteration:   2695    step:    16815     combined loss: 4505.28356     paf loss 20.76622     hm loss 4484.51733\n",
      "Iteration:   2700    step:    16820     combined loss: 3634.39156     paf loss 20.60066     hm loss 3613.79089\n",
      "Iteration:   2705    step:    16825     combined loss: 3410.54102     paf loss 20.55286     hm loss 3389.98816\n",
      "Iteration:   2710    step:    16830     combined loss: 4051.92360     paf loss 17.68911     hm loss 4034.23450\n",
      "Iteration:   2715    step:    16835     combined loss: 3091.80891     paf loss 18.34309     hm loss 3073.46582\n",
      "Iteration:   2720    step:    16840     combined loss: 3397.35532     paf loss 19.12168     hm loss 3378.23364\n",
      "Iteration:   2725    step:    16845     combined loss: 4117.73794     paf loss 24.93606     hm loss 4092.80188\n",
      "Iteration:   2730    step:    16850     combined loss: 4125.34851     paf loss 29.69690     hm loss 4095.65161\n",
      "Iteration:   2735    step:    16855     combined loss: 3179.82508     paf loss 18.95313     hm loss 3160.87195\n",
      "Iteration:   2740    step:    16860     combined loss: 5144.56069     paf loss 18.38271     hm loss 5126.17798\n",
      "Iteration:   2745    step:    16865     combined loss: 3425.38762     paf loss 16.35087     hm loss 3409.03674\n",
      "Iteration:   2750    step:    16870     combined loss: 3434.26197     paf loss 22.44923     hm loss 3411.81274\n",
      "Iteration:   2755    step:    16875     combined loss: 4156.63714     paf loss 25.61566     hm loss 4131.02148\n",
      "Iteration:   2760    step:    16880     combined loss: 4480.44465     paf loss 26.08991     hm loss 4454.35474\n",
      "Iteration:   2765    step:    16885     combined loss: 5180.44600     paf loss 20.80855     hm loss 5159.63745\n",
      "Iteration:   2770    step:    16890     combined loss: 2691.15122     paf loss 18.35899     hm loss 2672.79224\n",
      "Iteration:   2775    step:    16895     combined loss: 5516.73482     paf loss 19.83076     hm loss 5496.90405\n",
      "Iteration:   2780    step:    16900     combined loss: 3855.34814     paf loss 20.73413     hm loss 3834.61401\n",
      "Iteration:   2785    step:    16905     combined loss: 2410.21446     paf loss 12.91734     hm loss 2397.29712\n",
      "Iteration:   2790    step:    16910     combined loss: 3971.17480     paf loss 21.22290     hm loss 3949.95190\n",
      "Iteration:   2795    step:    16915     combined loss: 2655.02107     paf loss 18.95869     hm loss 2636.06238\n",
      "Iteration:   2800    step:    16920     combined loss: 3416.49955     paf loss 12.93022     hm loss 3403.56934\n",
      "Iteration:   2805    step:    16925     combined loss: 3541.22363     paf loss 20.17627     hm loss 3521.04736\n",
      "Iteration:   2810    step:    16930     combined loss: 2504.06148     paf loss 17.97029     hm loss 2486.09119\n",
      "Iteration:   2815    step:    16935     combined loss: 4810.44781     paf loss 20.77838     hm loss 4789.66943\n",
      "Iteration:   2820    step:    16940     combined loss: 4499.11004     paf loss 26.21673     hm loss 4472.89331\n",
      "Train Loss: 3556.9205    PAF Loss:  19.9329    HM Loss:  3536.9876    Acc: NA\n",
      "Val Loss: 3896.6339    PAF Loss:  16.5108    HM Loss:  3880.1231     Acc: NA\n",
      "Epoch 6/9\n",
      "----------\n",
      "Iteration:      0    step:    16944     combined loss: 2619.65453     paf loss 14.22521     hm loss 2605.42932\n",
      "Iteration:      5    step:    16949     combined loss: 4338.92724     paf loss 24.23925     hm loss 4314.68799\n",
      "Iteration:     10    step:    16954     combined loss: 2558.94431     paf loss 15.87119     hm loss 2543.07312\n",
      "Iteration:     15    step:    16959     combined loss: 3153.37154     paf loss 16.92183     hm loss 3136.44971\n",
      "Iteration:     20    step:    16964     combined loss: 4296.68475     paf loss 20.51678     hm loss 4276.16797\n",
      "Iteration:     25    step:    16969     combined loss: 5386.69293     paf loss 22.89288     hm loss 5363.80005\n",
      "Iteration:     30    step:    16974     combined loss: 3150.13837     paf loss 23.12678     hm loss 3127.01160\n",
      "Iteration:     35    step:    16979     combined loss: 3325.65276     paf loss 20.52374     hm loss 3305.12903\n",
      "Iteration:     40    step:    16984     combined loss: 4551.81017     paf loss 19.41442     hm loss 4532.39575\n",
      "Iteration:     45    step:    16989     combined loss: 2704.56910     paf loss 12.71510     hm loss 2691.85400\n",
      "Iteration:     50    step:    16994     combined loss: 3076.09258     paf loss 15.80425     hm loss 3060.28833\n",
      "Iteration:     55    step:    16999     combined loss: 4094.80129     paf loss 24.50771     hm loss 4070.29358\n",
      "Iteration:     60    step:    17004     combined loss: 6490.21231     paf loss 23.57364     hm loss 6466.63867\n",
      "Iteration:     65    step:    17009     combined loss: 2641.54598     paf loss 16.90108     hm loss 2624.64490\n",
      "Iteration:     70    step:    17014     combined loss: 3974.97205     paf loss 21.41578     hm loss 3953.55627\n",
      "Iteration:     75    step:    17019     combined loss: 3191.20769     paf loss 23.25078     hm loss 3167.95691\n",
      "Iteration:     80    step:    17024     combined loss: 2531.13933     paf loss 16.32133     hm loss 2514.81799\n",
      "Iteration:     85    step:    17029     combined loss: 3855.66862     paf loss 22.86967     hm loss 3832.79895\n",
      "Iteration:     90    step:    17034     combined loss: 4603.66954     paf loss 18.39610     hm loss 4585.27344\n",
      "Iteration:     95    step:    17039     combined loss: 5189.83845     paf loss 22.85554     hm loss 5166.98291\n",
      "Iteration:    100    step:    17044     combined loss: 3725.21471     paf loss 23.23778     hm loss 3701.97693\n",
      "Iteration:    105    step:    17049     combined loss: 4303.94444     paf loss 24.49400     hm loss 4279.45044\n",
      "Iteration:    110    step:    17054     combined loss: 2727.99834     paf loss 24.02336     hm loss 2703.97498\n",
      "Iteration:    115    step:    17059     combined loss: 2933.44933     paf loss 18.77587     hm loss 2914.67346\n",
      "Iteration:    120    step:    17064     combined loss: 2794.83652     paf loss 20.44504     hm loss 2774.39148\n",
      "Iteration:    125    step:    17069     combined loss: 2839.74879     paf loss 20.98476     hm loss 2818.76404\n",
      "Iteration:    130    step:    17074     combined loss: 4151.58783     paf loss 21.51019     hm loss 4130.07764\n",
      "Iteration:    135    step:    17079     combined loss: 4070.27831     paf loss 16.65222     hm loss 4053.62610\n",
      "Iteration:    140    step:    17084     combined loss: 4055.85718     paf loss 19.56189     hm loss 4036.29529\n",
      "Iteration:    145    step:    17089     combined loss: 3768.21775     paf loss 22.99180     hm loss 3745.22595\n",
      "Iteration:    150    step:    17094     combined loss: 4753.40692     paf loss 19.45672     hm loss 4733.95020\n",
      "Iteration:    155    step:    17099     combined loss: 2720.58450     paf loss 20.20559     hm loss 2700.37891\n",
      "Iteration:    160    step:    17104     combined loss: 2705.86203     paf loss 18.90841     hm loss 2686.95361\n",
      "Iteration:    165    step:    17109     combined loss: 4239.12943     paf loss 25.47074     hm loss 4213.65869\n",
      "Iteration:    170    step:    17114     combined loss: 4068.64777     paf loss 22.98163     hm loss 4045.66614\n",
      "Iteration:    175    step:    17119     combined loss: 3589.43643     paf loss 20.37319     hm loss 3569.06323\n",
      "Iteration:    180    step:    17124     combined loss: 3005.89351     paf loss 17.73848     hm loss 2988.15503\n",
      "Iteration:    185    step:    17129     combined loss: 2327.49636     paf loss 20.28603     hm loss 2307.21033\n",
      "Iteration:    190    step:    17134     combined loss: 3160.82995     paf loss 17.90979     hm loss 3142.92017\n",
      "Iteration:    195    step:    17139     combined loss: 3996.25546     paf loss 23.12765     hm loss 3973.12781\n",
      "Iteration:    200    step:    17144     combined loss: 3379.10769     paf loss 16.28933     hm loss 3362.81836\n",
      "Iteration:    205    step:    17149     combined loss: 3356.86682     paf loss 22.53320     hm loss 3334.33362\n",
      "Iteration:    210    step:    17154     combined loss: 3047.44064     paf loss 16.78244     hm loss 3030.65820\n",
      "Iteration:    215    step:    17159     combined loss: 2727.26682     paf loss 15.84287     hm loss 2711.42395\n",
      "Iteration:    220    step:    17164     combined loss: 3749.01397     paf loss 17.95013     hm loss 3731.06384\n",
      "Iteration:    225    step:    17169     combined loss: 3258.43052     paf loss 22.64695     hm loss 3235.78357\n",
      "Iteration:    230    step:    17174     combined loss: 2931.93012     paf loss 14.57978     hm loss 2917.35034\n",
      "Iteration:    235    step:    17179     combined loss: 3272.34348     paf loss 19.02427     hm loss 3253.31921\n",
      "Iteration:    240    step:    17184     combined loss: 3255.77048     paf loss 16.66794     hm loss 3239.10254\n",
      "Iteration:    245    step:    17189     combined loss: 1721.84365     paf loss 11.03176     hm loss 1710.81189\n",
      "Iteration:    250    step:    17194     combined loss: 4177.13007     paf loss 21.67792     hm loss 4155.45215\n",
      "Iteration:    255    step:    17199     combined loss: 2921.20577     paf loss 15.95186     hm loss 2905.25391\n",
      "Iteration:    260    step:    17204     combined loss: 3377.31398     paf loss 24.19374     hm loss 3353.12024\n",
      "Iteration:    265    step:    17209     combined loss: 4510.07108     paf loss 19.50614     hm loss 4490.56494\n",
      "Iteration:    270    step:    17214     combined loss: 5054.02900     paf loss 21.71503     hm loss 5032.31396\n",
      "Iteration:    275    step:    17219     combined loss: 3730.99620     paf loss 19.04100     hm loss 3711.95520\n",
      "Iteration:    280    step:    17224     combined loss: 2820.86857     paf loss 23.27628     hm loss 2797.59229\n",
      "Iteration:    285    step:    17229     combined loss: 2731.27069     paf loss 15.62079     hm loss 2715.64990\n",
      "Iteration:    290    step:    17234     combined loss: 3617.57846     paf loss 18.49619     hm loss 3599.08228\n",
      "Iteration:    295    step:    17239     combined loss: 3641.29023     paf loss 21.35822     hm loss 3619.93201\n",
      "Iteration:    300    step:    17244     combined loss: 5248.52049     paf loss 18.73631     hm loss 5229.78418\n",
      "Iteration:    305    step:    17249     combined loss: 4633.55313     paf loss 24.11100     hm loss 4609.44214\n",
      "Iteration:    310    step:    17254     combined loss: 2512.37134     paf loss 17.52112     hm loss 2494.85022\n",
      "Iteration:    315    step:    17259     combined loss: 3159.80070     paf loss 21.27848     hm loss 3138.52222\n",
      "Iteration:    320    step:    17264     combined loss: 3212.73610     paf loss 20.44435     hm loss 3192.29175\n",
      "Iteration:    325    step:    17269     combined loss: 4025.13567     paf loss 19.32586     hm loss 4005.80981\n",
      "Iteration:    330    step:    17274     combined loss: 2204.42667     paf loss 16.83146     hm loss 2187.59521\n",
      "Iteration:    335    step:    17279     combined loss: 4331.29479     paf loss 20.31945     hm loss 4310.97534\n",
      "Iteration:    340    step:    17284     combined loss: 3302.07715     paf loss 27.81531     hm loss 3274.26184\n",
      "Iteration:    345    step:    17289     combined loss: 3586.47633     paf loss 14.40382     hm loss 3572.07251\n",
      "Iteration:    350    step:    17294     combined loss: 2833.94705     paf loss 17.75906     hm loss 2816.18799\n",
      "Iteration:    355    step:    17299     combined loss: 3687.55628     paf loss 30.07581     hm loss 3657.48047\n",
      "Iteration:    360    step:    17304     combined loss: 4397.73231     paf loss 24.87220     hm loss 4372.86011\n",
      "Iteration:    365    step:    17309     combined loss: 2926.78980     paf loss 19.73988     hm loss 2907.04993\n",
      "Iteration:    370    step:    17314     combined loss: 4202.18735     paf loss 16.98716     hm loss 4185.20020\n",
      "Iteration:    375    step:    17319     combined loss: 4419.66575     paf loss 20.05345     hm loss 4399.61230\n",
      "Iteration:    380    step:    17324     combined loss: 3094.60067     paf loss 18.89351     hm loss 3075.70715\n",
      "Iteration:    385    step:    17329     combined loss: 2142.61206     paf loss 16.25122     hm loss 2126.36084\n",
      "Iteration:    390    step:    17334     combined loss: 2750.23490     paf loss 19.13896     hm loss 2731.09595\n",
      "Iteration:    395    step:    17339     combined loss: 2784.36678     paf loss 19.95150     hm loss 2764.41528\n",
      "Iteration:    400    step:    17344     combined loss: 3064.01465     paf loss 18.17834     hm loss 3045.83630\n",
      "Iteration:    405    step:    17349     combined loss: 3093.36443     paf loss 13.12224     hm loss 3080.24219\n",
      "Iteration:    410    step:    17354     combined loss: 3028.85238     paf loss 22.66720     hm loss 3006.18518\n",
      "Iteration:    415    step:    17359     combined loss: 5197.16668     paf loss 17.32561     hm loss 5179.84106\n",
      "Iteration:    420    step:    17364     combined loss: 4590.44498     paf loss 21.01090     hm loss 4569.43408\n",
      "Iteration:    425    step:    17369     combined loss: 4314.42670     paf loss 17.73163     hm loss 4296.69507\n",
      "Iteration:    430    step:    17374     combined loss: 4749.45122     paf loss 24.16240     hm loss 4725.28882\n",
      "Iteration:    435    step:    17379     combined loss: 3130.80189     paf loss 21.33606     hm loss 3109.46582\n",
      "Iteration:    440    step:    17384     combined loss: 3081.31653     paf loss 19.97290     hm loss 3061.34363\n",
      "Iteration:    445    step:    17389     combined loss: 4611.16861     paf loss 27.81876     hm loss 4583.34985\n",
      "Iteration:    450    step:    17394     combined loss: 4141.27720     paf loss 21.48716     hm loss 4119.79004\n",
      "Iteration:    455    step:    17399     combined loss: 2934.80152     paf loss 18.80640     hm loss 2915.99512\n",
      "Iteration:    460    step:    17404     combined loss: 4908.18516     paf loss 23.71396     hm loss 4884.47119\n",
      "Iteration:    465    step:    17409     combined loss: 4122.25707     paf loss 25.72899     hm loss 4096.52808\n",
      "Iteration:    470    step:    17414     combined loss: 3517.28069     paf loss 21.06219     hm loss 3496.21851\n",
      "Iteration:    475    step:    17419     combined loss: 3403.07437     paf loss 20.95254     hm loss 3382.12183\n",
      "Iteration:    480    step:    17424     combined loss: 4207.80961     paf loss 21.38139     hm loss 4186.42822\n",
      "Iteration:    485    step:    17429     combined loss: 3363.96151     paf loss 17.35043     hm loss 3346.61108\n",
      "Iteration:    490    step:    17434     combined loss: 3969.11984     paf loss 19.72165     hm loss 3949.39819\n",
      "Iteration:    495    step:    17439     combined loss: 2318.61102     paf loss 14.81219     hm loss 2303.79883\n",
      "Iteration:    500    step:    17444     combined loss: 3043.03083     paf loss 18.91950     hm loss 3024.11133\n",
      "Iteration:    505    step:    17449     combined loss: 4188.41582     paf loss 22.04424     hm loss 4166.37158\n",
      "Iteration:    510    step:    17454     combined loss: 3628.74124     paf loss 20.67337     hm loss 3608.06787\n",
      "Iteration:    515    step:    17459     combined loss: 3013.41930     paf loss 14.50743     hm loss 2998.91187\n",
      "Iteration:    520    step:    17464     combined loss: 3340.89815     paf loss 19.78157     hm loss 3321.11658\n",
      "Iteration:    525    step:    17469     combined loss: 4454.38108     paf loss 32.21604     hm loss 4422.16504\n",
      "Iteration:    530    step:    17474     combined loss: 3753.98658     paf loss 16.33510     hm loss 3737.65149\n",
      "Iteration:    535    step:    17479     combined loss: 4105.12450     paf loss 17.18468     hm loss 4087.93982\n",
      "Iteration:    540    step:    17484     combined loss: 2037.21596     paf loss 13.96468     hm loss 2023.25128\n",
      "Iteration:    545    step:    17489     combined loss: 3671.63260     paf loss 20.19754     hm loss 3651.43506\n",
      "Iteration:    550    step:    17494     combined loss: 5081.10746     paf loss 19.11479     hm loss 5061.99268\n",
      "Iteration:    555    step:    17499     combined loss: 3226.56588     paf loss 17.59005     hm loss 3208.97583\n",
      "Iteration:    560    step:    17504     combined loss: 4946.51234     paf loss 22.96693     hm loss 4923.54541\n",
      "Iteration:    565    step:    17509     combined loss: 3348.17058     paf loss 23.45354     hm loss 3324.71704\n",
      "Iteration:    570    step:    17514     combined loss: 4088.89211     paf loss 27.54678     hm loss 4061.34534\n",
      "Iteration:    575    step:    17519     combined loss: 3831.59441     paf loss 19.94072     hm loss 3811.65369\n",
      "Iteration:    580    step:    17524     combined loss: 3906.48695     paf loss 22.39454     hm loss 3884.09241\n",
      "Iteration:    585    step:    17529     combined loss: 3593.69791     paf loss 19.75126     hm loss 3573.94666\n",
      "Iteration:    590    step:    17534     combined loss: 3349.45518     paf loss 22.89061     hm loss 3326.56458\n",
      "Iteration:    595    step:    17539     combined loss: 3629.69854     paf loss 19.36663     hm loss 3610.33191\n",
      "Iteration:    600    step:    17544     combined loss: 2988.55324     paf loss 26.42177     hm loss 2962.13147\n",
      "Iteration:    605    step:    17549     combined loss: 3099.19780     paf loss 19.34355     hm loss 3079.85425\n",
      "Iteration:    610    step:    17554     combined loss: 5335.73354     paf loss 25.26186     hm loss 5310.47168\n",
      "Iteration:    615    step:    17559     combined loss: 4201.63814     paf loss 17.06856     hm loss 4184.56958\n",
      "Iteration:    620    step:    17564     combined loss: 2773.60399     paf loss 20.18383     hm loss 2753.42017\n",
      "Iteration:    625    step:    17569     combined loss: 2865.46786     paf loss 19.02829     hm loss 2846.43958\n",
      "Iteration:    630    step:    17574     combined loss: 2977.69043     paf loss 17.58887     hm loss 2960.10156\n",
      "Iteration:    635    step:    17579     combined loss: 2910.98010     paf loss 21.29199     hm loss 2889.68811\n",
      "Iteration:    640    step:    17584     combined loss: 3161.94919     paf loss 23.27329     hm loss 3138.67590\n",
      "Iteration:    645    step:    17589     combined loss: 4385.08673     paf loss 20.44732     hm loss 4364.63940\n",
      "Iteration:    650    step:    17594     combined loss: 4438.31136     paf loss 19.94979     hm loss 4418.36157\n",
      "Iteration:    655    step:    17599     combined loss: 4082.50485     paf loss 19.75595     hm loss 4062.74890\n",
      "Iteration:    660    step:    17604     combined loss: 3514.73852     paf loss 23.06262     hm loss 3491.67590\n",
      "Iteration:    665    step:    17609     combined loss: 6023.58741     paf loss 21.62476     hm loss 6001.96265\n",
      "Iteration:    670    step:    17614     combined loss: 3575.34759     paf loss 26.41424     hm loss 3548.93335\n",
      "Iteration:    675    step:    17619     combined loss: 2659.93865     paf loss 16.18230     hm loss 2643.75635\n",
      "Iteration:    680    step:    17624     combined loss: 2542.96762     paf loss 13.71469     hm loss 2529.25293\n",
      "Iteration:    685    step:    17629     combined loss: 4315.52062     paf loss 21.43029     hm loss 4294.09033\n",
      "Iteration:    690    step:    17634     combined loss: 2763.10521     paf loss 16.60448     hm loss 2746.50073\n",
      "Iteration:    695    step:    17639     combined loss: 2389.01312     paf loss 11.00909     hm loss 2378.00403\n",
      "Iteration:    700    step:    17644     combined loss: 4208.40658     paf loss 22.20907     hm loss 4186.19751\n",
      "Iteration:    705    step:    17649     combined loss: 3714.57457     paf loss 20.16893     hm loss 3694.40564\n",
      "Iteration:    710    step:    17654     combined loss: 3835.66406     paf loss 15.83508     hm loss 3819.82898\n",
      "Iteration:    715    step:    17659     combined loss: 2127.90776     paf loss 16.25663     hm loss 2111.65112\n",
      "Iteration:    720    step:    17664     combined loss: 2850.04197     paf loss 16.22385     hm loss 2833.81812\n",
      "Iteration:    725    step:    17669     combined loss: 3989.26805     paf loss 24.83080     hm loss 3964.43726\n",
      "Iteration:    730    step:    17674     combined loss: 3033.72128     paf loss 25.62790     hm loss 3008.09338\n",
      "Iteration:    735    step:    17679     combined loss: 2647.70503     paf loss 14.59028     hm loss 2633.11475\n",
      "Iteration:    740    step:    17684     combined loss: 3824.76572     paf loss 25.37595     hm loss 3799.38977\n",
      "Iteration:    745    step:    17689     combined loss: 2038.94478     paf loss 13.54622     hm loss 2025.39856\n",
      "Iteration:    750    step:    17694     combined loss: 4156.00582     paf loss 23.23678     hm loss 4132.76904\n",
      "Iteration:    755    step:    17699     combined loss: 3081.85434     paf loss 15.23020     hm loss 3066.62415\n",
      "Iteration:    760    step:    17704     combined loss: 4243.37041     paf loss 19.65922     hm loss 4223.71118\n",
      "Iteration:    765    step:    17709     combined loss: 5585.72319     paf loss 24.33891     hm loss 5561.38428\n",
      "Iteration:    770    step:    17714     combined loss: 2701.70408     paf loss 23.76963     hm loss 2677.93445\n",
      "Iteration:    775    step:    17719     combined loss: 4557.39772     paf loss 19.99391     hm loss 4537.40381\n",
      "Iteration:    780    step:    17724     combined loss: 3453.15750     paf loss 23.83120     hm loss 3429.32629\n",
      "Iteration:    785    step:    17729     combined loss: 2547.60656     paf loss 19.35986     hm loss 2528.24670\n",
      "Iteration:    790    step:    17734     combined loss: 4234.35771     paf loss 24.61821     hm loss 4209.73950\n",
      "Iteration:    795    step:    17739     combined loss: 3054.79628     paf loss 20.06520     hm loss 3034.73108\n",
      "Iteration:    800    step:    17744     combined loss: 3884.88924     paf loss 17.15144     hm loss 3867.73779\n",
      "Iteration:    805    step:    17749     combined loss: 4698.65444     paf loss 23.44204     hm loss 4675.21240\n",
      "Iteration:    810    step:    17754     combined loss: 4091.25676     paf loss 25.54789     hm loss 4065.70886\n",
      "Iteration:    815    step:    17759     combined loss: 4972.58658     paf loss 21.29337     hm loss 4951.29321\n",
      "Iteration:    820    step:    17764     combined loss: 3772.99527     paf loss 20.25040     hm loss 3752.74487\n",
      "Iteration:    825    step:    17769     combined loss: 3687.13991     paf loss 20.38503     hm loss 3666.75488\n",
      "Iteration:    830    step:    17774     combined loss: 3532.83506     paf loss 16.23862     hm loss 3516.59644\n",
      "Iteration:    835    step:    17779     combined loss: 2940.58158     paf loss 20.19168     hm loss 2920.38989\n",
      "Iteration:    840    step:    17784     combined loss: 2807.65160     paf loss 24.43175     hm loss 2783.21985\n",
      "Iteration:    845    step:    17789     combined loss: 5226.52892     paf loss 24.71935     hm loss 5201.80957\n",
      "Iteration:    850    step:    17794     combined loss: 2686.18147     paf loss 13.20564     hm loss 2672.97583\n",
      "Iteration:    855    step:    17799     combined loss: 2540.75698     paf loss 16.84670     hm loss 2523.91028\n",
      "Iteration:    860    step:    17804     combined loss: 4429.58358     paf loss 18.60238     hm loss 4410.98120\n",
      "Iteration:    865    step:    17809     combined loss: 3705.35479     paf loss 13.72625     hm loss 3691.62854\n",
      "Iteration:    870    step:    17814     combined loss: 3896.26733     paf loss 23.98168     hm loss 3872.28564\n",
      "Iteration:    875    step:    17819     combined loss: 2618.64797     paf loss 19.74123     hm loss 2598.90674\n",
      "Iteration:    880    step:    17824     combined loss: 3055.73612     paf loss 22.93802     hm loss 3032.79810\n",
      "Iteration:    885    step:    17829     combined loss: 3291.44480     paf loss 20.37425     hm loss 3271.07056\n",
      "Iteration:    890    step:    17834     combined loss: 3572.47425     paf loss 23.29944     hm loss 3549.17480\n",
      "Iteration:    895    step:    17839     combined loss: 3670.93861     paf loss 15.13368     hm loss 3655.80493\n",
      "Iteration:    900    step:    17844     combined loss: 3824.68228     paf loss 26.72427     hm loss 3797.95801\n",
      "Iteration:    905    step:    17849     combined loss: 3291.25011     paf loss 23.69909     hm loss 3267.55103\n",
      "Iteration:    910    step:    17854     combined loss: 3620.85821     paf loss 18.26629     hm loss 3602.59192\n",
      "Iteration:    915    step:    17859     combined loss: 3533.76774     paf loss 25.03178     hm loss 3508.73596\n",
      "Iteration:    920    step:    17864     combined loss: 5252.18187     paf loss 18.70164     hm loss 5233.48022\n",
      "Iteration:    925    step:    17869     combined loss: 3421.35604     paf loss 20.34249     hm loss 3401.01355\n",
      "Iteration:    930    step:    17874     combined loss: 2462.76862     paf loss 14.69233     hm loss 2448.07629\n",
      "Iteration:    935    step:    17879     combined loss: 1882.82480     paf loss 17.14346     hm loss 1865.68134\n",
      "Iteration:    940    step:    17884     combined loss: 2793.23053     paf loss 21.75446     hm loss 2771.47607\n",
      "Iteration:    945    step:    17889     combined loss: 4421.68256     paf loss 20.43427     hm loss 4401.24829\n",
      "Iteration:    950    step:    17894     combined loss: 3619.95220     paf loss 16.29363     hm loss 3603.65857\n",
      "Iteration:    955    step:    17899     combined loss: 3668.57374     paf loss 15.49708     hm loss 3653.07666\n",
      "Iteration:    960    step:    17904     combined loss: 3190.93793     paf loss 11.52069     hm loss 3179.41724\n",
      "Iteration:    965    step:    17909     combined loss: 3636.03842     paf loss 18.61618     hm loss 3617.42224\n",
      "Iteration:    970    step:    17914     combined loss: 5013.94735     paf loss 20.20296     hm loss 4993.74438\n",
      "Iteration:    975    step:    17919     combined loss: 3880.21255     paf loss 20.62198     hm loss 3859.59058\n",
      "Iteration:    980    step:    17924     combined loss: 3185.19814     paf loss 16.43337     hm loss 3168.76477\n",
      "Iteration:    985    step:    17929     combined loss: 4042.46961     paf loss 21.31873     hm loss 4021.15088\n",
      "Iteration:    990    step:    17934     combined loss: 2814.06125     paf loss 17.05466     hm loss 2797.00659\n",
      "Iteration:    995    step:    17939     combined loss: 4088.69236     paf loss 26.75682     hm loss 4061.93555\n",
      "Iteration:   1000    step:    17944     combined loss: 5111.31521     paf loss 25.39065     hm loss 5085.92456\n",
      "Iteration:   1005    step:    17949     combined loss: 2721.20841     paf loss 16.61088     hm loss 2704.59753\n",
      "Iteration:   1010    step:    17954     combined loss: 3453.75642     paf loss 15.84077     hm loss 3437.91565\n",
      "Iteration:   1015    step:    17959     combined loss: 3754.67069     paf loss 25.27604     hm loss 3729.39465\n",
      "Iteration:   1020    step:    17964     combined loss: 3442.63017     paf loss 17.76995     hm loss 3424.86023\n",
      "Iteration:   1025    step:    17969     combined loss: 4979.93879     paf loss 27.97273     hm loss 4951.96606\n",
      "Iteration:   1030    step:    17974     combined loss: 3511.68271     paf loss 19.90707     hm loss 3491.77563\n",
      "Iteration:   1035    step:    17979     combined loss: 2982.97991     paf loss 16.87701     hm loss 2966.10291\n",
      "Iteration:   1040    step:    17984     combined loss: 3180.03971     paf loss 19.81217     hm loss 3160.22754\n",
      "Iteration:   1045    step:    17989     combined loss: 3366.93317     paf loss 20.30012     hm loss 3346.63306\n",
      "Iteration:   1050    step:    17994     combined loss: 2981.11987     paf loss 19.76575     hm loss 2961.35413\n",
      "Iteration:   1055    step:    17999     combined loss: 7484.96763     paf loss 23.86436     hm loss 7461.10327\n",
      "Iteration:   1060    step:    18004     combined loss: 3329.77857     paf loss 18.64856     hm loss 3311.13000\n",
      "Iteration:   1065    step:    18009     combined loss: 3012.25143     paf loss 14.87911     hm loss 2997.37231\n",
      "Iteration:   1070    step:    18014     combined loss: 6451.81904     paf loss 24.44429     hm loss 6427.37476\n",
      "Iteration:   1075    step:    18019     combined loss: 2509.69417     paf loss 14.62361     hm loss 2495.07056\n",
      "Iteration:   1080    step:    18024     combined loss: 4118.72694     paf loss 26.06544     hm loss 4092.66150\n",
      "Iteration:   1085    step:    18029     combined loss: 4014.32047     paf loss 23.68936     hm loss 3990.63110\n",
      "Iteration:   1090    step:    18034     combined loss: 2152.83212     paf loss 14.19772     hm loss 2138.63440\n",
      "Iteration:   1095    step:    18039     combined loss: 2994.38689     paf loss 15.88934     hm loss 2978.49756\n",
      "Iteration:   1100    step:    18044     combined loss: 3375.88766     paf loss 20.73410     hm loss 3355.15356\n",
      "Iteration:   1105    step:    18049     combined loss: 3341.16584     paf loss 20.40986     hm loss 3320.75598\n",
      "Iteration:   1110    step:    18054     combined loss: 2737.60203     paf loss 21.68333     hm loss 2715.91870\n",
      "Iteration:   1115    step:    18059     combined loss: 3827.59211     paf loss 23.44807     hm loss 3804.14404\n",
      "Iteration:   1120    step:    18064     combined loss: 2929.21743     paf loss 18.25307     hm loss 2910.96436\n",
      "Iteration:   1125    step:    18069     combined loss: 4197.97869     paf loss 20.55950     hm loss 4177.41919\n",
      "Iteration:   1130    step:    18074     combined loss: 4954.91448     paf loss 33.18621     hm loss 4921.72827\n",
      "Iteration:   1135    step:    18079     combined loss: 3909.67743     paf loss 22.08808     hm loss 3887.58936\n",
      "Iteration:   1140    step:    18084     combined loss: 4339.50593     paf loss 22.33601     hm loss 4317.16992\n",
      "Iteration:   1145    step:    18089     combined loss: 3645.58833     paf loss 17.99653     hm loss 3627.59180\n",
      "Iteration:   1150    step:    18094     combined loss: 3676.22102     paf loss 21.71760     hm loss 3654.50342\n",
      "Iteration:   1155    step:    18099     combined loss: 4197.88159     paf loss 22.69409     hm loss 4175.18750\n",
      "Iteration:   1160    step:    18104     combined loss: 2871.34032     paf loss 12.30981     hm loss 2859.03052\n",
      "Iteration:   1165    step:    18109     combined loss: 3486.54885     paf loss 16.32790     hm loss 3470.22095\n",
      "Iteration:   1170    step:    18114     combined loss: 5009.93935     paf loss 25.77138     hm loss 4984.16797\n",
      "Iteration:   1175    step:    18119     combined loss: 3666.17049     paf loss 23.01729     hm loss 3643.15320\n",
      "Iteration:   1180    step:    18124     combined loss: 2508.39769     paf loss 19.00328     hm loss 2489.39441\n",
      "Iteration:   1185    step:    18129     combined loss: 2904.56394     paf loss 16.15097     hm loss 2888.41296\n",
      "Iteration:   1190    step:    18134     combined loss: 3102.40113     paf loss 18.51649     hm loss 3083.88464\n",
      "Iteration:   1195    step:    18139     combined loss: 3203.68135     paf loss 13.35579     hm loss 3190.32556\n",
      "Iteration:   1200    step:    18144     combined loss: 3386.70122     paf loss 21.73869     hm loss 3364.96252\n",
      "Iteration:   1205    step:    18149     combined loss: 3327.43805     paf loss 23.59918     hm loss 3303.83887\n",
      "Iteration:   1210    step:    18154     combined loss: 4042.83716     paf loss 23.94543     hm loss 4018.89172\n",
      "Iteration:   1215    step:    18159     combined loss: 3076.73901     paf loss 21.84094     hm loss 3054.89807\n",
      "Iteration:   1220    step:    18164     combined loss: 3110.64957     paf loss 20.19328     hm loss 3090.45630\n",
      "Iteration:   1225    step:    18169     combined loss: 3231.17375     paf loss 22.86711     hm loss 3208.30664\n",
      "Iteration:   1230    step:    18174     combined loss: 3481.23860     paf loss 22.32319     hm loss 3458.91541\n",
      "Iteration:   1235    step:    18179     combined loss: 3353.91747     paf loss 17.95617     hm loss 3335.96130\n",
      "Iteration:   1240    step:    18184     combined loss: 4876.13554     paf loss 23.26518     hm loss 4852.87036\n",
      "Iteration:   1245    step:    18189     combined loss: 2253.63520     paf loss 17.71028     hm loss 2235.92493\n",
      "Iteration:   1250    step:    18194     combined loss: 3465.28754     paf loss 19.54254     hm loss 3445.74500\n",
      "Iteration:   1255    step:    18199     combined loss: 3206.08076     paf loss 22.07307     hm loss 3184.00769\n",
      "Iteration:   1260    step:    18204     combined loss: 2661.30200     paf loss 20.65076     hm loss 2640.65125\n",
      "Iteration:   1265    step:    18209     combined loss: 5941.29511     paf loss 22.32441     hm loss 5918.97070\n",
      "Iteration:   1270    step:    18214     combined loss: 2553.24945     paf loss 20.93940     hm loss 2532.31006\n",
      "Iteration:   1275    step:    18219     combined loss: 3684.76457     paf loss 19.25578     hm loss 3665.50879\n",
      "Iteration:   1280    step:    18224     combined loss: 2393.75776     paf loss 18.50569     hm loss 2375.25208\n",
      "Iteration:   1285    step:    18229     combined loss: 3830.92225     paf loss 21.01026     hm loss 3809.91199\n",
      "Iteration:   1290    step:    18234     combined loss: 2151.25131     paf loss 14.37436     hm loss 2136.87695\n",
      "Iteration:   1295    step:    18239     combined loss: 2294.42628     paf loss 14.19459     hm loss 2280.23169\n",
      "Iteration:   1300    step:    18244     combined loss: 3365.96693     paf loss 18.39857     hm loss 3347.56836\n",
      "Iteration:   1305    step:    18249     combined loss: 2882.44980     paf loss 26.25180     hm loss 2856.19800\n",
      "Iteration:   1310    step:    18254     combined loss: 2761.46424     paf loss 16.59180     hm loss 2744.87244\n",
      "Iteration:   1315    step:    18259     combined loss: 3888.36728     paf loss 25.02365     hm loss 3863.34363\n",
      "Iteration:   1320    step:    18264     combined loss: 2413.91483     paf loss 15.58829     hm loss 2398.32654\n",
      "Iteration:   1325    step:    18269     combined loss: 3579.31365     paf loss 16.94976     hm loss 3562.36389\n",
      "Iteration:   1330    step:    18274     combined loss: 3422.51803     paf loss 20.98215     hm loss 3401.53589\n",
      "Iteration:   1335    step:    18279     combined loss: 4593.66466     paf loss 28.03356     hm loss 4565.63110\n",
      "Iteration:   1340    step:    18284     combined loss: 3996.54683     paf loss 21.85970     hm loss 3974.68713\n",
      "Iteration:   1345    step:    18289     combined loss: 4336.79358     paf loss 20.19616     hm loss 4316.59741\n",
      "Iteration:   1350    step:    18294     combined loss: 3129.69260     paf loss 21.64316     hm loss 3108.04944\n",
      "Iteration:   1355    step:    18299     combined loss: 4466.84540     paf loss 24.21967     hm loss 4442.62573\n",
      "Iteration:   1360    step:    18304     combined loss: 2420.14275     paf loss 15.01421     hm loss 2405.12854\n",
      "Iteration:   1365    step:    18309     combined loss: 2210.51707     paf loss 16.51939     hm loss 2193.99768\n",
      "Iteration:   1370    step:    18314     combined loss: 3410.43231     paf loss 20.39068     hm loss 3390.04163\n",
      "Iteration:   1375    step:    18319     combined loss: 2805.43583     paf loss 15.31583     hm loss 2790.12000\n",
      "Iteration:   1380    step:    18324     combined loss: 3566.58779     paf loss 23.61050     hm loss 3542.97729\n",
      "Iteration:   1385    step:    18329     combined loss: 4834.36279     paf loss 19.47924     hm loss 4814.88354\n",
      "Iteration:   1390    step:    18334     combined loss: 4737.96352     paf loss 22.64760     hm loss 4715.31592\n",
      "Iteration:   1395    step:    18339     combined loss: 4480.16891     paf loss 19.03268     hm loss 4461.13623\n",
      "Iteration:   1400    step:    18344     combined loss: 4727.42206     paf loss 21.38763     hm loss 4706.03442\n",
      "Iteration:   1405    step:    18349     combined loss: 3622.89031     paf loss 24.77691     hm loss 3598.11340\n",
      "Iteration:   1410    step:    18354     combined loss: 2944.50186     paf loss 15.57376     hm loss 2928.92810\n",
      "Iteration:   1415    step:    18359     combined loss: 3479.33560     paf loss 17.88651     hm loss 3461.44910\n",
      "Iteration:   1420    step:    18364     combined loss: 3118.26051     paf loss 22.90187     hm loss 3095.35864\n",
      "Iteration:   1425    step:    18369     combined loss: 2617.16936     paf loss 18.93999     hm loss 2598.22937\n",
      "Iteration:   1430    step:    18374     combined loss: 2513.39618     paf loss 19.05548     hm loss 2494.34070\n",
      "Iteration:   1435    step:    18379     combined loss: 4146.74300     paf loss 24.10934     hm loss 4122.63367\n",
      "Iteration:   1440    step:    18384     combined loss: 5430.45994     paf loss 14.88011     hm loss 5415.57983\n",
      "Iteration:   1445    step:    18389     combined loss: 2578.42736     paf loss 14.78601     hm loss 2563.64136\n",
      "Iteration:   1450    step:    18394     combined loss: 3053.43404     paf loss 18.59188     hm loss 3034.84216\n",
      "Iteration:   1455    step:    18399     combined loss: 3625.16329     paf loss 21.92232     hm loss 3603.24097\n",
      "Iteration:   1460    step:    18404     combined loss: 3587.65268     paf loss 22.36460     hm loss 3565.28809\n",
      "Iteration:   1465    step:    18409     combined loss: 3496.58840     paf loss 22.43471     hm loss 3474.15369\n",
      "Iteration:   1470    step:    18414     combined loss: 4068.61609     paf loss 18.62134     hm loss 4049.99475\n",
      "Iteration:   1475    step:    18419     combined loss: 4521.95927     paf loss 26.47148     hm loss 4495.48779\n",
      "Iteration:   1480    step:    18424     combined loss: 2009.27939     paf loss 16.37675     hm loss 1992.90265\n",
      "Iteration:   1485    step:    18429     combined loss: 4940.17765     paf loss 22.20231     hm loss 4917.97534\n",
      "Iteration:   1490    step:    18434     combined loss: 3312.63478     paf loss 22.30860     hm loss 3290.32617\n",
      "Iteration:   1495    step:    18439     combined loss: 4988.32127     paf loss 24.53294     hm loss 4963.78833\n",
      "Iteration:   1500    step:    18444     combined loss: 3409.03567     paf loss 15.13650     hm loss 3393.89917\n",
      "Iteration:   1505    step:    18449     combined loss: 3024.79946     paf loss 27.94899     hm loss 2996.85046\n",
      "Iteration:   1510    step:    18454     combined loss: 4360.58604     paf loss 24.16148     hm loss 4336.42456\n",
      "Iteration:   1515    step:    18459     combined loss: 2885.34422     paf loss 20.57481     hm loss 2864.76941\n",
      "Iteration:   1520    step:    18464     combined loss: 3600.88356     paf loss 23.53701     hm loss 3577.34656\n",
      "Iteration:   1525    step:    18469     combined loss: 3479.06278     paf loss 20.45865     hm loss 3458.60413\n",
      "Iteration:   1530    step:    18474     combined loss: 2820.43743     paf loss 22.74334     hm loss 2797.69409\n",
      "Iteration:   1535    step:    18479     combined loss: 2727.82124     paf loss 18.71418     hm loss 2709.10706\n",
      "Iteration:   1540    step:    18484     combined loss: 2395.77412     paf loss 16.10298     hm loss 2379.67114\n",
      "Iteration:   1545    step:    18489     combined loss: 6621.26362     paf loss 23.18451     hm loss 6598.07910\n",
      "Iteration:   1550    step:    18494     combined loss: 3689.06180     paf loss 25.51199     hm loss 3663.54980\n",
      "Iteration:   1555    step:    18499     combined loss: 4076.89458     paf loss 18.61309     hm loss 4058.28149\n",
      "Iteration:   1560    step:    18504     combined loss: 5094.85571     paf loss 24.36328     hm loss 5070.49243\n",
      "Iteration:   1565    step:    18509     combined loss: 3692.58849     paf loss 21.81322     hm loss 3670.77527\n",
      "Iteration:   1570    step:    18514     combined loss: 3120.40463     paf loss 20.96139     hm loss 3099.44324\n",
      "Iteration:   1575    step:    18519     combined loss: 3778.59072     paf loss 21.82632     hm loss 3756.76440\n",
      "Iteration:   1580    step:    18524     combined loss: 3498.09782     paf loss 22.12882     hm loss 3475.96899\n",
      "Iteration:   1585    step:    18529     combined loss: 2695.71978     paf loss 12.71514     hm loss 2683.00464\n",
      "Iteration:   1590    step:    18534     combined loss: 4298.58990     paf loss 20.47198     hm loss 4278.11792\n",
      "Iteration:   1595    step:    18539     combined loss: 6078.16157     paf loss 27.02436     hm loss 6051.13721\n",
      "Iteration:   1600    step:    18544     combined loss: 3040.96288     paf loss 19.80907     hm loss 3021.15381\n",
      "Iteration:   1605    step:    18549     combined loss: 4047.14976     paf loss 24.10472     hm loss 4023.04504\n",
      "Iteration:   1610    step:    18554     combined loss: 4080.94961     paf loss 16.34109     hm loss 4064.60852\n",
      "Iteration:   1615    step:    18559     combined loss: 4364.04775     paf loss 20.27090     hm loss 4343.77686\n",
      "Iteration:   1620    step:    18564     combined loss: 3583.94236     paf loss 19.11936     hm loss 3564.82300\n",
      "Iteration:   1625    step:    18569     combined loss: 4744.22145     paf loss 21.68897     hm loss 4722.53247\n",
      "Iteration:   1630    step:    18574     combined loss: 2690.71860     paf loss 14.89707     hm loss 2675.82153\n",
      "Iteration:   1635    step:    18579     combined loss: 4154.58239     paf loss 18.56432     hm loss 4136.01807\n",
      "Iteration:   1640    step:    18584     combined loss: 2831.40636     paf loss 18.56065     hm loss 2812.84570\n",
      "Iteration:   1645    step:    18589     combined loss: 3123.25683     paf loss 20.35608     hm loss 3102.90076\n",
      "Iteration:   1650    step:    18594     combined loss: 2775.63894     paf loss 19.57644     hm loss 2756.06250\n",
      "Iteration:   1655    step:    18599     combined loss: 5395.78944     paf loss 21.61977     hm loss 5374.16968\n",
      "Iteration:   1660    step:    18604     combined loss: 3200.29155     paf loss 18.17106     hm loss 3182.12048\n",
      "Iteration:   1665    step:    18609     combined loss: 3813.24086     paf loss 25.73916     hm loss 3787.50171\n",
      "Iteration:   1670    step:    18614     combined loss: 3675.40417     paf loss 19.37963     hm loss 3656.02454\n",
      "Iteration:   1675    step:    18619     combined loss: 3042.68013     paf loss 20.71590     hm loss 3021.96423\n",
      "Iteration:   1680    step:    18624     combined loss: 3346.64877     paf loss 16.82736     hm loss 3329.82141\n",
      "Iteration:   1685    step:    18629     combined loss: 2522.88108     paf loss 21.02574     hm loss 2501.85535\n",
      "Iteration:   1690    step:    18634     combined loss: 3327.10130     paf loss 19.46824     hm loss 3307.63306\n",
      "Iteration:   1695    step:    18639     combined loss: 5029.53594     paf loss 22.18389     hm loss 5007.35205\n",
      "Iteration:   1700    step:    18644     combined loss: 2933.78369     paf loss 19.41797     hm loss 2914.36572\n",
      "Iteration:   1705    step:    18649     combined loss: 2804.60281     paf loss 18.29080     hm loss 2786.31201\n",
      "Iteration:   1710    step:    18654     combined loss: 4748.81509     paf loss 22.78701     hm loss 4726.02808\n",
      "Iteration:   1715    step:    18659     combined loss: 4581.55394     paf loss 19.83885     hm loss 4561.71509\n",
      "Iteration:   1720    step:    18664     combined loss: 3267.82117     paf loss 23.29077     hm loss 3244.53040\n",
      "Iteration:   1725    step:    18669     combined loss: 3687.15789     paf loss 22.28961     hm loss 3664.86829\n",
      "Iteration:   1730    step:    18674     combined loss: 2697.36922     paf loss 19.25301     hm loss 2678.11621\n",
      "Iteration:   1735    step:    18679     combined loss: 2737.92929     paf loss 13.99948     hm loss 2723.92981\n",
      "Iteration:   1740    step:    18684     combined loss: 1909.32881     paf loss 17.19582     hm loss 1892.13300\n",
      "Iteration:   1745    step:    18689     combined loss: 5057.47651     paf loss 22.32929     hm loss 5035.14722\n",
      "Iteration:   1750    step:    18694     combined loss: 3615.86931     paf loss 20.07158     hm loss 3595.79773\n",
      "Iteration:   1755    step:    18699     combined loss: 4449.03424     paf loss 23.30280     hm loss 4425.73145\n",
      "Iteration:   1760    step:    18704     combined loss: 3199.70656     paf loss 18.75808     hm loss 3180.94849\n",
      "Iteration:   1765    step:    18709     combined loss: 2629.09124     paf loss 16.19329     hm loss 2612.89795\n",
      "Iteration:   1770    step:    18714     combined loss: 2651.47375     paf loss 17.04650     hm loss 2634.42725\n",
      "Iteration:   1775    step:    18719     combined loss: 2611.23715     paf loss 20.82760     hm loss 2590.40955\n",
      "Iteration:   1780    step:    18724     combined loss: 4271.82600     paf loss 21.99836     hm loss 4249.82764\n",
      "Iteration:   1785    step:    18729     combined loss: 2976.01639     paf loss 18.57035     hm loss 2957.44604\n",
      "Iteration:   1790    step:    18734     combined loss: 3836.89440     paf loss 19.20128     hm loss 3817.69312\n",
      "Iteration:   1795    step:    18739     combined loss: 3070.27169     paf loss 17.19820     hm loss 3053.07349\n",
      "Iteration:   1800    step:    18744     combined loss: 3420.86083     paf loss 20.75317     hm loss 3400.10767\n",
      "Iteration:   1805    step:    18749     combined loss: 2778.16070     paf loss 14.97467     hm loss 2763.18604\n",
      "Iteration:   1810    step:    18754     combined loss: 3262.93674     paf loss 20.31613     hm loss 3242.62061\n",
      "Iteration:   1815    step:    18759     combined loss: 5052.40193     paf loss 19.42415     hm loss 5032.97778\n",
      "Iteration:   1820    step:    18764     combined loss: 5182.59258     paf loss 20.44390     hm loss 5162.14868\n",
      "Iteration:   1825    step:    18769     combined loss: 2657.85299     paf loss 20.10775     hm loss 2637.74524\n",
      "Iteration:   1830    step:    18774     combined loss: 2008.65611     paf loss 15.45372     hm loss 1993.20239\n",
      "Iteration:   1835    step:    18779     combined loss: 3311.62766     paf loss 19.77903     hm loss 3291.84863\n",
      "Iteration:   1840    step:    18784     combined loss: 3437.92674     paf loss 16.47545     hm loss 3421.45129\n",
      "Iteration:   1845    step:    18789     combined loss: 4155.91370     paf loss 24.31617     hm loss 4131.59753\n",
      "Iteration:   1850    step:    18794     combined loss: 4132.61798     paf loss 22.13116     hm loss 4110.48682\n",
      "Iteration:   1855    step:    18799     combined loss: 3268.43440     paf loss 16.42524     hm loss 3252.00916\n",
      "Iteration:   1860    step:    18804     combined loss: 2465.76085     paf loss 16.89549     hm loss 2448.86536\n",
      "Iteration:   1865    step:    18809     combined loss: 4203.37084     paf loss 21.25145     hm loss 4182.11938\n",
      "Iteration:   1870    step:    18814     combined loss: 3722.40644     paf loss 21.15607     hm loss 3701.25037\n",
      "Iteration:   1875    step:    18819     combined loss: 2687.66763     paf loss 18.87027     hm loss 2668.79736\n",
      "Iteration:   1880    step:    18824     combined loss: 4004.01307     paf loss 21.84388     hm loss 3982.16919\n",
      "Iteration:   1885    step:    18829     combined loss: 4151.04885     paf loss 21.02786     hm loss 4130.02100\n",
      "Iteration:   1890    step:    18834     combined loss: 4702.21191     paf loss 22.77489     hm loss 4679.43701\n",
      "Iteration:   1895    step:    18839     combined loss: 3048.26670     paf loss 23.39060     hm loss 3024.87610\n",
      "Iteration:   1900    step:    18844     combined loss: 2942.53890     paf loss 13.87032     hm loss 2928.66858\n",
      "Iteration:   1905    step:    18849     combined loss: 3639.47333     paf loss 16.96625     hm loss 3622.50708\n",
      "Iteration:   1910    step:    18854     combined loss: 2312.92306     paf loss 12.76828     hm loss 2300.15479\n",
      "Iteration:   1915    step:    18859     combined loss: 3934.39994     paf loss 20.72940     hm loss 3913.67053\n",
      "Iteration:   1920    step:    18864     combined loss: 2306.20288     paf loss 15.76319     hm loss 2290.43970\n",
      "Iteration:   1925    step:    18869     combined loss: 4102.24509     paf loss 22.66281     hm loss 4079.58228\n",
      "Iteration:   1930    step:    18874     combined loss: 3664.06455     paf loss 19.57248     hm loss 3644.49207\n",
      "Iteration:   1935    step:    18879     combined loss: 5040.83905     paf loss 18.39032     hm loss 5022.44873\n",
      "Iteration:   1940    step:    18884     combined loss: 3367.93269     paf loss 16.65681     hm loss 3351.27588\n",
      "Iteration:   1945    step:    18889     combined loss: 2664.09603     paf loss 19.85237     hm loss 2644.24365\n",
      "Iteration:   1950    step:    18894     combined loss: 3274.59709     paf loss 19.04899     hm loss 3255.54810\n",
      "Iteration:   1955    step:    18899     combined loss: 4490.73381     paf loss 21.40740     hm loss 4469.32642\n",
      "Iteration:   1960    step:    18904     combined loss: 4163.70002     paf loss 29.61713     hm loss 4134.08289\n",
      "Iteration:   1965    step:    18909     combined loss: 3325.96722     paf loss 18.98968     hm loss 3306.97754\n",
      "Iteration:   1970    step:    18914     combined loss: 3021.53119     paf loss 21.48077     hm loss 3000.05042\n",
      "Iteration:   1975    step:    18919     combined loss: 2438.81774     paf loss 17.23680     hm loss 2421.58093\n",
      "Iteration:   1980    step:    18924     combined loss: 3949.09664     paf loss 22.01705     hm loss 3927.07959\n",
      "Iteration:   1985    step:    18929     combined loss: 2497.61345     paf loss 15.76067     hm loss 2481.85278\n",
      "Iteration:   1990    step:    18934     combined loss: 3473.21215     paf loss 20.24633     hm loss 3452.96582\n",
      "Iteration:   1995    step:    18939     combined loss: 4147.55737     paf loss 26.50683     hm loss 4121.05054\n",
      "Iteration:   2000    step:    18944     combined loss: 3637.33563     paf loss 19.70562     hm loss 3617.63000\n",
      "Iteration:   2005    step:    18949     combined loss: 3196.70256     paf loss 16.98491     hm loss 3179.71765\n",
      "Iteration:   2010    step:    18954     combined loss: 2631.55470     paf loss 18.62636     hm loss 2612.92834\n",
      "Iteration:   2015    step:    18959     combined loss: 4450.65801     paf loss 25.07110     hm loss 4425.58691\n",
      "Iteration:   2020    step:    18964     combined loss: 3865.08627     paf loss 24.76876     hm loss 3840.31750\n",
      "Iteration:   2025    step:    18969     combined loss: 5546.20656     paf loss 21.88942     hm loss 5524.31714\n",
      "Iteration:   2030    step:    18974     combined loss: 3367.85631     paf loss 19.02074     hm loss 3348.83557\n",
      "Iteration:   2035    step:    18979     combined loss: 2816.86689     paf loss 18.44928     hm loss 2798.41760\n",
      "Iteration:   2040    step:    18984     combined loss: 4016.08560     paf loss 23.58316     hm loss 3992.50244\n",
      "Iteration:   2045    step:    18989     combined loss: 5370.42901     paf loss 22.94854     hm loss 5347.48047\n",
      "Iteration:   2050    step:    18994     combined loss: 2966.32849     paf loss 13.46546     hm loss 2952.86304\n",
      "Iteration:   2055    step:    18999     combined loss: 3691.89340     paf loss 23.09579     hm loss 3668.79761\n",
      "Iteration:   2060    step:    19004     combined loss: 3439.42408     paf loss 23.07679     hm loss 3416.34729\n",
      "Iteration:   2065    step:    19009     combined loss: 2239.17950     paf loss 16.45905     hm loss 2222.72046\n",
      "Iteration:   2070    step:    19014     combined loss: 5615.78534     paf loss 29.42719     hm loss 5586.35815\n",
      "Iteration:   2075    step:    19019     combined loss: 4446.00496     paf loss 27.50985     hm loss 4418.49512\n",
      "Iteration:   2080    step:    19024     combined loss: 3147.14225     paf loss 15.86906     hm loss 3131.27319\n",
      "Iteration:   2085    step:    19029     combined loss: 4956.65542     paf loss 23.55411     hm loss 4933.10132\n",
      "Iteration:   2090    step:    19034     combined loss: 2637.18600     paf loss 18.76535     hm loss 2618.42065\n",
      "Iteration:   2095    step:    19039     combined loss: 3393.74906     paf loss 15.32499     hm loss 3378.42407\n",
      "Iteration:   2100    step:    19044     combined loss: 4030.14373     paf loss 28.40813     hm loss 4001.73560\n",
      "Iteration:   2105    step:    19049     combined loss: 3276.63784     paf loss 18.66115     hm loss 3257.97668\n",
      "Iteration:   2110    step:    19054     combined loss: 4256.90282     paf loss 21.80077     hm loss 4235.10205\n",
      "Iteration:   2115    step:    19059     combined loss: 4292.11371     paf loss 22.22699     hm loss 4269.88672\n",
      "Iteration:   2120    step:    19064     combined loss: 3530.97547     paf loss 21.21778     hm loss 3509.75769\n",
      "Iteration:   2125    step:    19069     combined loss: 2939.72803     paf loss 17.31226     hm loss 2922.41577\n",
      "Iteration:   2130    step:    19074     combined loss: 3052.60016     paf loss 13.04523     hm loss 3039.55493\n",
      "Iteration:   2135    step:    19079     combined loss: 2887.80788     paf loss 20.73170     hm loss 2867.07617\n",
      "Iteration:   2140    step:    19084     combined loss: 3863.41625     paf loss 20.17943     hm loss 3843.23682\n",
      "Iteration:   2145    step:    19089     combined loss: 4192.47981     paf loss 21.86092     hm loss 4170.61890\n",
      "Iteration:   2150    step:    19094     combined loss: 3632.51352     paf loss 20.13608     hm loss 3612.37744\n",
      "Iteration:   2155    step:    19099     combined loss: 3580.52832     paf loss 13.96216     hm loss 3566.56616\n",
      "Iteration:   2160    step:    19104     combined loss: 4109.91712     paf loss 24.54200     hm loss 4085.37512\n",
      "Iteration:   2165    step:    19109     combined loss: 3122.39984     paf loss 20.48223     hm loss 3101.91760\n",
      "Iteration:   2170    step:    19114     combined loss: 2559.01064     paf loss 18.80947     hm loss 2540.20117\n",
      "Iteration:   2175    step:    19119     combined loss: 2682.45763     paf loss 17.77660     hm loss 2664.68103\n",
      "Iteration:   2180    step:    19124     combined loss: 3495.64926     paf loss 17.90158     hm loss 3477.74768\n",
      "Iteration:   2185    step:    19129     combined loss: 3187.05674     paf loss 21.03269     hm loss 3166.02405\n",
      "Iteration:   2190    step:    19134     combined loss: 4098.54044     paf loss 21.77714     hm loss 4076.76331\n",
      "Iteration:   2195    step:    19139     combined loss: 3780.75341     paf loss 27.95092     hm loss 3752.80249\n",
      "Iteration:   2200    step:    19144     combined loss: 2947.62584     paf loss 20.53148     hm loss 2927.09436\n",
      "Iteration:   2205    step:    19149     combined loss: 3275.44486     paf loss 16.81412     hm loss 3258.63074\n",
      "Iteration:   2210    step:    19154     combined loss: 5223.61746     paf loss 23.15139     hm loss 5200.46606\n",
      "Iteration:   2215    step:    19159     combined loss: 4308.47665     paf loss 19.24863     hm loss 4289.22803\n",
      "Iteration:   2220    step:    19164     combined loss: 2404.60561     paf loss 19.16274     hm loss 2385.44287\n",
      "Iteration:   2225    step:    19169     combined loss: 4068.54915     paf loss 22.42891     hm loss 4046.12024\n",
      "Iteration:   2230    step:    19174     combined loss: 3227.85472     paf loss 12.56981     hm loss 3215.28491\n",
      "Iteration:   2235    step:    19179     combined loss: 2906.11710     paf loss 19.88517     hm loss 2886.23193\n",
      "Iteration:   2240    step:    19184     combined loss: 2204.92605     paf loss 13.98428     hm loss 2190.94177\n",
      "Iteration:   2245    step:    19189     combined loss: 2746.60929     paf loss 15.22379     hm loss 2731.38550\n",
      "learning rate change: 1.5625e-05 --> 7.8125e-06\n",
      "Iteration:   2250    step:    19194     combined loss: 4922.06335     paf loss 24.47351     hm loss 4897.58984\n",
      "Iteration:   2255    step:    19199     combined loss: 2745.87236     paf loss 13.26018     hm loss 2732.61218\n",
      "Iteration:   2260    step:    19204     combined loss: 3336.22445     paf loss 17.37935     hm loss 3318.84509\n",
      "Iteration:   2265    step:    19209     combined loss: 2512.61997     paf loss 12.08481     hm loss 2500.53516\n",
      "Iteration:   2270    step:    19214     combined loss: 3947.83722     paf loss 27.73297     hm loss 3920.10425\n",
      "Iteration:   2275    step:    19219     combined loss: 2966.60316     paf loss 11.99061     hm loss 2954.61255\n",
      "Iteration:   2280    step:    19224     combined loss: 3144.02669     paf loss 19.29952     hm loss 3124.72717\n",
      "Iteration:   2285    step:    19229     combined loss: 4349.40553     paf loss 20.99098     hm loss 4328.41455\n",
      "Iteration:   2290    step:    19234     combined loss: 2687.63505     paf loss 21.59794     hm loss 2666.03711\n",
      "Iteration:   2295    step:    19239     combined loss: 3846.75644     paf loss 19.02927     hm loss 3827.72717\n",
      "Iteration:   2300    step:    19244     combined loss: 3849.60055     paf loss 23.50998     hm loss 3826.09058\n",
      "Iteration:   2305    step:    19249     combined loss: 3106.88421     paf loss 19.33343     hm loss 3087.55078\n",
      "Iteration:   2310    step:    19254     combined loss: 4340.11528     paf loss 17.98124     hm loss 4322.13403\n",
      "Iteration:   2315    step:    19259     combined loss: 4321.10938     paf loss 21.40113     hm loss 4299.70825\n",
      "Iteration:   2320    step:    19264     combined loss: 2872.64438     paf loss 16.69687     hm loss 2855.94751\n",
      "Iteration:   2325    step:    19269     combined loss: 3137.00910     paf loss 15.92914     hm loss 3121.07996\n",
      "Iteration:   2330    step:    19274     combined loss: 3365.37451     paf loss 17.55396     hm loss 3347.82056\n",
      "Iteration:   2335    step:    19279     combined loss: 2963.37747     paf loss 17.71939     hm loss 2945.65808\n",
      "Iteration:   2340    step:    19284     combined loss: 2684.62040     paf loss 13.15605     hm loss 2671.46436\n",
      "Iteration:   2345    step:    19289     combined loss: 4026.54429     paf loss 23.07004     hm loss 4003.47424\n",
      "Iteration:   2350    step:    19294     combined loss: 3940.06929     paf loss 21.34736     hm loss 3918.72192\n",
      "Iteration:   2355    step:    19299     combined loss: 2463.18531     paf loss 15.94752     hm loss 2447.23779\n",
      "Iteration:   2360    step:    19304     combined loss: 2902.98514     paf loss 16.96805     hm loss 2886.01709\n",
      "Iteration:   2365    step:    19309     combined loss: 4019.72869     paf loss 25.46501     hm loss 3994.26367\n",
      "Iteration:   2370    step:    19314     combined loss: 2886.79703     paf loss 16.50162     hm loss 2870.29541\n",
      "Iteration:   2375    step:    19319     combined loss: 2789.90224     paf loss 15.57839     hm loss 2774.32385\n",
      "Iteration:   2380    step:    19324     combined loss: 3245.73802     paf loss 22.59495     hm loss 3223.14307\n",
      "Iteration:   2385    step:    19329     combined loss: 3198.26589     paf loss 15.19290     hm loss 3183.07300\n",
      "Iteration:   2390    step:    19334     combined loss: 3097.75939     paf loss 17.69824     hm loss 3080.06116\n",
      "Iteration:   2395    step:    19339     combined loss: 4543.10083     paf loss 27.21313     hm loss 4515.88770\n",
      "Iteration:   2400    step:    19344     combined loss: 3159.48857     paf loss 21.55509     hm loss 3137.93347\n",
      "Iteration:   2405    step:    19349     combined loss: 4592.35918     paf loss 22.19048     hm loss 4570.16870\n",
      "Iteration:   2410    step:    19354     combined loss: 3629.70895     paf loss 20.80550     hm loss 3608.90344\n",
      "Iteration:   2415    step:    19359     combined loss: 4112.89092     paf loss 22.99444     hm loss 4089.89648\n",
      "Iteration:   2420    step:    19364     combined loss: 3866.47228     paf loss 22.24255     hm loss 3844.22974\n",
      "Iteration:   2425    step:    19369     combined loss: 2705.13469     paf loss 18.10393     hm loss 2687.03076\n",
      "Iteration:   2430    step:    19374     combined loss: 3202.36474     paf loss 17.82898     hm loss 3184.53577\n",
      "Iteration:   2435    step:    19379     combined loss: 3480.82373     paf loss 19.06641     hm loss 3461.75732\n",
      "Iteration:   2440    step:    19384     combined loss: 4316.52448     paf loss 23.01959     hm loss 4293.50488\n",
      "Iteration:   2445    step:    19389     combined loss: 4176.64997     paf loss 20.75690     hm loss 4155.89307\n",
      "Iteration:   2450    step:    19394     combined loss: 4210.42958     paf loss 23.24501     hm loss 4187.18457\n",
      "Iteration:   2455    step:    19399     combined loss: 4305.16506     paf loss 18.86623     hm loss 4286.29883\n",
      "Iteration:   2460    step:    19404     combined loss: 4832.83125     paf loss 20.27045     hm loss 4812.56079\n",
      "Iteration:   2465    step:    19409     combined loss: 2807.04168     paf loss 20.06524     hm loss 2786.97644\n",
      "Iteration:   2470    step:    19414     combined loss: 3553.38356     paf loss 14.26600     hm loss 3539.11755\n",
      "Iteration:   2475    step:    19419     combined loss: 2485.36357     paf loss 14.59441     hm loss 2470.76917\n",
      "Iteration:   2480    step:    19424     combined loss: 6173.71444     paf loss 31.91635     hm loss 6141.79810\n",
      "Iteration:   2485    step:    19429     combined loss: 3193.49755     paf loss 18.31359     hm loss 3175.18396\n",
      "Iteration:   2490    step:    19434     combined loss: 3312.82570     paf loss 22.12392     hm loss 3290.70178\n",
      "Iteration:   2495    step:    19439     combined loss: 3253.56096     paf loss 18.79485     hm loss 3234.76611\n",
      "Iteration:   2500    step:    19444     combined loss: 3528.13508     paf loss 18.69441     hm loss 3509.44067\n",
      "Iteration:   2505    step:    19449     combined loss: 2287.63725     paf loss 22.17228     hm loss 2265.46497\n",
      "Iteration:   2510    step:    19454     combined loss: 2536.41890     paf loss 17.99776     hm loss 2518.42114\n",
      "Iteration:   2515    step:    19459     combined loss: 3645.96588     paf loss 16.36187     hm loss 3629.60400\n",
      "Iteration:   2520    step:    19464     combined loss: 2829.42256     paf loss 14.74349     hm loss 2814.67908\n",
      "Iteration:   2525    step:    19469     combined loss: 2535.37761     paf loss 23.18474     hm loss 2512.19287\n",
      "Iteration:   2530    step:    19474     combined loss: 3377.51261     paf loss 16.83231     hm loss 3360.68030\n",
      "Iteration:   2535    step:    19479     combined loss: 3103.28638     paf loss 20.57910     hm loss 3082.70728\n",
      "Iteration:   2540    step:    19484     combined loss: 2735.12619     paf loss 12.51120     hm loss 2722.61499\n",
      "Iteration:   2545    step:    19489     combined loss: 3091.57979     paf loss 12.10786     hm loss 3079.47192\n",
      "Iteration:   2550    step:    19494     combined loss: 2451.65016     paf loss 15.81557     hm loss 2435.83459\n",
      "Iteration:   2555    step:    19499     combined loss: 2639.87644     paf loss 16.21556     hm loss 2623.66089\n",
      "Iteration:   2560    step:    19504     combined loss: 3146.60347     paf loss 17.45564     hm loss 3129.14783\n",
      "Iteration:   2565    step:    19509     combined loss: 3738.33537     paf loss 19.99382     hm loss 3718.34155\n",
      "Iteration:   2570    step:    19514     combined loss: 2906.04901     paf loss 18.77326     hm loss 2887.27576\n",
      "Iteration:   2575    step:    19519     combined loss: 3816.71014     paf loss 25.70940     hm loss 3791.00073\n",
      "Iteration:   2580    step:    19524     combined loss: 3691.20472     paf loss 25.69069     hm loss 3665.51404\n",
      "Iteration:   2585    step:    19529     combined loss: 3235.43411     paf loss 19.54825     hm loss 3215.88586\n",
      "Iteration:   2590    step:    19534     combined loss: 3458.81746     paf loss 16.08553     hm loss 3442.73193\n",
      "Iteration:   2595    step:    19539     combined loss: 2422.07984     paf loss 18.83704     hm loss 2403.24280\n",
      "Iteration:   2600    step:    19544     combined loss: 2787.25115     paf loss 14.35577     hm loss 2772.89539\n",
      "Iteration:   2605    step:    19549     combined loss: 3629.02058     paf loss 20.12178     hm loss 3608.89880\n",
      "Iteration:   2610    step:    19554     combined loss: 3777.76113     paf loss 17.51686     hm loss 3760.24426\n",
      "Iteration:   2615    step:    19559     combined loss: 4913.58257     paf loss 23.58599     hm loss 4889.99658\n",
      "Iteration:   2620    step:    19564     combined loss: 1819.58643     paf loss 11.90955     hm loss 1807.67688\n",
      "Iteration:   2625    step:    19569     combined loss: 2994.71522     paf loss 16.15418     hm loss 2978.56104\n",
      "Iteration:   2630    step:    19574     combined loss: 4453.75432     paf loss 24.78947     hm loss 4428.96484\n",
      "Iteration:   2635    step:    19579     combined loss: 2726.43256     paf loss 13.56305     hm loss 2712.86951\n",
      "Iteration:   2640    step:    19584     combined loss: 2886.37431     paf loss 22.22563     hm loss 2864.14868\n",
      "Iteration:   2645    step:    19589     combined loss: 3044.85012     paf loss 22.02017     hm loss 3022.82996\n",
      "Iteration:   2650    step:    19594     combined loss: 4184.88411     paf loss 20.33015     hm loss 4164.55396\n",
      "Iteration:   2655    step:    19599     combined loss: 2806.93296     paf loss 21.85447     hm loss 2785.07849\n",
      "Iteration:   2660    step:    19604     combined loss: 4302.83902     paf loss 20.02701     hm loss 4282.81201\n",
      "Iteration:   2665    step:    19609     combined loss: 3087.00895     paf loss 20.43314     hm loss 3066.57581\n",
      "Iteration:   2670    step:    19614     combined loss: 3283.60460     paf loss 22.35350     hm loss 3261.25110\n",
      "Iteration:   2675    step:    19619     combined loss: 3870.12308     paf loss 17.74978     hm loss 3852.37329\n",
      "Iteration:   2680    step:    19624     combined loss: 5495.72194     paf loss 21.32960     hm loss 5474.39233\n",
      "Iteration:   2685    step:    19629     combined loss: 4583.58303     paf loss 20.61721     hm loss 4562.96582\n",
      "Iteration:   2690    step:    19634     combined loss: 2490.37016     paf loss 17.90861     hm loss 2472.46155\n",
      "Iteration:   2695    step:    19639     combined loss: 4878.61981     paf loss 24.76606     hm loss 4853.85376\n",
      "Iteration:   2700    step:    19644     combined loss: 4264.95337     paf loss 25.74975     hm loss 4239.20361\n",
      "Iteration:   2705    step:    19649     combined loss: 3165.10778     paf loss 12.78063     hm loss 3152.32715\n",
      "Iteration:   2710    step:    19654     combined loss: 2944.72895     paf loss 16.90192     hm loss 2927.82703\n",
      "Iteration:   2715    step:    19659     combined loss: 3585.17175     paf loss 21.87463     hm loss 3563.29712\n",
      "Iteration:   2720    step:    19664     combined loss: 3550.64394     paf loss 19.42641     hm loss 3531.21753\n",
      "Iteration:   2725    step:    19669     combined loss: 3920.10096     paf loss 18.06458     hm loss 3902.03638\n",
      "Iteration:   2730    step:    19674     combined loss: 4662.48182     paf loss 17.81654     hm loss 4644.66528\n",
      "Iteration:   2735    step:    19679     combined loss: 3487.12029     paf loss 20.64665     hm loss 3466.47363\n",
      "Iteration:   2740    step:    19684     combined loss: 3825.11337     paf loss 20.00913     hm loss 3805.10425\n",
      "Iteration:   2745    step:    19689     combined loss: 6054.61019     paf loss 28.87459     hm loss 6025.73560\n",
      "Iteration:   2750    step:    19694     combined loss: 3071.43358     paf loss 14.99230     hm loss 3056.44128\n",
      "Iteration:   2755    step:    19699     combined loss: 3265.30198     paf loss 16.40135     hm loss 3248.90063\n",
      "Iteration:   2760    step:    19704     combined loss: 1922.48542     paf loss 17.10474     hm loss 1905.38068\n",
      "Iteration:   2765    step:    19709     combined loss: 3400.12720     paf loss 22.37634     hm loss 3377.75085\n",
      "Iteration:   2770    step:    19714     combined loss: 5337.46220     paf loss 21.50224     hm loss 5315.95996\n",
      "Iteration:   2775    step:    19719     combined loss: 4445.00613     paf loss 29.63943     hm loss 4415.36670\n",
      "Iteration:   2780    step:    19724     combined loss: 3132.02529     paf loss 19.69838     hm loss 3112.32690\n",
      "Iteration:   2785    step:    19729     combined loss: 2147.78318     paf loss 18.23448     hm loss 2129.54871\n",
      "Iteration:   2790    step:    19734     combined loss: 3349.98347     paf loss 17.50849     hm loss 3332.47498\n",
      "Iteration:   2795    step:    19739     combined loss: 2630.19750     paf loss 16.45776     hm loss 2613.73975\n",
      "Iteration:   2800    step:    19744     combined loss: 2461.59649     paf loss 18.73858     hm loss 2442.85791\n",
      "Iteration:   2805    step:    19749     combined loss: 4531.89309     paf loss 18.69338     hm loss 4513.19971\n",
      "Iteration:   2810    step:    19754     combined loss: 5085.91247     paf loss 21.79431     hm loss 5064.11816\n",
      "Iteration:   2815    step:    19759     combined loss: 3500.51144     paf loss 21.20431     hm loss 3479.30713\n",
      "Iteration:   2820    step:    19764     combined loss: 4636.09652     paf loss 20.45370     hm loss 4615.64282\n",
      "Train Loss: 3554.2557    PAF Loss:  19.9195    HM Loss:  3534.3363    Acc: NA\n",
      "Val Loss: 3850.2126    PAF Loss:  16.3643    HM Loss:  3833.8484     Acc: NA\n",
      "Epoch 7/9\n",
      "----------\n",
      "Iteration:      0    step:    19768     combined loss: 3447.63201     paf loss 17.23943     hm loss 3430.39258\n",
      "Iteration:      5    step:    19773     combined loss: 3316.87816     paf loss 18.97606     hm loss 3297.90210\n",
      "Iteration:     10    step:    19778     combined loss: 4223.84744     paf loss 24.23831     hm loss 4199.60913\n",
      "Iteration:     15    step:    19783     combined loss: 4659.87745     paf loss 26.30885     hm loss 4633.56860\n",
      "Iteration:     20    step:    19788     combined loss: 3101.84817     paf loss 20.92190     hm loss 3080.92627\n",
      "Iteration:     25    step:    19793     combined loss: 3742.28114     paf loss 18.61866     hm loss 3723.66248\n",
      "Iteration:     30    step:    19798     combined loss: 3349.57768     paf loss 22.84196     hm loss 3326.73572\n",
      "Iteration:     35    step:    19803     combined loss: 2574.30632     paf loss 21.11455     hm loss 2553.19177\n",
      "Iteration:     40    step:    19808     combined loss: 4353.22603     paf loss 20.00118     hm loss 4333.22485\n",
      "Iteration:     45    step:    19813     combined loss: 3335.78784     paf loss 16.75965     hm loss 3319.02820\n",
      "Iteration:     50    step:    19818     combined loss: 3757.41786     paf loss 20.21999     hm loss 3737.19788\n",
      "Iteration:     55    step:    19823     combined loss: 4405.26494     paf loss 20.66313     hm loss 4384.60181\n",
      "Iteration:     60    step:    19828     combined loss: 4554.06067     paf loss 26.18396     hm loss 4527.87671\n",
      "Iteration:     65    step:    19833     combined loss: 4415.90165     paf loss 28.44877     hm loss 4387.45288\n",
      "Iteration:     70    step:    19838     combined loss: 4085.82088     paf loss 22.77364     hm loss 4063.04724\n",
      "Iteration:     75    step:    19843     combined loss: 3162.17671     paf loss 23.88606     hm loss 3138.29065\n",
      "Iteration:     80    step:    19848     combined loss: 2271.73776     paf loss 13.26083     hm loss 2258.47693\n",
      "Iteration:     85    step:    19853     combined loss: 3516.65566     paf loss 18.60488     hm loss 3498.05078\n",
      "Iteration:     90    step:    19858     combined loss: 4114.14125     paf loss 19.63307     hm loss 4094.50818\n",
      "Iteration:     95    step:    19863     combined loss: 7583.83355     paf loss 25.52813     hm loss 7558.30542\n",
      "Iteration:    100    step:    19868     combined loss: 2584.96619     paf loss 16.15467     hm loss 2568.81152\n",
      "Iteration:    105    step:    19873     combined loss: 3137.39630     paf loss 20.48272     hm loss 3116.91357\n",
      "Iteration:    110    step:    19878     combined loss: 3445.28848     paf loss 21.36538     hm loss 3423.92310\n",
      "Iteration:    115    step:    19883     combined loss: 3571.03977     paf loss 21.00791     hm loss 3550.03186\n",
      "Iteration:    120    step:    19888     combined loss: 3295.57268     paf loss 24.06145     hm loss 3271.51123\n",
      "Iteration:    125    step:    19893     combined loss: 4268.06775     paf loss 24.00256     hm loss 4244.06519\n",
      "Iteration:    130    step:    19898     combined loss: 2476.44602     paf loss 17.75388     hm loss 2458.69214\n",
      "Iteration:    135    step:    19903     combined loss: 4264.79362     paf loss 22.69841     hm loss 4242.09521\n",
      "Iteration:    140    step:    19908     combined loss: 2790.88001     paf loss 15.53199     hm loss 2775.34802\n",
      "Iteration:    145    step:    19913     combined loss: 3381.92802     paf loss 17.94353     hm loss 3363.98450\n",
      "Iteration:    150    step:    19918     combined loss: 4130.39372     paf loss 21.35515     hm loss 4109.03857\n",
      "Iteration:    155    step:    19923     combined loss: 4175.02086     paf loss 21.47545     hm loss 4153.54541\n",
      "Iteration:    160    step:    19928     combined loss: 3873.18273     paf loss 22.08032     hm loss 3851.10242\n",
      "Iteration:    165    step:    19933     combined loss: 3097.39165     paf loss 18.20196     hm loss 3079.18970\n",
      "Iteration:    170    step:    19938     combined loss: 3973.55041     paf loss 20.72118     hm loss 3952.82922\n",
      "Iteration:    175    step:    19943     combined loss: 5427.04610     paf loss 18.23751     hm loss 5408.80859\n",
      "Iteration:    180    step:    19948     combined loss: 3240.48950     paf loss 22.99964     hm loss 3217.48987\n",
      "Iteration:    185    step:    19953     combined loss: 3192.56456     paf loss 13.65172     hm loss 3178.91284\n",
      "Iteration:    190    step:    19958     combined loss: 2790.77780     paf loss 18.42099     hm loss 2772.35681\n",
      "Iteration:    195    step:    19963     combined loss: 4252.07063     paf loss 27.36921     hm loss 4224.70142\n",
      "Iteration:    200    step:    19968     combined loss: 2792.72066     paf loss 18.61007     hm loss 2774.11060\n",
      "Iteration:    205    step:    19973     combined loss: 4791.15747     paf loss 24.27441     hm loss 4766.88306\n",
      "Iteration:    210    step:    19978     combined loss: 3918.29876     paf loss 24.47284     hm loss 3893.82593\n",
      "Iteration:    215    step:    19983     combined loss: 3070.39207     paf loss 24.13304     hm loss 3046.25903\n",
      "Iteration:    220    step:    19988     combined loss: 3150.63331     paf loss 23.84412     hm loss 3126.78918\n",
      "Iteration:    225    step:    19993     combined loss: 3638.73635     paf loss 16.18032     hm loss 3622.55603\n",
      "Iteration:    230    step:    19998     combined loss: 4748.76902     paf loss 17.43821     hm loss 4731.33081\n",
      "Iteration:    235    step:    20003     combined loss: 3854.88166     paf loss 20.78302     hm loss 3834.09863\n",
      "Iteration:    240    step:    20008     combined loss: 3622.57177     paf loss 26.77783     hm loss 3595.79395\n",
      "Iteration:    245    step:    20013     combined loss: 5117.37688     paf loss 18.11297     hm loss 5099.26392\n",
      "Iteration:    250    step:    20018     combined loss: 3339.77512     paf loss 23.44138     hm loss 3316.33374\n",
      "Iteration:    255    step:    20023     combined loss: 4105.34565     paf loss 20.88386     hm loss 4084.46179\n",
      "Iteration:    260    step:    20028     combined loss: 5554.63520     paf loss 24.80952     hm loss 5529.82568\n",
      "Iteration:    265    step:    20033     combined loss: 3779.76697     paf loss 21.78772     hm loss 3757.97925\n",
      "Iteration:    270    step:    20038     combined loss: 3847.24012     paf loss 17.32948     hm loss 3829.91064\n",
      "Iteration:    275    step:    20043     combined loss: 3248.16061     paf loss 20.82309     hm loss 3227.33752\n",
      "Iteration:    280    step:    20048     combined loss: 2446.07218     paf loss 16.93632     hm loss 2429.13586\n",
      "Iteration:    285    step:    20053     combined loss: 4391.99155     paf loss 21.08897     hm loss 4370.90259\n",
      "Iteration:    290    step:    20058     combined loss: 4030.02683     paf loss 22.89170     hm loss 4007.13513\n",
      "Iteration:    295    step:    20063     combined loss: 3232.10016     paf loss 18.77704     hm loss 3213.32312\n",
      "Iteration:    300    step:    20068     combined loss: 3951.95373     paf loss 19.72070     hm loss 3932.23303\n",
      "Iteration:    305    step:    20073     combined loss: 2524.81055     paf loss 19.23059     hm loss 2505.57996\n",
      "Iteration:    310    step:    20078     combined loss: 4322.49503     paf loss 23.08903     hm loss 4299.40601\n",
      "Iteration:    315    step:    20083     combined loss: 2971.86561     paf loss 15.77431     hm loss 2956.09131\n",
      "Iteration:    320    step:    20088     combined loss: 2718.33667     paf loss 17.57581     hm loss 2700.76086\n",
      "Iteration:    325    step:    20093     combined loss: 2666.95251     paf loss 15.22204     hm loss 2651.73047\n",
      "Iteration:    330    step:    20098     combined loss: 2998.20235     paf loss 17.58943     hm loss 2980.61292\n",
      "Iteration:    335    step:    20103     combined loss: 4476.63920     paf loss 23.67509     hm loss 4452.96411\n",
      "Iteration:    340    step:    20108     combined loss: 3318.83793     paf loss 18.69841     hm loss 3300.13953\n",
      "Iteration:    345    step:    20113     combined loss: 4688.51172     paf loss 24.99316     hm loss 4663.51855\n",
      "Iteration:    350    step:    20118     combined loss: 3225.96531     paf loss 25.52207     hm loss 3200.44324\n",
      "Iteration:    355    step:    20123     combined loss: 2976.29672     paf loss 18.08176     hm loss 2958.21497\n",
      "Iteration:    360    step:    20128     combined loss: 3751.89646     paf loss 19.82517     hm loss 3732.07129\n",
      "Iteration:    365    step:    20133     combined loss: 2957.52554     paf loss 18.34610     hm loss 2939.17944\n",
      "Iteration:    370    step:    20138     combined loss: 2635.56438     paf loss 17.90337     hm loss 2617.66101\n",
      "Iteration:    375    step:    20143     combined loss: 4993.54100     paf loss 22.88572     hm loss 4970.65527\n",
      "Iteration:    380    step:    20148     combined loss: 3187.56647     paf loss 24.89667     hm loss 3162.66980\n",
      "Iteration:    385    step:    20153     combined loss: 4729.77110     paf loss 23.23740     hm loss 4706.53369\n",
      "Iteration:    390    step:    20158     combined loss: 3010.97959     paf loss 13.36167     hm loss 2997.61792\n",
      "Iteration:    395    step:    20163     combined loss: 3534.39635     paf loss 20.06322     hm loss 3514.33313\n",
      "Iteration:    400    step:    20168     combined loss: 3672.45014     paf loss 19.94489     hm loss 3652.50525\n",
      "Iteration:    405    step:    20173     combined loss: 3245.44952     paf loss 17.75763     hm loss 3227.69189\n",
      "Iteration:    410    step:    20178     combined loss: 4540.37458     paf loss 21.59406     hm loss 4518.78052\n",
      "Iteration:    415    step:    20183     combined loss: 4106.64648     paf loss 17.51501     hm loss 4089.13147\n",
      "Iteration:    420    step:    20188     combined loss: 2646.69426     paf loss 13.35430     hm loss 2633.33997\n",
      "Iteration:    425    step:    20193     combined loss: 4226.19338     paf loss 20.77370     hm loss 4205.41968\n",
      "Iteration:    430    step:    20198     combined loss: 3997.35637     paf loss 26.60295     hm loss 3970.75342\n",
      "Iteration:    435    step:    20203     combined loss: 3014.29856     paf loss 19.50279     hm loss 2994.79578\n",
      "Iteration:    440    step:    20208     combined loss: 2809.44465     paf loss 17.99507     hm loss 2791.44958\n",
      "Iteration:    445    step:    20213     combined loss: 4000.32720     paf loss 25.31109     hm loss 3975.01611\n",
      "Iteration:    450    step:    20218     combined loss: 3114.06103     paf loss 18.57812     hm loss 3095.48291\n",
      "Iteration:    455    step:    20223     combined loss: 4992.60061     paf loss 22.70290     hm loss 4969.89771\n",
      "Iteration:    460    step:    20228     combined loss: 3128.64077     paf loss 18.84109     hm loss 3109.79968\n",
      "Iteration:    465    step:    20233     combined loss: 2581.46577     paf loss 19.01961     hm loss 2562.44617\n",
      "Iteration:    470    step:    20238     combined loss: 3400.40361     paf loss 19.73137     hm loss 3380.67224\n",
      "Iteration:    475    step:    20243     combined loss: 2907.93314     paf loss 15.28775     hm loss 2892.64539\n",
      "Iteration:    480    step:    20248     combined loss: 2339.06791     paf loss 18.78239     hm loss 2320.28552\n",
      "Iteration:    485    step:    20253     combined loss: 2881.17949     paf loss 16.94475     hm loss 2864.23474\n",
      "Iteration:    490    step:    20258     combined loss: 2255.08051     paf loss 19.46185     hm loss 2235.61865\n",
      "Iteration:    495    step:    20263     combined loss: 3758.82396     paf loss 19.29356     hm loss 3739.53040\n",
      "Iteration:    500    step:    20268     combined loss: 3618.84113     paf loss 20.31403     hm loss 3598.52710\n",
      "Iteration:    505    step:    20273     combined loss: 2200.86044     paf loss 14.68356     hm loss 2186.17688\n",
      "Iteration:    510    step:    20278     combined loss: 8418.72235     paf loss 26.02997     hm loss 8392.69238\n",
      "Iteration:    515    step:    20283     combined loss: 2851.52905     paf loss 19.34778     hm loss 2832.18127\n",
      "Iteration:    520    step:    20288     combined loss: 3751.00456     paf loss 18.85820     hm loss 3732.14636\n",
      "Iteration:    525    step:    20293     combined loss: 3213.75555     paf loss 21.76348     hm loss 3191.99207\n",
      "Iteration:    530    step:    20298     combined loss: 3424.79024     paf loss 19.10042     hm loss 3405.68982\n",
      "Iteration:    535    step:    20303     combined loss: 4506.92249     paf loss 21.15052     hm loss 4485.77197\n",
      "Iteration:    540    step:    20308     combined loss: 2838.99724     paf loss 16.22832     hm loss 2822.76892\n",
      "Iteration:    545    step:    20313     combined loss: 3572.31499     paf loss 16.61736     hm loss 3555.69763\n",
      "Iteration:    550    step:    20318     combined loss: 2269.64471     paf loss 14.39288     hm loss 2255.25183\n",
      "Iteration:    555    step:    20323     combined loss: 3883.20658     paf loss 23.14322     hm loss 3860.06335\n",
      "Iteration:    560    step:    20328     combined loss: 4354.19562     paf loss 22.28400     hm loss 4331.91162\n",
      "Iteration:    565    step:    20333     combined loss: 3308.47769     paf loss 23.94498     hm loss 3284.53271\n",
      "Iteration:    570    step:    20338     combined loss: 4185.99939     paf loss 22.04944     hm loss 4163.94995\n",
      "Iteration:    575    step:    20343     combined loss: 2732.82647     paf loss 17.58807     hm loss 2715.23840\n",
      "Iteration:    580    step:    20348     combined loss: 2742.90947     paf loss 20.22747     hm loss 2722.68201\n",
      "Iteration:    585    step:    20353     combined loss: 4028.61517     paf loss 21.98992     hm loss 4006.62524\n",
      "Iteration:    590    step:    20358     combined loss: 2810.16070     paf loss 19.13482     hm loss 2791.02588\n",
      "Iteration:    595    step:    20363     combined loss: 4506.98403     paf loss 21.43325     hm loss 4485.55078\n",
      "Iteration:    600    step:    20368     combined loss: 3409.18724     paf loss 17.60485     hm loss 3391.58240\n",
      "Iteration:    605    step:    20373     combined loss: 3188.13913     paf loss 15.42868     hm loss 3172.71045\n",
      "Iteration:    610    step:    20378     combined loss: 2481.35246     paf loss 12.86931     hm loss 2468.48315\n",
      "Iteration:    615    step:    20383     combined loss: 3166.78560     paf loss 19.25654     hm loss 3147.52905\n",
      "Iteration:    620    step:    20388     combined loss: 3307.08565     paf loss 23.72359     hm loss 3283.36206\n",
      "Iteration:    625    step:    20393     combined loss: 3510.63014     paf loss 23.52919     hm loss 3487.10095\n",
      "Iteration:    630    step:    20398     combined loss: 2314.49244     paf loss 17.70569     hm loss 2296.78674\n",
      "Iteration:    635    step:    20403     combined loss: 2558.26528     paf loss 17.15786     hm loss 2541.10742\n",
      "Iteration:    640    step:    20408     combined loss: 3912.54270     paf loss 17.80271     hm loss 3894.73999\n",
      "Iteration:    645    step:    20413     combined loss: 2994.73229     paf loss 17.12597     hm loss 2977.60632\n",
      "Iteration:    650    step:    20418     combined loss: 5579.59587     paf loss 24.60612     hm loss 5554.98975\n",
      "Iteration:    655    step:    20423     combined loss: 2720.66339     paf loss 17.81488     hm loss 2702.84851\n",
      "Iteration:    660    step:    20428     combined loss: 4326.53926     paf loss 22.23335     hm loss 4304.30591\n",
      "Iteration:    665    step:    20433     combined loss: 4581.55702     paf loss 25.08608     hm loss 4556.47095\n",
      "Iteration:    670    step:    20438     combined loss: 3491.75926     paf loss 17.90673     hm loss 3473.85254\n",
      "Iteration:    675    step:    20443     combined loss: 3679.23322     paf loss 20.88239     hm loss 3658.35083\n",
      "Iteration:    680    step:    20448     combined loss: 4154.81759     paf loss 23.59151     hm loss 4131.22607\n",
      "Iteration:    685    step:    20453     combined loss: 3700.47945     paf loss 19.71481     hm loss 3680.76465\n",
      "Iteration:    690    step:    20458     combined loss: 4215.10396     paf loss 19.91842     hm loss 4195.18555\n",
      "Iteration:    695    step:    20463     combined loss: 2421.10997     paf loss 17.81359     hm loss 2403.29639\n",
      "Iteration:    700    step:    20468     combined loss: 2675.19124     paf loss 17.80172     hm loss 2657.38953\n",
      "Iteration:    705    step:    20473     combined loss: 3285.14460     paf loss 21.46882     hm loss 3263.67578\n",
      "Iteration:    710    step:    20478     combined loss: 2968.05859     paf loss 16.85851     hm loss 2951.20007\n",
      "Iteration:    715    step:    20483     combined loss: 2697.09166     paf loss 16.14610     hm loss 2680.94556\n",
      "Iteration:    720    step:    20488     combined loss: 5292.96981     paf loss 23.49251     hm loss 5269.47729\n",
      "Iteration:    725    step:    20493     combined loss: 4632.91346     paf loss 18.73939     hm loss 4614.17407\n",
      "Iteration:    730    step:    20498     combined loss: 2682.12474     paf loss 18.01829     hm loss 2664.10645\n",
      "Iteration:    735    step:    20503     combined loss: 3982.76762     paf loss 21.84257     hm loss 3960.92505\n",
      "Iteration:    740    step:    20508     combined loss: 3738.58631     paf loss 19.78357     hm loss 3718.80273\n",
      "Iteration:    745    step:    20513     combined loss: 3324.48972     paf loss 18.07773     hm loss 3306.41199\n",
      "Iteration:    750    step:    20518     combined loss: 3752.59487     paf loss 22.71987     hm loss 3729.87500\n",
      "Iteration:    755    step:    20523     combined loss: 3822.59118     paf loss 21.53198     hm loss 3801.05920\n",
      "Iteration:    760    step:    20528     combined loss: 4499.70404     paf loss 22.31488     hm loss 4477.38916\n",
      "Iteration:    765    step:    20533     combined loss: 3918.49330     paf loss 22.02626     hm loss 3896.46704\n",
      "Iteration:    770    step:    20538     combined loss: 4910.80473     paf loss 23.26811     hm loss 4887.53662\n",
      "Iteration:    775    step:    20543     combined loss: 3365.08938     paf loss 20.65224     hm loss 3344.43713\n",
      "Iteration:    780    step:    20548     combined loss: 3645.87656     paf loss 18.26938     hm loss 3627.60718\n",
      "Iteration:    785    step:    20553     combined loss: 2980.61629     paf loss 16.77400     hm loss 2963.84229\n",
      "Iteration:    790    step:    20558     combined loss: 2320.43799     paf loss 13.78028     hm loss 2306.65771\n",
      "Iteration:    795    step:    20563     combined loss: 3844.41031     paf loss 16.83829     hm loss 3827.57202\n",
      "Iteration:    800    step:    20568     combined loss: 3584.61910     paf loss 18.47726     hm loss 3566.14185\n",
      "Iteration:    805    step:    20573     combined loss: 3816.61841     paf loss 16.99866     hm loss 3799.61975\n",
      "Iteration:    810    step:    20578     combined loss: 3570.53000     paf loss 18.49386     hm loss 3552.03613\n",
      "Iteration:    815    step:    20583     combined loss: 4015.52428     paf loss 17.90734     hm loss 3997.61694\n",
      "Iteration:    820    step:    20588     combined loss: 3040.50857     paf loss 19.50271     hm loss 3021.00586\n",
      "Iteration:    825    step:    20593     combined loss: 2114.56644     paf loss 17.27408     hm loss 2097.29236\n",
      "Iteration:    830    step:    20598     combined loss: 4795.34804     paf loss 28.38124     hm loss 4766.96680\n",
      "Iteration:    835    step:    20603     combined loss: 3601.67024     paf loss 18.82344     hm loss 3582.84680\n",
      "Iteration:    840    step:    20608     combined loss: 2976.51859     paf loss 16.41349     hm loss 2960.10510\n",
      "Iteration:    845    step:    20613     combined loss: 2998.94004     paf loss 18.81394     hm loss 2980.12610\n",
      "Iteration:    850    step:    20618     combined loss: 3791.48740     paf loss 25.22642     hm loss 3766.26099\n",
      "Iteration:    855    step:    20623     combined loss: 3377.53795     paf loss 20.35705     hm loss 3357.18091\n",
      "Iteration:    860    step:    20628     combined loss: 3990.13325     paf loss 21.47200     hm loss 3968.66125\n",
      "Iteration:    865    step:    20633     combined loss: 2674.45847     paf loss 20.43516     hm loss 2654.02332\n",
      "Iteration:    870    step:    20638     combined loss: 6279.70807     paf loss 23.98810     hm loss 6255.71997\n",
      "Iteration:    875    step:    20643     combined loss: 2494.94297     paf loss 19.00962     hm loss 2475.93335\n",
      "Iteration:    880    step:    20648     combined loss: 2390.33227     paf loss 15.97534     hm loss 2374.35693\n",
      "Iteration:    885    step:    20653     combined loss: 2999.55665     paf loss 17.50294     hm loss 2982.05371\n",
      "Iteration:    890    step:    20658     combined loss: 4587.14247     paf loss 23.21742     hm loss 4563.92505\n",
      "Iteration:    895    step:    20663     combined loss: 2955.93254     paf loss 18.53874     hm loss 2937.39380\n",
      "Iteration:    900    step:    20668     combined loss: 4862.60395     paf loss 21.21332     hm loss 4841.39062\n",
      "Iteration:    905    step:    20673     combined loss: 3166.39879     paf loss 16.98864     hm loss 3149.41016\n",
      "Iteration:    910    step:    20678     combined loss: 5435.90923     paf loss 27.38530     hm loss 5408.52393\n",
      "Iteration:    915    step:    20683     combined loss: 3546.99052     paf loss 21.96952     hm loss 3525.02100\n",
      "Iteration:    920    step:    20688     combined loss: 3224.83123     paf loss 17.71051     hm loss 3207.12073\n",
      "Iteration:    925    step:    20693     combined loss: 2926.31804     paf loss 17.18523     hm loss 2909.13281\n",
      "Iteration:    930    step:    20698     combined loss: 2776.36672     paf loss 15.10647     hm loss 2761.26025\n",
      "Iteration:    935    step:    20703     combined loss: 2297.80926     paf loss 13.47039     hm loss 2284.33887\n",
      "Iteration:    940    step:    20708     combined loss: 3434.04894     paf loss 19.88195     hm loss 3414.16699\n",
      "Iteration:    945    step:    20713     combined loss: 2258.31016     paf loss 16.17039     hm loss 2242.13977\n",
      "Iteration:    950    step:    20718     combined loss: 4007.72637     paf loss 26.22600     hm loss 3981.50037\n",
      "Iteration:    955    step:    20723     combined loss: 4971.77817     paf loss 28.09262     hm loss 4943.68555\n",
      "Iteration:    960    step:    20728     combined loss: 3581.33941     paf loss 16.97210     hm loss 3564.36731\n",
      "Iteration:    965    step:    20733     combined loss: 4946.21965     paf loss 26.04508     hm loss 4920.17456\n",
      "Iteration:    970    step:    20738     combined loss: 2793.40020     paf loss 22.05022     hm loss 2771.34998\n",
      "Iteration:    975    step:    20743     combined loss: 3064.43640     paf loss 25.93481     hm loss 3038.50159\n",
      "Iteration:    980    step:    20748     combined loss: 5560.57285     paf loss 19.45908     hm loss 5541.11377\n",
      "Iteration:    985    step:    20753     combined loss: 2966.74550     paf loss 18.49599     hm loss 2948.24951\n",
      "Iteration:    990    step:    20758     combined loss: 2766.82103     paf loss 19.79576     hm loss 2747.02527\n",
      "Iteration:    995    step:    20763     combined loss: 3604.53072     paf loss 28.56551     hm loss 3575.96521\n",
      "Iteration:   1000    step:    20768     combined loss: 4337.98909     paf loss 23.46077     hm loss 4314.52832\n",
      "Iteration:   1005    step:    20773     combined loss: 3920.90396     paf loss 16.90055     hm loss 3904.00342\n",
      "Iteration:   1010    step:    20778     combined loss: 5485.86219     paf loss 23.06580     hm loss 5462.79639\n",
      "Iteration:   1015    step:    20783     combined loss: 4047.72761     paf loss 23.57185     hm loss 4024.15576\n",
      "Iteration:   1020    step:    20788     combined loss: 3379.41529     paf loss 17.69776     hm loss 3361.71753\n",
      "Iteration:   1025    step:    20793     combined loss: 2132.78802     paf loss 18.63775     hm loss 2114.15027\n",
      "Iteration:   1030    step:    20798     combined loss: 3701.13078     paf loss 17.72380     hm loss 3683.40698\n",
      "Iteration:   1035    step:    20803     combined loss: 3022.42086     paf loss 18.95088     hm loss 3003.46997\n",
      "Iteration:   1040    step:    20808     combined loss: 2928.92900     paf loss 17.71965     hm loss 2911.20935\n",
      "Iteration:   1045    step:    20813     combined loss: 2770.03089     paf loss 15.38905     hm loss 2754.64185\n",
      "Iteration:   1050    step:    20818     combined loss: 3055.73573     paf loss 21.19801     hm loss 3034.53772\n",
      "Iteration:   1055    step:    20823     combined loss: 2580.73764     paf loss 18.34946     hm loss 2562.38818\n",
      "Iteration:   1060    step:    20828     combined loss: 3627.29273     paf loss 20.77466     hm loss 3606.51807\n",
      "Iteration:   1065    step:    20833     combined loss: 5398.18050     paf loss 26.30696     hm loss 5371.87354\n",
      "Iteration:   1070    step:    20838     combined loss: 5961.45749     paf loss 28.38596     hm loss 5933.07153\n",
      "Iteration:   1075    step:    20843     combined loss: 2305.42494     paf loss 19.09938     hm loss 2286.32556\n",
      "Iteration:   1080    step:    20848     combined loss: 3994.15745     paf loss 18.08176     hm loss 3976.07568\n",
      "Iteration:   1085    step:    20853     combined loss: 2348.53421     paf loss 19.71732     hm loss 2328.81689\n",
      "Iteration:   1090    step:    20858     combined loss: 4842.31159     paf loss 24.98542     hm loss 4817.32617\n",
      "Iteration:   1095    step:    20863     combined loss: 2768.07578     paf loss 19.59397     hm loss 2748.48181\n",
      "Iteration:   1100    step:    20868     combined loss: 4354.74413     paf loss 22.79393     hm loss 4331.95020\n",
      "Iteration:   1105    step:    20873     combined loss: 2552.59170     paf loss 16.17178     hm loss 2536.41992\n",
      "Iteration:   1110    step:    20878     combined loss: 3460.07751     paf loss 18.53088     hm loss 3441.54663\n",
      "Iteration:   1115    step:    20883     combined loss: 1823.67348     paf loss 14.02639     hm loss 1809.64709\n",
      "Iteration:   1120    step:    20888     combined loss: 5085.06073     paf loss 23.31562     hm loss 5061.74512\n",
      "Iteration:   1125    step:    20893     combined loss: 2668.17616     paf loss 17.64405     hm loss 2650.53210\n",
      "Iteration:   1130    step:    20898     combined loss: 3509.12763     paf loss 17.16560     hm loss 3491.96204\n",
      "Iteration:   1135    step:    20903     combined loss: 3512.00894     paf loss 22.25613     hm loss 3489.75281\n",
      "Iteration:   1140    step:    20908     combined loss: 4984.50650     paf loss 22.81485     hm loss 4961.69165\n",
      "Iteration:   1145    step:    20913     combined loss: 3214.44418     paf loss 21.00326     hm loss 3193.44092\n",
      "Iteration:   1150    step:    20918     combined loss: 3753.34869     paf loss 20.92034     hm loss 3732.42834\n",
      "Iteration:   1155    step:    20923     combined loss: 3550.91700     paf loss 19.28052     hm loss 3531.63647\n",
      "Iteration:   1160    step:    20928     combined loss: 3519.75804     paf loss 21.97227     hm loss 3497.78577\n",
      "Iteration:   1165    step:    20933     combined loss: 2692.69282     paf loss 18.79463     hm loss 2673.89819\n",
      "Iteration:   1170    step:    20938     combined loss: 2300.15932     paf loss 18.59157     hm loss 2281.56775\n",
      "Iteration:   1175    step:    20943     combined loss: 2992.78940     paf loss 25.30344     hm loss 2967.48596\n",
      "Iteration:   1180    step:    20948     combined loss: 2302.78340     paf loss 18.92012     hm loss 2283.86328\n",
      "Iteration:   1185    step:    20953     combined loss: 3229.65681     paf loss 16.76374     hm loss 3212.89307\n",
      "Iteration:   1190    step:    20958     combined loss: 2760.03141     paf loss 15.97489     hm loss 2744.05652\n",
      "Iteration:   1195    step:    20963     combined loss: 3107.22563     paf loss 18.44231     hm loss 3088.78333\n",
      "Iteration:   1200    step:    20968     combined loss: 3228.28606     paf loss 20.80290     hm loss 3207.48315\n",
      "Iteration:   1205    step:    20973     combined loss: 3142.99531     paf loss 18.12250     hm loss 3124.87280\n",
      "Iteration:   1210    step:    20978     combined loss: 2701.34325     paf loss 20.25646     hm loss 2681.08679\n",
      "Iteration:   1215    step:    20983     combined loss: 3215.41038     paf loss 15.86851     hm loss 3199.54187\n",
      "Iteration:   1220    step:    20988     combined loss: 3125.66841     paf loss 17.11408     hm loss 3108.55432\n",
      "Iteration:   1225    step:    20993     combined loss: 3455.39493     paf loss 17.85196     hm loss 3437.54297\n",
      "Iteration:   1230    step:    20998     combined loss: 4096.08746     paf loss 23.05938     hm loss 4073.02808\n",
      "Iteration:   1235    step:    21003     combined loss: 3142.86467     paf loss 17.40068     hm loss 3125.46399\n",
      "Iteration:   1240    step:    21008     combined loss: 4867.54055     paf loss 20.11330     hm loss 4847.42725\n",
      "Iteration:   1245    step:    21013     combined loss: 4340.91319     paf loss 23.80064     hm loss 4317.11255\n",
      "Iteration:   1250    step:    21018     combined loss: 2354.95448     paf loss 22.35475     hm loss 2332.59973\n",
      "Iteration:   1255    step:    21023     combined loss: 3807.64697     paf loss 20.14856     hm loss 3787.49841\n",
      "Iteration:   1260    step:    21028     combined loss: 2565.97514     paf loss 14.17545     hm loss 2551.79968\n",
      "Iteration:   1265    step:    21033     combined loss: 3455.12081     paf loss 23.24179     hm loss 3431.87903\n",
      "Iteration:   1270    step:    21038     combined loss: 2135.73319     paf loss 14.47746     hm loss 2121.25574\n",
      "Iteration:   1275    step:    21043     combined loss: 2465.58286     paf loss 18.35642     hm loss 2447.22644\n",
      "Iteration:   1280    step:    21048     combined loss: 2532.51465     paf loss 12.18750     hm loss 2520.32715\n",
      "Iteration:   1285    step:    21053     combined loss: 3402.22400     paf loss 19.22156     hm loss 3383.00244\n",
      "Iteration:   1290    step:    21058     combined loss: 3371.06416     paf loss 19.90279     hm loss 3351.16138\n",
      "Iteration:   1295    step:    21063     combined loss: 3188.55281     paf loss 17.65145     hm loss 3170.90137\n",
      "Iteration:   1300    step:    21068     combined loss: 2571.03711     paf loss 22.07251     hm loss 2548.96460\n",
      "Iteration:   1305    step:    21073     combined loss: 3661.26445     paf loss 22.82048     hm loss 3638.44397\n",
      "Iteration:   1310    step:    21078     combined loss: 2716.11038     paf loss 17.25820     hm loss 2698.85217\n",
      "Iteration:   1315    step:    21083     combined loss: 3166.44695     paf loss 16.52654     hm loss 3149.92041\n",
      "Iteration:   1320    step:    21088     combined loss: 2787.57562     paf loss 15.84906     hm loss 2771.72656\n",
      "Iteration:   1325    step:    21093     combined loss: 2629.08781     paf loss 20.51298     hm loss 2608.57483\n",
      "Iteration:   1330    step:    21098     combined loss: 3905.43908     paf loss 15.66979     hm loss 3889.76929\n",
      "Iteration:   1335    step:    21103     combined loss: 2222.90694     paf loss 17.19685     hm loss 2205.71008\n",
      "Iteration:   1340    step:    21108     combined loss: 2349.47423     paf loss 16.87438     hm loss 2332.59985\n",
      "Iteration:   1345    step:    21113     combined loss: 5687.23811     paf loss 24.05549     hm loss 5663.18262\n",
      "Iteration:   1350    step:    21118     combined loss: 3730.92624     paf loss 21.48117     hm loss 3709.44507\n",
      "Iteration:   1355    step:    21123     combined loss: 3557.92546     paf loss 22.64860     hm loss 3535.27686\n",
      "Iteration:   1360    step:    21128     combined loss: 3573.28278     paf loss 20.66523     hm loss 3552.61755\n",
      "Iteration:   1365    step:    21133     combined loss: 4045.98587     paf loss 17.78153     hm loss 4028.20435\n",
      "Iteration:   1370    step:    21138     combined loss: 3935.64418     paf loss 19.73073     hm loss 3915.91345\n",
      "Iteration:   1375    step:    21143     combined loss: 4208.95748     paf loss 22.29879     hm loss 4186.65869\n",
      "Iteration:   1380    step:    21148     combined loss: 5024.62279     paf loss 18.25658     hm loss 5006.36621\n",
      "Iteration:   1385    step:    21153     combined loss: 3738.44600     paf loss 18.18880     hm loss 3720.25720\n",
      "Iteration:   1390    step:    21158     combined loss: 3557.69868     paf loss 21.19209     hm loss 3536.50659\n",
      "Iteration:   1395    step:    21163     combined loss: 3234.10982     paf loss 23.17953     hm loss 3210.93030\n",
      "Iteration:   1400    step:    21168     combined loss: 2574.85213     paf loss 18.74153     hm loss 2556.11060\n",
      "Iteration:   1405    step:    21173     combined loss: 2415.48650     paf loss 15.41313     hm loss 2400.07336\n",
      "Iteration:   1410    step:    21178     combined loss: 4087.07208     paf loss 17.33380     hm loss 4069.73828\n",
      "Iteration:   1415    step:    21183     combined loss: 4603.13579     paf loss 18.55986     hm loss 4584.57593\n",
      "Iteration:   1420    step:    21188     combined loss: 4160.58752     paf loss 17.06823     hm loss 4143.51929\n",
      "learning rate change: 7.8125e-06 --> 3.90625e-06\n",
      "Iteration:   1425    step:    21193     combined loss: 3400.81225     paf loss 19.51855     hm loss 3381.29370\n",
      "Iteration:   1430    step:    21198     combined loss: 2358.86743     paf loss 17.66052     hm loss 2341.20691\n",
      "Iteration:   1435    step:    21203     combined loss: 3221.55734     paf loss 23.72397     hm loss 3197.83337\n",
      "Iteration:   1440    step:    21208     combined loss: 3936.10713     paf loss 16.83479     hm loss 3919.27234\n",
      "Iteration:   1445    step:    21213     combined loss: 2292.07284     paf loss 15.17196     hm loss 2276.90088\n",
      "Iteration:   1450    step:    21218     combined loss: 2628.45885     paf loss 23.78697     hm loss 2604.67188\n",
      "Iteration:   1455    step:    21223     combined loss: 5143.85374     paf loss 26.41795     hm loss 5117.43579\n",
      "Iteration:   1460    step:    21228     combined loss: 3350.29899     paf loss 22.85160     hm loss 3327.44739\n",
      "Iteration:   1465    step:    21233     combined loss: 3638.16647     paf loss 16.23459     hm loss 3621.93188\n",
      "Iteration:   1470    step:    21238     combined loss: 2735.31642     paf loss 20.76918     hm loss 2714.54724\n",
      "Iteration:   1475    step:    21243     combined loss: 3979.52082     paf loss 17.80708     hm loss 3961.71375\n",
      "Iteration:   1480    step:    21248     combined loss: 3720.55960     paf loss 15.83499     hm loss 3704.72461\n",
      "Iteration:   1485    step:    21253     combined loss: 3537.06858     paf loss 18.31223     hm loss 3518.75635\n",
      "Iteration:   1490    step:    21258     combined loss: 4472.42625     paf loss 25.60960     hm loss 4446.81665\n",
      "Iteration:   1495    step:    21263     combined loss: 3527.04492     paf loss 30.72766     hm loss 3496.31726\n",
      "Iteration:   1500    step:    21268     combined loss: 4310.62534     paf loss 19.06553     hm loss 4291.55981\n",
      "Iteration:   1505    step:    21273     combined loss: 4306.94785     paf loss 22.99351     hm loss 4283.95435\n",
      "Iteration:   1510    step:    21278     combined loss: 3752.36119     paf loss 25.55492     hm loss 3726.80627\n",
      "Iteration:   1515    step:    21283     combined loss: 4058.56251     paf loss 21.40223     hm loss 4037.16028\n",
      "Iteration:   1520    step:    21288     combined loss: 4420.12596     paf loss 25.01219     hm loss 4395.11377\n",
      "Iteration:   1525    step:    21293     combined loss: 2719.70706     paf loss 17.04898     hm loss 2702.65808\n",
      "Iteration:   1530    step:    21298     combined loss: 2153.97104     paf loss 15.23727     hm loss 2138.73376\n",
      "Iteration:   1535    step:    21303     combined loss: 3176.02127     paf loss 21.04641     hm loss 3154.97485\n",
      "Iteration:   1540    step:    21308     combined loss: 3231.13378     paf loss 18.31078     hm loss 3212.82300\n",
      "Iteration:   1545    step:    21313     combined loss: 2510.56096     paf loss 15.85612     hm loss 2494.70483\n",
      "Iteration:   1550    step:    21318     combined loss: 4230.08285     paf loss 22.40267     hm loss 4207.68018\n",
      "Iteration:   1555    step:    21323     combined loss: 2931.15173     paf loss 18.62804     hm loss 2912.52368\n",
      "Iteration:   1560    step:    21328     combined loss: 2790.84739     paf loss 18.44969     hm loss 2772.39771\n",
      "Iteration:   1565    step:    21333     combined loss: 3233.51970     paf loss 20.25616     hm loss 3213.26355\n",
      "Iteration:   1570    step:    21338     combined loss: 3021.14240     paf loss 23.65021     hm loss 2997.49219\n",
      "Iteration:   1575    step:    21343     combined loss: 2815.24963     paf loss 15.34228     hm loss 2799.90735\n",
      "Iteration:   1580    step:    21348     combined loss: 4092.30036     paf loss 25.30329     hm loss 4066.99707\n",
      "Iteration:   1585    step:    21353     combined loss: 2631.44000     paf loss 18.41705     hm loss 2613.02295\n",
      "Iteration:   1590    step:    21358     combined loss: 2948.49360     paf loss 19.01057     hm loss 2929.48303\n",
      "Iteration:   1595    step:    21363     combined loss: 3336.27476     paf loss 17.70848     hm loss 3318.56628\n",
      "Iteration:   1600    step:    21368     combined loss: 2717.62618     paf loss 13.15450     hm loss 2704.47168\n",
      "Iteration:   1605    step:    21373     combined loss: 4546.83366     paf loss 25.65300     hm loss 4521.18066\n",
      "Iteration:   1610    step:    21378     combined loss: 3125.22281     paf loss 20.68143     hm loss 3104.54138\n",
      "Iteration:   1615    step:    21383     combined loss: 3187.75707     paf loss 18.78393     hm loss 3168.97314\n",
      "Iteration:   1620    step:    21388     combined loss: 4290.52152     paf loss 22.43729     hm loss 4268.08423\n",
      "Iteration:   1625    step:    21393     combined loss: 4855.90734     paf loss 28.36827     hm loss 4827.53906\n",
      "Iteration:   1630    step:    21398     combined loss: 2624.16670     paf loss 20.72407     hm loss 2603.44263\n",
      "Iteration:   1635    step:    21403     combined loss: 3491.74625     paf loss 22.26273     hm loss 3469.48352\n",
      "Iteration:   1640    step:    21408     combined loss: 2031.22025     paf loss 16.05386     hm loss 2015.16638\n",
      "Iteration:   1645    step:    21413     combined loss: 3280.77046     paf loss 22.35054     hm loss 3258.41992\n",
      "Iteration:   1650    step:    21418     combined loss: 1797.36843     paf loss 15.31094     hm loss 1782.05750\n",
      "Iteration:   1655    step:    21423     combined loss: 1962.84833     paf loss 10.11151     hm loss 1952.73682\n",
      "Iteration:   1660    step:    21428     combined loss: 3064.47306     paf loss 14.76126     hm loss 3049.71179\n",
      "Iteration:   1665    step:    21433     combined loss: 4407.07897     paf loss 19.19835     hm loss 4387.88062\n",
      "Iteration:   1670    step:    21438     combined loss: 5428.50220     paf loss 18.53760     hm loss 5409.96460\n",
      "Iteration:   1675    step:    21443     combined loss: 4554.04088     paf loss 23.43419     hm loss 4530.60669\n",
      "Iteration:   1680    step:    21448     combined loss: 4805.35049     paf loss 17.50161     hm loss 4787.84888\n",
      "Iteration:   1685    step:    21453     combined loss: 3249.81097     paf loss 18.52093     hm loss 3231.29004\n",
      "Iteration:   1690    step:    21458     combined loss: 4797.14891     paf loss 21.90843     hm loss 4775.24048\n",
      "Iteration:   1695    step:    21463     combined loss: 3817.91121     paf loss 23.25142     hm loss 3794.65979\n",
      "Iteration:   1700    step:    21468     combined loss: 2612.46348     paf loss 14.59739     hm loss 2597.86609\n",
      "Iteration:   1705    step:    21473     combined loss: 2746.62292     paf loss 10.65991     hm loss 2735.96301\n",
      "Iteration:   1710    step:    21478     combined loss: 3347.01276     paf loss 19.12604     hm loss 3327.88672\n",
      "Iteration:   1715    step:    21483     combined loss: 2322.29162     paf loss 20.24291     hm loss 2302.04871\n",
      "Iteration:   1720    step:    21488     combined loss: 2871.12049     paf loss 20.35071     hm loss 2850.76978\n",
      "Iteration:   1725    step:    21493     combined loss: 6302.96161     paf loss 28.12665     hm loss 6274.83496\n",
      "Iteration:   1730    step:    21498     combined loss: 4948.81715     paf loss 31.10011     hm loss 4917.71704\n",
      "Iteration:   1735    step:    21503     combined loss: 4536.92228     paf loss 17.85099     hm loss 4519.07129\n",
      "Iteration:   1740    step:    21508     combined loss: 2901.31577     paf loss 14.46299     hm loss 2886.85278\n",
      "Iteration:   1745    step:    21513     combined loss: 5041.77938     paf loss 22.77596     hm loss 5019.00342\n",
      "Iteration:   1750    step:    21518     combined loss: 4250.21299     paf loss 25.21030     hm loss 4225.00269\n",
      "Iteration:   1755    step:    21523     combined loss: 4768.67451     paf loss 20.81196     hm loss 4747.86255\n",
      "Iteration:   1760    step:    21528     combined loss: 2574.79165     paf loss 15.20437     hm loss 2559.58728\n",
      "Iteration:   1765    step:    21533     combined loss: 4056.33141     paf loss 22.59642     hm loss 4033.73499\n",
      "Iteration:   1770    step:    21538     combined loss: 4634.97325     paf loss 24.60801     hm loss 4610.36523\n",
      "Iteration:   1775    step:    21543     combined loss: 2545.35170     paf loss 16.17763     hm loss 2529.17407\n",
      "Iteration:   1780    step:    21548     combined loss: 3729.52051     paf loss 22.48816     hm loss 3707.03235\n",
      "Iteration:   1785    step:    21553     combined loss: 4169.64002     paf loss 19.66712     hm loss 4149.97290\n",
      "Iteration:   1790    step:    21558     combined loss: 3761.67716     paf loss 23.03751     hm loss 3738.63965\n",
      "Iteration:   1795    step:    21563     combined loss: 2306.96033     paf loss 18.00952     hm loss 2288.95081\n",
      "Iteration:   1800    step:    21568     combined loss: 3638.85642     paf loss 20.74057     hm loss 3618.11584\n",
      "Iteration:   1805    step:    21573     combined loss: 3206.25555     paf loss 16.40289     hm loss 3189.85266\n",
      "Iteration:   1810    step:    21578     combined loss: 3291.17112     paf loss 19.29038     hm loss 3271.88074\n",
      "Iteration:   1815    step:    21583     combined loss: 2184.63922     paf loss 15.03253     hm loss 2169.60669\n",
      "Iteration:   1820    step:    21588     combined loss: 3260.01617     paf loss 20.24102     hm loss 3239.77515\n",
      "Iteration:   1825    step:    21593     combined loss: 2879.49789     paf loss 18.49606     hm loss 2861.00183\n",
      "Iteration:   1830    step:    21598     combined loss: 3672.93530     paf loss 14.39551     hm loss 3658.53979\n",
      "Iteration:   1835    step:    21603     combined loss: 4115.56358     paf loss 19.59165     hm loss 4095.97192\n",
      "Iteration:   1840    step:    21608     combined loss: 3913.53598     paf loss 24.39950     hm loss 3889.13647\n",
      "Iteration:   1845    step:    21613     combined loss: 3676.02547     paf loss 19.20100     hm loss 3656.82446\n",
      "Iteration:   1850    step:    21618     combined loss: 3172.70751     paf loss 20.35424     hm loss 3152.35327\n",
      "Iteration:   1855    step:    21623     combined loss: 2551.71065     paf loss 18.61409     hm loss 2533.09656\n",
      "Iteration:   1860    step:    21628     combined loss: 4206.30048     paf loss 19.73993     hm loss 4186.56055\n",
      "Iteration:   1865    step:    21633     combined loss: 6185.11967     paf loss 26.53910     hm loss 6158.58057\n",
      "Iteration:   1870    step:    21638     combined loss: 3642.65491     paf loss 15.11512     hm loss 3627.53979\n",
      "Iteration:   1875    step:    21643     combined loss: 3479.04252     paf loss 19.78789     hm loss 3459.25464\n",
      "Iteration:   1880    step:    21648     combined loss: 2880.46609     paf loss 14.52321     hm loss 2865.94287\n",
      "Iteration:   1885    step:    21653     combined loss: 3432.03739     paf loss 13.20182     hm loss 3418.83557\n",
      "Iteration:   1890    step:    21658     combined loss: 3408.08044     paf loss 20.18798     hm loss 3387.89246\n",
      "Iteration:   1895    step:    21663     combined loss: 4422.14521     paf loss 22.93622     hm loss 4399.20898\n",
      "Iteration:   1900    step:    21668     combined loss: 2749.47826     paf loss 19.22399     hm loss 2730.25427\n",
      "Iteration:   1905    step:    21673     combined loss: 2273.07926     paf loss 17.27640     hm loss 2255.80286\n",
      "Iteration:   1910    step:    21678     combined loss: 3278.61687     paf loss 18.49931     hm loss 3260.11755\n",
      "Iteration:   1915    step:    21683     combined loss: 3728.12398     paf loss 24.65035     hm loss 3703.47363\n",
      "Iteration:   1920    step:    21688     combined loss: 2465.19155     paf loss 13.66811     hm loss 2451.52344\n",
      "Iteration:   1925    step:    21693     combined loss: 3582.03366     paf loss 18.34262     hm loss 3563.69104\n",
      "Iteration:   1930    step:    21698     combined loss: 2640.66314     paf loss 16.67583     hm loss 2623.98730\n",
      "Iteration:   1935    step:    21703     combined loss: 3499.09257     paf loss 18.05851     hm loss 3481.03406\n",
      "Iteration:   1940    step:    21708     combined loss: 4023.48705     paf loss 24.46044     hm loss 3999.02661\n",
      "Iteration:   1945    step:    21713     combined loss: 4184.52984     paf loss 23.58868     hm loss 4160.94116\n",
      "Iteration:   1950    step:    21718     combined loss: 3279.12455     paf loss 17.48978     hm loss 3261.63477\n",
      "Iteration:   1955    step:    21723     combined loss: 3361.95655     paf loss 20.35206     hm loss 3341.60449\n",
      "Iteration:   1960    step:    21728     combined loss: 3865.74773     paf loss 17.52691     hm loss 3848.22083\n",
      "Iteration:   1965    step:    21733     combined loss: 3497.81243     paf loss 20.53936     hm loss 3477.27307\n",
      "Iteration:   1970    step:    21738     combined loss: 3060.38403     paf loss 20.05493     hm loss 3040.32910\n",
      "Iteration:   1975    step:    21743     combined loss: 3777.57746     paf loss 14.98396     hm loss 3762.59351\n",
      "Iteration:   1980    step:    21748     combined loss: 3556.46804     paf loss 24.33450     hm loss 3532.13354\n",
      "Iteration:   1985    step:    21753     combined loss: 5320.82714     paf loss 19.81933     hm loss 5301.00781\n",
      "Iteration:   1990    step:    21758     combined loss: 4793.75188     paf loss 27.63933     hm loss 4766.11255\n",
      "Iteration:   1995    step:    21763     combined loss: 3106.04135     paf loss 19.85165     hm loss 3086.18970\n",
      "Iteration:   2000    step:    21768     combined loss: 4022.56016     paf loss 23.30369     hm loss 3999.25647\n",
      "Iteration:   2005    step:    21773     combined loss: 2268.35025     paf loss 14.74685     hm loss 2253.60339\n",
      "Iteration:   2010    step:    21778     combined loss: 2481.11172     paf loss 19.33767     hm loss 2461.77405\n",
      "Iteration:   2015    step:    21783     combined loss: 3533.01267     paf loss 17.73471     hm loss 3515.27795\n",
      "Iteration:   2020    step:    21788     combined loss: 5019.13272     paf loss 28.46573     hm loss 4990.66699\n",
      "Iteration:   2025    step:    21793     combined loss: 3106.40781     paf loss 19.74082     hm loss 3086.66699\n",
      "Iteration:   2030    step:    21798     combined loss: 3221.47217     paf loss 16.83264     hm loss 3204.63953\n",
      "Iteration:   2035    step:    21803     combined loss: 4002.12943     paf loss 23.09891     hm loss 3979.03052\n",
      "Iteration:   2040    step:    21808     combined loss: 2523.10216     paf loss 19.54173     hm loss 2503.56042\n",
      "Iteration:   2045    step:    21813     combined loss: 3945.10032     paf loss 19.41331     hm loss 3925.68701\n",
      "Iteration:   2050    step:    21818     combined loss: 5725.19382     paf loss 34.30612     hm loss 5690.88770\n",
      "Iteration:   2055    step:    21823     combined loss: 4060.78489     paf loss 20.21666     hm loss 4040.56824\n",
      "Iteration:   2060    step:    21828     combined loss: 3394.62179     paf loss 20.89059     hm loss 3373.73120\n",
      "Iteration:   2065    step:    21833     combined loss: 3969.68136     paf loss 16.32931     hm loss 3953.35205\n",
      "Iteration:   2070    step:    21838     combined loss: 3249.53108     paf loss 14.25605     hm loss 3235.27502\n",
      "Iteration:   2075    step:    21843     combined loss: 3264.89871     paf loss 17.07291     hm loss 3247.82581\n",
      "Iteration:   2080    step:    21848     combined loss: 3148.86416     paf loss 18.48306     hm loss 3130.38110\n",
      "Iteration:   2085    step:    21853     combined loss: 2328.83133     paf loss 20.46341     hm loss 2308.36792\n",
      "Iteration:   2090    step:    21858     combined loss: 3131.29185     paf loss 14.02501     hm loss 3117.26685\n",
      "Iteration:   2095    step:    21863     combined loss: 4002.25245     paf loss 20.86439     hm loss 3981.38806\n",
      "Iteration:   2100    step:    21868     combined loss: 3564.86289     paf loss 17.23680     hm loss 3547.62610\n",
      "Iteration:   2105    step:    21873     combined loss: 3251.15230     paf loss 15.33284     hm loss 3235.81946\n",
      "Iteration:   2110    step:    21878     combined loss: 3723.21754     paf loss 22.65834     hm loss 3700.55920\n",
      "Iteration:   2115    step:    21883     combined loss: 2761.33365     paf loss 19.64322     hm loss 2741.69043\n",
      "Iteration:   2120    step:    21888     combined loss: 4399.61882     paf loss 22.77654     hm loss 4376.84229\n",
      "Iteration:   2125    step:    21893     combined loss: 5749.15172     paf loss 23.15733     hm loss 5725.99438\n",
      "Iteration:   2130    step:    21898     combined loss: 3638.02379     paf loss 21.30589     hm loss 3616.71790\n",
      "Iteration:   2135    step:    21903     combined loss: 3113.23868     paf loss 19.05594     hm loss 3094.18274\n",
      "Iteration:   2140    step:    21908     combined loss: 2757.54275     paf loss 15.65042     hm loss 2741.89233\n",
      "Iteration:   2145    step:    21913     combined loss: 3772.81267     paf loss 18.71806     hm loss 3754.09460\n",
      "Iteration:   2150    step:    21918     combined loss: 1943.99456     paf loss 13.37200     hm loss 1930.62256\n",
      "Iteration:   2155    step:    21923     combined loss: 3203.17646     paf loss 20.87141     hm loss 3182.30505\n",
      "Iteration:   2160    step:    21928     combined loss: 3502.81881     paf loss 20.48703     hm loss 3482.33179\n",
      "Iteration:   2165    step:    21933     combined loss: 3389.48165     paf loss 15.21175     hm loss 3374.26990\n",
      "Iteration:   2170    step:    21938     combined loss: 3433.18880     paf loss 21.81257     hm loss 3411.37622\n",
      "Iteration:   2175    step:    21943     combined loss: 2392.81911     paf loss 14.00808     hm loss 2378.81104\n",
      "Iteration:   2180    step:    21948     combined loss: 4579.72432     paf loss 22.15498     hm loss 4557.56934\n",
      "Iteration:   2185    step:    21953     combined loss: 3847.57730     paf loss 18.17020     hm loss 3829.40710\n",
      "Iteration:   2190    step:    21958     combined loss: 3478.80179     paf loss 18.33475     hm loss 3460.46704\n",
      "Iteration:   2195    step:    21963     combined loss: 6089.54352     paf loss 18.57428     hm loss 6070.96924\n",
      "Iteration:   2200    step:    21968     combined loss: 3658.84281     paf loss 21.01969     hm loss 3637.82312\n",
      "Iteration:   2205    step:    21973     combined loss: 3479.65603     paf loss 20.42312     hm loss 3459.23291\n",
      "Iteration:   2210    step:    21978     combined loss: 2709.78935     paf loss 22.06291     hm loss 2687.72644\n",
      "Iteration:   2215    step:    21983     combined loss: 2952.57695     paf loss 15.08525     hm loss 2937.49170\n",
      "Iteration:   2220    step:    21988     combined loss: 2614.03972     paf loss 18.20342     hm loss 2595.83630\n",
      "Iteration:   2225    step:    21993     combined loss: 4598.69899     paf loss 22.47096     hm loss 4576.22803\n",
      "Iteration:   2230    step:    21998     combined loss: 3739.49940     paf loss 20.76734     hm loss 3718.73206\n",
      "Iteration:   2235    step:    22003     combined loss: 3229.59200     paf loss 17.80025     hm loss 3211.79175\n",
      "Iteration:   2240    step:    22008     combined loss: 3809.02317     paf loss 21.17649     hm loss 3787.84668\n",
      "Iteration:   2245    step:    22013     combined loss: 4157.17977     paf loss 17.91488     hm loss 4139.26489\n",
      "Iteration:   2250    step:    22018     combined loss: 3814.00082     paf loss 18.62362     hm loss 3795.37720\n",
      "Iteration:   2255    step:    22023     combined loss: 3977.13123     paf loss 20.72498     hm loss 3956.40625\n",
      "Iteration:   2260    step:    22028     combined loss: 4461.51802     paf loss 33.87984     hm loss 4427.63818\n",
      "Iteration:   2265    step:    22033     combined loss: 3319.27613     paf loss 15.49159     hm loss 3303.78455\n",
      "Iteration:   2270    step:    22038     combined loss: 4462.46652     paf loss 20.79147     hm loss 4441.67505\n",
      "Iteration:   2275    step:    22043     combined loss: 3785.75943     paf loss 17.16470     hm loss 3768.59473\n",
      "Iteration:   2280    step:    22048     combined loss: 2493.67864     paf loss 16.31133     hm loss 2477.36731\n",
      "Iteration:   2285    step:    22053     combined loss: 6061.67186     paf loss 26.41966     hm loss 6035.25220\n",
      "Iteration:   2290    step:    22058     combined loss: 3895.82500     paf loss 21.90081     hm loss 3873.92419\n",
      "Iteration:   2295    step:    22063     combined loss: 1964.42783     paf loss 13.49613     hm loss 1950.93170\n",
      "Iteration:   2300    step:    22068     combined loss: 3219.55933     paf loss 17.40405     hm loss 3202.15527\n",
      "Iteration:   2305    step:    22073     combined loss: 4546.47882     paf loss 25.20196     hm loss 4521.27686\n",
      "Iteration:   2310    step:    22078     combined loss: 4067.04484     paf loss 21.15763     hm loss 4045.88721\n",
      "Iteration:   2315    step:    22083     combined loss: 4063.48847     paf loss 22.39081     hm loss 4041.09766\n",
      "Iteration:   2320    step:    22088     combined loss: 3445.30872     paf loss 17.01026     hm loss 3428.29846\n",
      "Iteration:   2325    step:    22093     combined loss: 2835.74357     paf loss 21.05241     hm loss 2814.69116\n",
      "Iteration:   2330    step:    22098     combined loss: 3507.58746     paf loss 17.92547     hm loss 3489.66199\n",
      "Iteration:   2335    step:    22103     combined loss: 4317.59951     paf loss 24.16396     hm loss 4293.43555\n",
      "Iteration:   2340    step:    22108     combined loss: 2468.01592     paf loss 16.94500     hm loss 2451.07092\n",
      "Iteration:   2345    step:    22113     combined loss: 3662.59489     paf loss 21.34550     hm loss 3641.24939\n",
      "Iteration:   2350    step:    22118     combined loss: 4974.09812     paf loss 21.69260     hm loss 4952.40552\n",
      "Iteration:   2355    step:    22123     combined loss: 4100.89829     paf loss 17.14573     hm loss 4083.75256\n",
      "Iteration:   2360    step:    22128     combined loss: 2810.78815     paf loss 20.50934     hm loss 2790.27881\n",
      "Iteration:   2365    step:    22133     combined loss: 4759.96535     paf loss 21.35500     hm loss 4738.61035\n",
      "Iteration:   2370    step:    22138     combined loss: 2447.23224     paf loss 20.84418     hm loss 2426.38806\n",
      "Iteration:   2375    step:    22143     combined loss: 3294.84257     paf loss 20.62443     hm loss 3274.21814\n",
      "Iteration:   2380    step:    22148     combined loss: 4346.57053     paf loss 28.62278     hm loss 4317.94775\n",
      "Iteration:   2385    step:    22153     combined loss: 5031.78945     paf loss 18.70083     hm loss 5013.08862\n",
      "Iteration:   2390    step:    22158     combined loss: 4011.72161     paf loss 23.35710     hm loss 3988.36450\n",
      "Iteration:   2395    step:    22163     combined loss: 2267.74949     paf loss 18.41868     hm loss 2249.33081\n",
      "Iteration:   2400    step:    22168     combined loss: 4116.42232     paf loss 33.16011     hm loss 4083.26221\n",
      "Iteration:   2405    step:    22173     combined loss: 2800.96970     paf loss 19.65537     hm loss 2781.31433\n",
      "Iteration:   2410    step:    22178     combined loss: 3405.86369     paf loss 19.40520     hm loss 3386.45850\n",
      "Iteration:   2415    step:    22183     combined loss: 3079.90171     paf loss 19.46958     hm loss 3060.43213\n",
      "Iteration:   2420    step:    22188     combined loss: 4360.93929     paf loss 23.01693     hm loss 4337.92236\n",
      "Iteration:   2425    step:    22193     combined loss: 2709.64067     paf loss 16.04741     hm loss 2693.59326\n",
      "Iteration:   2430    step:    22198     combined loss: 3512.05429     paf loss 17.17526     hm loss 3494.87903\n",
      "Iteration:   2435    step:    22203     combined loss: 3289.23271     paf loss 19.37516     hm loss 3269.85754\n",
      "Iteration:   2440    step:    22208     combined loss: 3393.10325     paf loss 18.90574     hm loss 3374.19751\n",
      "Iteration:   2445    step:    22213     combined loss: 2704.02193     paf loss 15.94246     hm loss 2688.07947\n",
      "Iteration:   2450    step:    22218     combined loss: 4128.15892     paf loss 19.54491     hm loss 4108.61401\n",
      "Iteration:   2455    step:    22223     combined loss: 4311.28917     paf loss 22.90490     hm loss 4288.38428\n",
      "Iteration:   2460    step:    22228     combined loss: 3309.23436     paf loss 19.27269     hm loss 3289.96167\n",
      "Iteration:   2465    step:    22233     combined loss: 3259.98245     paf loss 17.20242     hm loss 3242.78003\n",
      "Iteration:   2470    step:    22238     combined loss: 3864.67850     paf loss 23.23746     hm loss 3841.44104\n",
      "Iteration:   2475    step:    22243     combined loss: 3036.94553     paf loss 14.40622     hm loss 3022.53931\n",
      "Iteration:   2480    step:    22248     combined loss: 3556.21971     paf loss 16.63597     hm loss 3539.58374\n",
      "Iteration:   2485    step:    22253     combined loss: 3300.37411     paf loss 17.37033     hm loss 3283.00378\n",
      "Iteration:   2490    step:    22258     combined loss: 3094.05602     paf loss 24.17272     hm loss 3069.88330\n",
      "Iteration:   2495    step:    22263     combined loss: 4339.29192     paf loss 19.56194     hm loss 4319.72998\n",
      "Iteration:   2500    step:    22268     combined loss: 3375.44461     paf loss 19.13382     hm loss 3356.31079\n",
      "Iteration:   2505    step:    22273     combined loss: 2478.02728     paf loss 18.06280     hm loss 2459.96448\n",
      "Iteration:   2510    step:    22278     combined loss: 2962.77094     paf loss 18.64985     hm loss 2944.12109\n",
      "Iteration:   2515    step:    22283     combined loss: 3502.00124     paf loss 24.68788     hm loss 3477.31335\n",
      "Iteration:   2520    step:    22288     combined loss: 2783.25619     paf loss 18.37253     hm loss 2764.88367\n",
      "Iteration:   2525    step:    22293     combined loss: 3555.75842     paf loss 22.71521     hm loss 3533.04321\n",
      "Iteration:   2530    step:    22298     combined loss: 3866.18728     paf loss 22.05190     hm loss 3844.13538\n",
      "Iteration:   2535    step:    22303     combined loss: 2753.28687     paf loss 16.75403     hm loss 2736.53284\n",
      "Iteration:   2540    step:    22308     combined loss: 2508.42334     paf loss 15.97497     hm loss 2492.44836\n",
      "Iteration:   2545    step:    22313     combined loss: 3783.58534     paf loss 17.31679     hm loss 3766.26855\n",
      "Iteration:   2550    step:    22318     combined loss: 3258.99254     paf loss 25.50633     hm loss 3233.48621\n",
      "Iteration:   2555    step:    22323     combined loss: 2374.32269     paf loss 21.62432     hm loss 2352.69836\n",
      "Iteration:   2560    step:    22328     combined loss: 3733.44805     paf loss 22.36468     hm loss 3711.08337\n",
      "Iteration:   2565    step:    22333     combined loss: 3742.64868     paf loss 18.15502     hm loss 3724.49365\n",
      "Iteration:   2570    step:    22338     combined loss: 3802.64520     paf loss 18.61505     hm loss 3784.03015\n",
      "Iteration:   2575    step:    22343     combined loss: 3457.73834     paf loss 25.32318     hm loss 3432.41516\n",
      "Iteration:   2580    step:    22348     combined loss: 2308.25266     paf loss 14.15451     hm loss 2294.09814\n",
      "Iteration:   2585    step:    22353     combined loss: 4171.53425     paf loss 29.30940     hm loss 4142.22485\n",
      "Iteration:   2590    step:    22358     combined loss: 5014.19310     paf loss 34.18602     hm loss 4980.00708\n",
      "Iteration:   2595    step:    22363     combined loss: 2628.20840     paf loss 17.12393     hm loss 2611.08447\n",
      "Iteration:   2600    step:    22368     combined loss: 4153.81402     paf loss 28.70293     hm loss 4125.11108\n",
      "Iteration:   2605    step:    22373     combined loss: 3534.72025     paf loss 21.04606     hm loss 3513.67419\n",
      "Iteration:   2610    step:    22378     combined loss: 2632.93475     paf loss 22.96649     hm loss 2609.96826\n",
      "Iteration:   2615    step:    22383     combined loss: 2864.40060     paf loss 18.38498     hm loss 2846.01562\n",
      "Iteration:   2620    step:    22388     combined loss: 5785.50652     paf loss 19.18621     hm loss 5766.32031\n",
      "Iteration:   2625    step:    22393     combined loss: 2467.05604     paf loss 21.08521     hm loss 2445.97083\n",
      "Iteration:   2630    step:    22398     combined loss: 3256.59659     paf loss 14.00504     hm loss 3242.59155\n",
      "Iteration:   2635    step:    22403     combined loss: 3930.47434     paf loss 20.48630     hm loss 3909.98804\n",
      "Iteration:   2640    step:    22408     combined loss: 3892.56587     paf loss 24.67891     hm loss 3867.88696\n",
      "Iteration:   2645    step:    22413     combined loss: 3132.54036     paf loss 23.85347     hm loss 3108.68689\n",
      "Iteration:   2650    step:    22418     combined loss: 3366.98701     paf loss 18.62056     hm loss 3348.36646\n",
      "Iteration:   2655    step:    22423     combined loss: 3264.96861     paf loss 25.46361     hm loss 3239.50500\n",
      "Iteration:   2660    step:    22428     combined loss: 4645.21169     paf loss 26.37673     hm loss 4618.83496\n",
      "Iteration:   2665    step:    22433     combined loss: 2534.82888     paf loss 21.75015     hm loss 2513.07874\n",
      "Iteration:   2670    step:    22438     combined loss: 2963.74711     paf loss 16.65446     hm loss 2947.09265\n",
      "Iteration:   2675    step:    22443     combined loss: 5044.11886     paf loss 20.86520     hm loss 5023.25366\n",
      "Iteration:   2680    step:    22448     combined loss: 3527.36803     paf loss 21.99962     hm loss 3505.36841\n",
      "Iteration:   2685    step:    22453     combined loss: 3619.81034     paf loss 23.34110     hm loss 3596.46924\n",
      "Iteration:   2690    step:    22458     combined loss: 5325.77051     paf loss 25.83349     hm loss 5299.93701\n",
      "Iteration:   2695    step:    22463     combined loss: 3062.95980     paf loss 21.91927     hm loss 3041.04053\n",
      "Iteration:   2700    step:    22468     combined loss: 4475.09644     paf loss 25.52759     hm loss 4449.56885\n",
      "Iteration:   2705    step:    22473     combined loss: 3805.02635     paf loss 20.13584     hm loss 3784.89050\n",
      "Iteration:   2710    step:    22478     combined loss: 3855.33022     paf loss 21.20547     hm loss 3834.12476\n",
      "Iteration:   2715    step:    22483     combined loss: 2669.36610     paf loss 15.38014     hm loss 2653.98596\n",
      "Iteration:   2720    step:    22488     combined loss: 2973.01100     paf loss 21.53480     hm loss 2951.47620\n",
      "Iteration:   2725    step:    22493     combined loss: 2276.28637     paf loss 15.16162     hm loss 2261.12476\n",
      "Iteration:   2730    step:    22498     combined loss: 2518.25650     paf loss 18.16763     hm loss 2500.08887\n",
      "Iteration:   2735    step:    22503     combined loss: 2764.48272     paf loss 15.26116     hm loss 2749.22156\n",
      "Iteration:   2740    step:    22508     combined loss: 3478.42595     paf loss 22.88908     hm loss 3455.53687\n",
      "Iteration:   2745    step:    22513     combined loss: 2822.03159     paf loss 19.20541     hm loss 2802.82617\n",
      "Iteration:   2750    step:    22518     combined loss: 3408.25603     paf loss 16.23931     hm loss 3392.01672\n",
      "Iteration:   2755    step:    22523     combined loss: 3458.20204     paf loss 18.52430     hm loss 3439.67773\n",
      "Iteration:   2760    step:    22528     combined loss: 4489.39658     paf loss 17.28574     hm loss 4472.11084\n",
      "Iteration:   2765    step:    22533     combined loss: 5350.38356     paf loss 30.99880     hm loss 5319.38477\n",
      "Iteration:   2770    step:    22538     combined loss: 3726.47467     paf loss 27.36017     hm loss 3699.11450\n",
      "Iteration:   2775    step:    22543     combined loss: 3241.37556     paf loss 20.33223     hm loss 3221.04333\n",
      "Iteration:   2780    step:    22548     combined loss: 4401.23565     paf loss 21.70733     hm loss 4379.52832\n",
      "Iteration:   2785    step:    22553     combined loss: 4068.98226     paf loss 26.52877     hm loss 4042.45349\n",
      "Iteration:   2790    step:    22558     combined loss: 3052.29690     paf loss 23.32901     hm loss 3028.96790\n",
      "Iteration:   2795    step:    22563     combined loss: 2714.43110     paf loss 14.81391     hm loss 2699.61719\n",
      "Iteration:   2800    step:    22568     combined loss: 3459.15775     paf loss 17.27262     hm loss 3441.88513\n",
      "Iteration:   2805    step:    22573     combined loss: 5264.49351     paf loss 23.09507     hm loss 5241.39844\n",
      "Iteration:   2810    step:    22578     combined loss: 3829.92757     paf loss 21.84761     hm loss 3808.07996\n",
      "Iteration:   2815    step:    22583     combined loss: 3607.91785     paf loss 17.63428     hm loss 3590.28357\n",
      "Iteration:   2820    step:    22588     combined loss: 3040.11220     paf loss 16.59011     hm loss 3023.52209\n",
      "Train Loss: 3520.4750    PAF Loss:  19.8924    HM Loss:  3500.5826    Acc: NA\n",
      "Val Loss: 3838.1860    PAF Loss:  16.4319    HM Loss:  3821.7541     Acc: NA\n",
      "Epoch 8/9\n",
      "----------\n",
      "Iteration:      0    step:    22592     combined loss: 3395.10835     paf loss 21.49043     hm loss 3373.61792\n",
      "Iteration:      5    step:    22597     combined loss: 3142.09219     paf loss 15.28347     hm loss 3126.80872\n",
      "Iteration:     10    step:    22602     combined loss: 3309.86443     paf loss 16.66179     hm loss 3293.20264\n",
      "Iteration:     15    step:    22607     combined loss: 3019.69483     paf loss 14.86267     hm loss 3004.83215\n",
      "Iteration:     20    step:    22612     combined loss: 2806.05388     paf loss 14.89849     hm loss 2791.15540\n",
      "Iteration:     25    step:    22617     combined loss: 2988.53251     paf loss 20.03654     hm loss 2968.49597\n",
      "Iteration:     30    step:    22622     combined loss: 3224.12203     paf loss 23.20943     hm loss 3200.91260\n",
      "Iteration:     35    step:    22627     combined loss: 3281.69294     paf loss 16.00727     hm loss 3265.68567\n",
      "Iteration:     40    step:    22632     combined loss: 2959.83296     paf loss 22.47493     hm loss 2937.35803\n",
      "Iteration:     45    step:    22637     combined loss: 2202.31662     paf loss 16.06516     hm loss 2186.25146\n",
      "Iteration:     50    step:    22642     combined loss: 2320.81728     paf loss 17.33242     hm loss 2303.48486\n",
      "Iteration:     55    step:    22647     combined loss: 3464.76181     paf loss 21.53610     hm loss 3443.22571\n",
      "Iteration:     60    step:    22652     combined loss: 3410.38352     paf loss 22.62375     hm loss 3387.75977\n",
      "Iteration:     65    step:    22657     combined loss: 4003.80539     paf loss 20.14243     hm loss 3983.66296\n",
      "Iteration:     70    step:    22662     combined loss: 3285.47203     paf loss 20.96544     hm loss 3264.50659\n",
      "Iteration:     75    step:    22667     combined loss: 4277.62493     paf loss 21.46575     hm loss 4256.15918\n",
      "Iteration:     80    step:    22672     combined loss: 4742.52405     paf loss 23.89416     hm loss 4718.62988\n",
      "Iteration:     85    step:    22677     combined loss: 2269.53278     paf loss 18.18708     hm loss 2251.34570\n",
      "Iteration:     90    step:    22682     combined loss: 3166.78285     paf loss 17.05873     hm loss 3149.72412\n",
      "Iteration:     95    step:    22687     combined loss: 3765.99590     paf loss 21.28459     hm loss 3744.71130\n",
      "Iteration:    100    step:    22692     combined loss: 3706.35503     paf loss 20.99431     hm loss 3685.36072\n",
      "Iteration:    105    step:    22697     combined loss: 4512.10038     paf loss 22.67802     hm loss 4489.42236\n",
      "Iteration:    110    step:    22702     combined loss: 3417.70049     paf loss 20.74077     hm loss 3396.95972\n",
      "Iteration:    115    step:    22707     combined loss: 3269.92171     paf loss 22.04158     hm loss 3247.88013\n",
      "Iteration:    120    step:    22712     combined loss: 2764.36707     paf loss 18.08496     hm loss 2746.28210\n",
      "Iteration:    125    step:    22717     combined loss: 2567.46565     paf loss 16.78560     hm loss 2550.68005\n",
      "Iteration:    130    step:    22722     combined loss: 3592.47172     paf loss 22.11466     hm loss 3570.35706\n",
      "Iteration:    135    step:    22727     combined loss: 3263.55585     paf loss 23.77289     hm loss 3239.78296\n",
      "Iteration:    140    step:    22732     combined loss: 3040.17451     paf loss 14.57966     hm loss 3025.59485\n",
      "Iteration:    145    step:    22737     combined loss: 3725.00670     paf loss 19.24254     hm loss 3705.76416\n",
      "Iteration:    150    step:    22742     combined loss: 4122.72112     paf loss 19.37126     hm loss 4103.34985\n",
      "Iteration:    155    step:    22747     combined loss: 3269.66066     paf loss 20.57009     hm loss 3249.09058\n",
      "Iteration:    160    step:    22752     combined loss: 3271.98154     paf loss 18.25278     hm loss 3253.72876\n",
      "Iteration:    165    step:    22757     combined loss: 2545.68821     paf loss 18.29588     hm loss 2527.39233\n",
      "Iteration:    170    step:    22762     combined loss: 3027.42805     paf loss 16.05159     hm loss 3011.37646\n",
      "Iteration:    175    step:    22767     combined loss: 5211.69472     paf loss 23.77187     hm loss 5187.92285\n",
      "Iteration:    180    step:    22772     combined loss: 4057.85863     paf loss 22.51170     hm loss 4035.34692\n",
      "Iteration:    185    step:    22777     combined loss: 2811.20885     paf loss 18.73229     hm loss 2792.47656\n",
      "Iteration:    190    step:    22782     combined loss: 3727.67167     paf loss 19.53300     hm loss 3708.13867\n",
      "Iteration:    195    step:    22787     combined loss: 3977.59299     paf loss 25.37009     hm loss 3952.22290\n",
      "Iteration:    200    step:    22792     combined loss: 3422.28887     paf loss 23.76470     hm loss 3398.52417\n",
      "Iteration:    205    step:    22797     combined loss: 3423.80671     paf loss 17.11152     hm loss 3406.69519\n",
      "Iteration:    210    step:    22802     combined loss: 3061.20491     paf loss 18.15913     hm loss 3043.04578\n",
      "Iteration:    215    step:    22807     combined loss: 4788.18677     paf loss 16.42554     hm loss 4771.76123\n",
      "Iteration:    220    step:    22812     combined loss: 3111.67671     paf loss 20.88472     hm loss 3090.79199\n",
      "Iteration:    225    step:    22817     combined loss: 3356.26113     paf loss 17.48745     hm loss 3338.77368\n",
      "Iteration:    230    step:    22822     combined loss: 5261.47214     paf loss 26.16281     hm loss 5235.30933\n",
      "Iteration:    235    step:    22827     combined loss: 2455.85841     paf loss 23.45106     hm loss 2432.40735\n",
      "Iteration:    240    step:    22832     combined loss: 4102.12104     paf loss 16.61139     hm loss 4085.50964\n",
      "Iteration:    245    step:    22837     combined loss: 2754.84831     paf loss 14.44621     hm loss 2740.40210\n",
      "Iteration:    250    step:    22842     combined loss: 4017.27586     paf loss 23.34434     hm loss 3993.93152\n",
      "Iteration:    255    step:    22847     combined loss: 5194.13533     paf loss 22.64803     hm loss 5171.48730\n",
      "Iteration:    260    step:    22852     combined loss: 3264.70928     paf loss 20.84526     hm loss 3243.86401\n",
      "Iteration:    265    step:    22857     combined loss: 4531.84965     paf loss 20.22026     hm loss 4511.62939\n",
      "Iteration:    270    step:    22862     combined loss: 3053.68782     paf loss 15.65169     hm loss 3038.03613\n",
      "Iteration:    275    step:    22867     combined loss: 3244.53990     paf loss 21.78465     hm loss 3222.75525\n",
      "Iteration:    280    step:    22872     combined loss: 3451.12141     paf loss 18.21589     hm loss 3432.90552\n",
      "Iteration:    285    step:    22877     combined loss: 4119.37497     paf loss 24.05026     hm loss 4095.32471\n",
      "Iteration:    290    step:    22882     combined loss: 4149.44070     paf loss 17.52419     hm loss 4131.91650\n",
      "Iteration:    295    step:    22887     combined loss: 3963.89756     paf loss 21.94944     hm loss 3941.94812\n",
      "Iteration:    300    step:    22892     combined loss: 3224.30692     paf loss 19.70585     hm loss 3204.60107\n",
      "Iteration:    305    step:    22897     combined loss: 4087.74540     paf loss 17.00810     hm loss 4070.73730\n",
      "Iteration:    310    step:    22902     combined loss: 3309.42837     paf loss 21.98453     hm loss 3287.44385\n",
      "Iteration:    315    step:    22907     combined loss: 2848.81655     paf loss 16.20571     hm loss 2832.61084\n",
      "Iteration:    320    step:    22912     combined loss: 2067.90840     paf loss 15.63356     hm loss 2052.27484\n",
      "Iteration:    325    step:    22917     combined loss: 3301.78881     paf loss 21.23681     hm loss 3280.55200\n",
      "Iteration:    330    step:    22922     combined loss: 3667.07843     paf loss 19.99652     hm loss 3647.08191\n",
      "Iteration:    335    step:    22927     combined loss: 3380.58907     paf loss 19.09835     hm loss 3361.49072\n",
      "Iteration:    340    step:    22932     combined loss: 3159.42410     paf loss 12.31082     hm loss 3147.11328\n",
      "Iteration:    345    step:    22937     combined loss: 2667.24708     paf loss 18.51600     hm loss 2648.73108\n",
      "Iteration:    350    step:    22942     combined loss: 3704.39660     paf loss 19.15258     hm loss 3685.24402\n",
      "Iteration:    355    step:    22947     combined loss: 3514.64574     paf loss 20.79711     hm loss 3493.84863\n",
      "Iteration:    360    step:    22952     combined loss: 3295.15836     paf loss 21.36783     hm loss 3273.79053\n",
      "Iteration:    365    step:    22957     combined loss: 3109.58262     paf loss 21.15867     hm loss 3088.42395\n",
      "Iteration:    370    step:    22962     combined loss: 3308.85613     paf loss 24.95013     hm loss 3283.90601\n",
      "Iteration:    375    step:    22967     combined loss: 4953.45529     paf loss 27.47507     hm loss 4925.98022\n",
      "Iteration:    380    step:    22972     combined loss: 3927.05221     paf loss 15.64010     hm loss 3911.41211\n",
      "Iteration:    385    step:    22977     combined loss: 3927.32922     paf loss 21.22131     hm loss 3906.10791\n",
      "Iteration:    390    step:    22982     combined loss: 4996.69207     paf loss 27.75970     hm loss 4968.93237\n",
      "Iteration:    395    step:    22987     combined loss: 2374.79354     paf loss 17.59359     hm loss 2357.19995\n",
      "Iteration:    400    step:    22992     combined loss: 4079.85159     paf loss 18.43875     hm loss 4061.41284\n",
      "Iteration:    405    step:    22997     combined loss: 5031.85793     paf loss 17.55837     hm loss 5014.29956\n",
      "Iteration:    410    step:    23002     combined loss: 3676.54992     paf loss 23.74853     hm loss 3652.80139\n",
      "Iteration:    415    step:    23007     combined loss: 2838.82080     paf loss 22.12635     hm loss 2816.69446\n",
      "Iteration:    420    step:    23012     combined loss: 5060.97288     paf loss 25.53441     hm loss 5035.43848\n",
      "Iteration:    425    step:    23017     combined loss: 3162.55663     paf loss 17.78417     hm loss 3144.77246\n",
      "Iteration:    430    step:    23022     combined loss: 4046.69611     paf loss 19.01557     hm loss 4027.68054\n",
      "Iteration:    435    step:    23027     combined loss: 4860.93795     paf loss 28.23629     hm loss 4832.70166\n",
      "Iteration:    440    step:    23032     combined loss: 4219.97500     paf loss 25.58291     hm loss 4194.39209\n",
      "Iteration:    445    step:    23037     combined loss: 2515.43359     paf loss 19.75842     hm loss 2495.67517\n",
      "Iteration:    450    step:    23042     combined loss: 3298.60928     paf loss 22.68008     hm loss 3275.92920\n",
      "Iteration:    455    step:    23047     combined loss: 4343.39887     paf loss 21.55097     hm loss 4321.84790\n",
      "Iteration:    460    step:    23052     combined loss: 5074.56297     paf loss 22.24925     hm loss 5052.31372\n",
      "Iteration:    465    step:    23057     combined loss: 3106.68719     paf loss 21.15497     hm loss 3085.53223\n",
      "Iteration:    470    step:    23062     combined loss: 4065.17047     paf loss 20.24408     hm loss 4044.92639\n",
      "Iteration:    475    step:    23067     combined loss: 2857.79107     paf loss 18.54754     hm loss 2839.24353\n",
      "Iteration:    480    step:    23072     combined loss: 2668.73490     paf loss 16.22220     hm loss 2652.51270\n",
      "Iteration:    485    step:    23077     combined loss: 2939.01916     paf loss 17.23815     hm loss 2921.78101\n",
      "Iteration:    490    step:    23082     combined loss: 2947.87638     paf loss 17.63529     hm loss 2930.24109\n",
      "Iteration:    495    step:    23087     combined loss: 2410.14074     paf loss 14.45446     hm loss 2395.68628\n",
      "Iteration:    500    step:    23092     combined loss: 2268.78145     paf loss 20.95479     hm loss 2247.82666\n",
      "Iteration:    505    step:    23097     combined loss: 4175.88575     paf loss 18.62940     hm loss 4157.25635\n",
      "Iteration:    510    step:    23102     combined loss: 2745.39237     paf loss 19.02299     hm loss 2726.36938\n",
      "Iteration:    515    step:    23107     combined loss: 3627.18666     paf loss 18.37514     hm loss 3608.81152\n",
      "Iteration:    520    step:    23112     combined loss: 2846.43403     paf loss 17.82685     hm loss 2828.60718\n",
      "Iteration:    525    step:    23117     combined loss: 4131.39929     paf loss 18.65002     hm loss 4112.74927\n",
      "Iteration:    530    step:    23122     combined loss: 2493.08424     paf loss 17.38197     hm loss 2475.70227\n",
      "Iteration:    535    step:    23127     combined loss: 4939.88542     paf loss 23.04826     hm loss 4916.83716\n",
      "Iteration:    540    step:    23132     combined loss: 3832.11288     paf loss 21.60653     hm loss 3810.50635\n",
      "Iteration:    545    step:    23137     combined loss: 3114.75323     paf loss 17.63360     hm loss 3097.11963\n",
      "Iteration:    550    step:    23142     combined loss: 3202.04643     paf loss 23.07524     hm loss 3178.97119\n",
      "Iteration:    555    step:    23147     combined loss: 3034.12300     paf loss 18.79560     hm loss 3015.32739\n",
      "Iteration:    560    step:    23152     combined loss: 2408.07572     paf loss 14.13224     hm loss 2393.94348\n",
      "Iteration:    565    step:    23157     combined loss: 3517.94372     paf loss 19.94469     hm loss 3497.99902\n",
      "Iteration:    570    step:    23162     combined loss: 3350.96812     paf loss 17.08713     hm loss 3333.88098\n",
      "Iteration:    575    step:    23167     combined loss: 2142.07080     paf loss 14.11548     hm loss 2127.95532\n",
      "Iteration:    580    step:    23172     combined loss: 2742.13166     paf loss 18.85077     hm loss 2723.28088\n",
      "Iteration:    585    step:    23177     combined loss: 2756.05309     paf loss 18.35058     hm loss 2737.70251\n",
      "Iteration:    590    step:    23182     combined loss: 2970.19446     paf loss 18.68823     hm loss 2951.50623\n",
      "Iteration:    595    step:    23187     combined loss: 2952.61238     paf loss 20.55306     hm loss 2932.05933\n",
      "Iteration:    600    step:    23192     combined loss: 2356.58605     paf loss 16.27404     hm loss 2340.31201\n",
      "Iteration:    605    step:    23197     combined loss: 2553.59949     paf loss 16.45349     hm loss 2537.14600\n",
      "Iteration:    610    step:    23202     combined loss: 3707.29901     paf loss 21.26276     hm loss 3686.03625\n",
      "Iteration:    615    step:    23207     combined loss: 4595.11173     paf loss 23.96768     hm loss 4571.14404\n",
      "Iteration:    620    step:    23212     combined loss: 2319.09345     paf loss 13.37287     hm loss 2305.72058\n",
      "Iteration:    625    step:    23217     combined loss: 3435.79583     paf loss 18.28020     hm loss 3417.51562\n",
      "Iteration:    630    step:    23222     combined loss: 3896.34638     paf loss 17.17256     hm loss 3879.17383\n",
      "Iteration:    635    step:    23227     combined loss: 2768.81255     paf loss 18.41252     hm loss 2750.40002\n",
      "Iteration:    640    step:    23232     combined loss: 2903.53198     paf loss 16.67334     hm loss 2886.85864\n",
      "Iteration:    645    step:    23237     combined loss: 2586.03259     paf loss 23.33533     hm loss 2562.69727\n",
      "Iteration:    650    step:    23242     combined loss: 3755.85572     paf loss 15.54823     hm loss 3740.30750\n",
      "Iteration:    655    step:    23247     combined loss: 2362.30039     paf loss 16.17454     hm loss 2346.12585\n",
      "Iteration:    660    step:    23252     combined loss: 3638.22282     paf loss 15.64274     hm loss 3622.58008\n",
      "Iteration:    665    step:    23257     combined loss: 2032.36677     paf loss 14.47175     hm loss 2017.89502\n",
      "Iteration:    670    step:    23262     combined loss: 3383.19506     paf loss 17.91259     hm loss 3365.28247\n",
      "Iteration:    675    step:    23267     combined loss: 3702.56863     paf loss 21.98611     hm loss 3680.58252\n",
      "Iteration:    680    step:    23272     combined loss: 3974.89264     paf loss 20.69769     hm loss 3954.19495\n",
      "Iteration:    685    step:    23277     combined loss: 3168.13928     paf loss 15.22522     hm loss 3152.91406\n",
      "Iteration:    690    step:    23282     combined loss: 2372.60811     paf loss 16.34651     hm loss 2356.26160\n",
      "Iteration:    695    step:    23287     combined loss: 3936.00599     paf loss 17.63136     hm loss 3918.37463\n",
      "Iteration:    700    step:    23292     combined loss: 2797.91944     paf loss 18.50611     hm loss 2779.41333\n",
      "Iteration:    705    step:    23297     combined loss: 3099.85977     paf loss 24.74759     hm loss 3075.11218\n",
      "Iteration:    710    step:    23302     combined loss: 3412.12225     paf loss 21.64569     hm loss 3390.47656\n",
      "Iteration:    715    step:    23307     combined loss: 2901.09901     paf loss 23.69984     hm loss 2877.39917\n",
      "Iteration:    720    step:    23312     combined loss: 5082.45486     paf loss 21.39016     hm loss 5061.06470\n",
      "Iteration:    725    step:    23317     combined loss: 2930.44790     paf loss 20.90335     hm loss 2909.54456\n",
      "Iteration:    730    step:    23322     combined loss: 2970.96182     paf loss 18.16202     hm loss 2952.79980\n",
      "Iteration:    735    step:    23327     combined loss: 4194.63465     paf loss 21.82191     hm loss 4172.81274\n",
      "Iteration:    740    step:    23332     combined loss: 5274.48384     paf loss 20.63642     hm loss 5253.84741\n",
      "Iteration:    745    step:    23337     combined loss: 2554.54079     paf loss 17.09169     hm loss 2537.44910\n",
      "Iteration:    750    step:    23342     combined loss: 3265.81530     paf loss 19.80370     hm loss 3246.01160\n",
      "Iteration:    755    step:    23347     combined loss: 4681.89346     paf loss 19.63906     hm loss 4662.25439\n",
      "Iteration:    760    step:    23352     combined loss: 3325.22026     paf loss 16.57743     hm loss 3308.64282\n",
      "Iteration:    765    step:    23357     combined loss: 3816.76371     paf loss 20.44535     hm loss 3796.31836\n",
      "Iteration:    770    step:    23362     combined loss: 2721.02237     paf loss 19.18937     hm loss 2701.83301\n",
      "Iteration:    775    step:    23367     combined loss: 3310.92389     paf loss 19.01202     hm loss 3291.91187\n",
      "Iteration:    780    step:    23372     combined loss: 2916.96770     paf loss 21.72344     hm loss 2895.24426\n",
      "Iteration:    785    step:    23377     combined loss: 2622.29095     paf loss 16.89044     hm loss 2605.40051\n",
      "Iteration:    790    step:    23382     combined loss: 3947.56288     paf loss 21.29628     hm loss 3926.26660\n",
      "Iteration:    795    step:    23387     combined loss: 3344.75471     paf loss 22.23286     hm loss 3322.52185\n",
      "Iteration:    800    step:    23392     combined loss: 4266.53433     paf loss 24.02432     hm loss 4242.51001\n",
      "Iteration:    805    step:    23397     combined loss: 2473.16552     paf loss 13.81774     hm loss 2459.34778\n",
      "Iteration:    810    step:    23402     combined loss: 3573.63475     paf loss 21.02257     hm loss 3552.61218\n",
      "Iteration:    815    step:    23407     combined loss: 3916.35820     paf loss 25.98002     hm loss 3890.37817\n",
      "Iteration:    820    step:    23412     combined loss: 4316.86519     paf loss 24.36885     hm loss 4292.49634\n",
      "Iteration:    825    step:    23417     combined loss: 4162.27334     paf loss 23.54190     hm loss 4138.73145\n",
      "learning rate change: 3.90625e-06 --> 1.953125e-06\n",
      "Iteration:    830    step:    23422     combined loss: 2988.61678     paf loss 21.19454     hm loss 2967.42224\n",
      "Iteration:    835    step:    23427     combined loss: 4134.65975     paf loss 18.99752     hm loss 4115.66223\n",
      "Iteration:    840    step:    23432     combined loss: 3083.95742     paf loss 19.51210     hm loss 3064.44531\n",
      "Iteration:    845    step:    23437     combined loss: 3099.37246     paf loss 18.54226     hm loss 3080.83020\n",
      "Iteration:    850    step:    23442     combined loss: 4375.70104     paf loss 22.16173     hm loss 4353.53931\n",
      "Iteration:    855    step:    23447     combined loss: 3344.84715     paf loss 21.96788     hm loss 3322.87927\n",
      "Iteration:    860    step:    23452     combined loss: 4273.96898     paf loss 26.59813     hm loss 4247.37085\n",
      "Iteration:    865    step:    23457     combined loss: 3164.64291     paf loss 18.21859     hm loss 3146.42432\n",
      "Iteration:    870    step:    23462     combined loss: 3651.94358     paf loss 14.70444     hm loss 3637.23914\n",
      "Iteration:    875    step:    23467     combined loss: 4044.54989     paf loss 22.44198     hm loss 4022.10791\n",
      "Iteration:    880    step:    23472     combined loss: 3285.95207     paf loss 22.57781     hm loss 3263.37427\n",
      "Iteration:    885    step:    23477     combined loss: 3383.80805     paf loss 22.98188     hm loss 3360.82617\n",
      "Iteration:    890    step:    23482     combined loss: 4554.55415     paf loss 16.16645     hm loss 4538.38770\n",
      "Iteration:    895    step:    23487     combined loss: 4531.04862     paf loss 19.44096     hm loss 4511.60767\n",
      "Iteration:    900    step:    23492     combined loss: 2823.60309     paf loss 15.32257     hm loss 2808.28052\n",
      "Iteration:    905    step:    23497     combined loss: 3342.69080     paf loss 22.83704     hm loss 3319.85376\n",
      "Iteration:    910    step:    23502     combined loss: 2355.03333     paf loss 19.43494     hm loss 2335.59839\n",
      "Iteration:    915    step:    23507     combined loss: 2962.98785     paf loss 19.51556     hm loss 2943.47229\n",
      "Iteration:    920    step:    23512     combined loss: 2778.26231     paf loss 14.24803     hm loss 2764.01428\n",
      "Iteration:    925    step:    23517     combined loss: 3286.57224     paf loss 17.05430     hm loss 3269.51794\n",
      "Iteration:    930    step:    23522     combined loss: 4327.23472     paf loss 19.09996     hm loss 4308.13477\n",
      "Iteration:    935    step:    23527     combined loss: 3550.90124     paf loss 19.32996     hm loss 3531.57129\n",
      "Iteration:    940    step:    23532     combined loss: 3344.06170     paf loss 20.79107     hm loss 3323.27063\n",
      "Iteration:    945    step:    23537     combined loss: 4746.49893     paf loss 23.39322     hm loss 4723.10571\n",
      "Iteration:    950    step:    23542     combined loss: 3294.43362     paf loss 23.80520     hm loss 3270.62842\n",
      "Iteration:    955    step:    23547     combined loss: 3306.46099     paf loss 17.99444     hm loss 3288.46655\n",
      "Iteration:    960    step:    23552     combined loss: 3302.10436     paf loss 20.09374     hm loss 3282.01062\n",
      "Iteration:    965    step:    23557     combined loss: 4283.79696     paf loss 18.13314     hm loss 4265.66382\n",
      "Iteration:    970    step:    23562     combined loss: 2843.45706     paf loss 18.14969     hm loss 2825.30737\n",
      "Iteration:    975    step:    23567     combined loss: 4774.92431     paf loss 20.62646     hm loss 4754.29785\n",
      "Iteration:    980    step:    23572     combined loss: 3229.00289     paf loss 20.09420     hm loss 3208.90869\n",
      "Iteration:    985    step:    23577     combined loss: 3748.18182     paf loss 21.26715     hm loss 3726.91467\n",
      "Iteration:    990    step:    23582     combined loss: 2726.70222     paf loss 19.21015     hm loss 2707.49207\n",
      "Iteration:    995    step:    23587     combined loss: 2946.85079     paf loss 22.96968     hm loss 2923.88110\n",
      "Iteration:   1000    step:    23592     combined loss: 3300.04153     paf loss 20.24685     hm loss 3279.79468\n",
      "Iteration:   1005    step:    23597     combined loss: 2627.49516     paf loss 16.89896     hm loss 2610.59619\n",
      "Iteration:   1010    step:    23602     combined loss: 3381.87840     paf loss 15.36497     hm loss 3366.51343\n",
      "Iteration:   1015    step:    23607     combined loss: 3325.56006     paf loss 18.91333     hm loss 3306.64673\n",
      "Iteration:   1020    step:    23612     combined loss: 4632.91449     paf loss 24.72845     hm loss 4608.18604\n",
      "Iteration:   1025    step:    23617     combined loss: 3014.09186     paf loss 21.62140     hm loss 2992.47046\n",
      "Iteration:   1030    step:    23622     combined loss: 2574.54337     paf loss 13.79068     hm loss 2560.75269\n",
      "Iteration:   1035    step:    23627     combined loss: 2801.32383     paf loss 16.92564     hm loss 2784.39819\n",
      "Iteration:   1040    step:    23632     combined loss: 3965.59552     paf loss 21.79987     hm loss 3943.79565\n",
      "Iteration:   1045    step:    23637     combined loss: 3411.07691     paf loss 25.55152     hm loss 3385.52539\n",
      "Iteration:   1050    step:    23642     combined loss: 3253.35048     paf loss 30.36818     hm loss 3222.98230\n",
      "Iteration:   1055    step:    23647     combined loss: 3326.74472     paf loss 18.19015     hm loss 3308.55457\n",
      "Iteration:   1060    step:    23652     combined loss: 4242.34578     paf loss 24.57625     hm loss 4217.76953\n",
      "Iteration:   1065    step:    23657     combined loss: 3538.55003     paf loss 22.61192     hm loss 3515.93811\n",
      "Iteration:   1070    step:    23662     combined loss: 3135.12211     paf loss 17.96562     hm loss 3117.15649\n",
      "Iteration:   1075    step:    23667     combined loss: 2878.67085     paf loss 22.22517     hm loss 2856.44568\n",
      "Iteration:   1080    step:    23672     combined loss: 3176.79948     paf loss 15.41288     hm loss 3161.38660\n",
      "Iteration:   1085    step:    23677     combined loss: 4565.87623     paf loss 23.47072     hm loss 4542.40552\n",
      "Iteration:   1090    step:    23682     combined loss: 3395.15536     paf loss 21.38388     hm loss 3373.77148\n",
      "Iteration:   1095    step:    23687     combined loss: 2844.71848     paf loss 15.73508     hm loss 2828.98340\n",
      "Iteration:   1100    step:    23692     combined loss: 2977.49864     paf loss 21.75120     hm loss 2955.74744\n",
      "Iteration:   1105    step:    23697     combined loss: 3490.47172     paf loss 21.78361     hm loss 3468.68811\n",
      "Iteration:   1110    step:    23702     combined loss: 5428.39181     paf loss 32.24777     hm loss 5396.14404\n",
      "Iteration:   1115    step:    23707     combined loss: 2489.06453     paf loss 19.71077     hm loss 2469.35376\n",
      "Iteration:   1120    step:    23712     combined loss: 3147.04619     paf loss 19.16447     hm loss 3127.88171\n",
      "Iteration:   1125    step:    23717     combined loss: 3650.76622     paf loss 20.21410     hm loss 3630.55212\n",
      "Iteration:   1130    step:    23722     combined loss: 4214.44554     paf loss 16.68040     hm loss 4197.76514\n",
      "Iteration:   1135    step:    23727     combined loss: 2697.45656     paf loss 17.21791     hm loss 2680.23865\n",
      "Iteration:   1140    step:    23732     combined loss: 3594.60892     paf loss 21.93899     hm loss 3572.66992\n",
      "Iteration:   1145    step:    23737     combined loss: 2894.06631     paf loss 18.23416     hm loss 2875.83215\n",
      "Iteration:   1150    step:    23742     combined loss: 3032.85723     paf loss 20.19732     hm loss 3012.65991\n",
      "Iteration:   1155    step:    23747     combined loss: 3515.20077     paf loss 19.55795     hm loss 3495.64282\n",
      "Iteration:   1160    step:    23752     combined loss: 2716.57496     paf loss 16.96034     hm loss 2699.61462\n",
      "Iteration:   1165    step:    23757     combined loss: 3441.60752     paf loss 17.55027     hm loss 3424.05725\n",
      "Iteration:   1170    step:    23762     combined loss: 3251.59387     paf loss 20.67102     hm loss 3230.92285\n",
      "Iteration:   1175    step:    23767     combined loss: 4286.00202     paf loss 26.77033     hm loss 4259.23169\n",
      "Iteration:   1180    step:    23772     combined loss: 3830.89011     paf loss 20.22715     hm loss 3810.66296\n",
      "Iteration:   1185    step:    23777     combined loss: 3268.20003     paf loss 25.01498     hm loss 3243.18506\n",
      "Iteration:   1190    step:    23782     combined loss: 3979.62681     paf loss 23.75486     hm loss 3955.87195\n",
      "Iteration:   1195    step:    23787     combined loss: 3493.26624     paf loss 17.76417     hm loss 3475.50208\n",
      "Iteration:   1200    step:    23792     combined loss: 2222.43953     paf loss 18.14486     hm loss 2204.29468\n",
      "Iteration:   1205    step:    23797     combined loss: 3975.61950     paf loss 18.03442     hm loss 3957.58508\n",
      "Iteration:   1210    step:    23802     combined loss: 3836.78275     paf loss 19.01212     hm loss 3817.77063\n",
      "Iteration:   1215    step:    23807     combined loss: 3101.12894     paf loss 21.74857     hm loss 3079.38037\n",
      "Iteration:   1220    step:    23812     combined loss: 3738.08525     paf loss 24.50114     hm loss 3713.58411\n",
      "Iteration:   1225    step:    23817     combined loss: 4090.32598     paf loss 26.03765     hm loss 4064.28833\n",
      "Iteration:   1230    step:    23822     combined loss: 2919.72570     paf loss 18.14122     hm loss 2901.58447\n",
      "Iteration:   1235    step:    23827     combined loss: 3016.38523     paf loss 21.58579     hm loss 2994.79944\n",
      "Iteration:   1240    step:    23832     combined loss: 2735.12768     paf loss 18.32324     hm loss 2716.80444\n",
      "Iteration:   1245    step:    23837     combined loss: 4237.93417     paf loss 23.92782     hm loss 4214.00635\n",
      "Iteration:   1250    step:    23842     combined loss: 3657.65013     paf loss 18.53246     hm loss 3639.11768\n",
      "Iteration:   1255    step:    23847     combined loss: 4004.79526     paf loss 19.80954     hm loss 3984.98572\n",
      "Iteration:   1260    step:    23852     combined loss: 3288.85262     paf loss 22.48213     hm loss 3266.37048\n",
      "Iteration:   1265    step:    23857     combined loss: 3562.75781     paf loss 17.36243     hm loss 3545.39539\n",
      "Iteration:   1270    step:    23862     combined loss: 2309.95933     paf loss 15.51939     hm loss 2294.43994\n",
      "Iteration:   1275    step:    23867     combined loss: 2843.19492     paf loss 23.25754     hm loss 2819.93738\n",
      "Iteration:   1280    step:    23872     combined loss: 3034.13598     paf loss 19.16589     hm loss 3014.97009\n",
      "Iteration:   1285    step:    23877     combined loss: 3464.35847     paf loss 23.11519     hm loss 3441.24329\n",
      "Iteration:   1290    step:    23882     combined loss: 1861.08812     paf loss 17.03954     hm loss 1844.04858\n",
      "Iteration:   1295    step:    23887     combined loss: 3873.28940     paf loss 25.67490     hm loss 3847.61450\n",
      "Iteration:   1300    step:    23892     combined loss: 3080.07789     paf loss 15.92286     hm loss 3064.15503\n",
      "Iteration:   1305    step:    23897     combined loss: 2889.77522     paf loss 17.83016     hm loss 2871.94507\n",
      "Iteration:   1310    step:    23902     combined loss: 2862.42636     paf loss 19.05515     hm loss 2843.37122\n",
      "Iteration:   1315    step:    23907     combined loss: 3500.89938     paf loss 17.46262     hm loss 3483.43677\n",
      "Iteration:   1320    step:    23912     combined loss: 3718.34955     paf loss 21.32306     hm loss 3697.02649\n",
      "Iteration:   1325    step:    23917     combined loss: 3986.08864     paf loss 24.25905     hm loss 3961.82959\n",
      "Iteration:   1330    step:    23922     combined loss: 3415.05819     paf loss 19.93881     hm loss 3395.11938\n",
      "Iteration:   1335    step:    23927     combined loss: 3733.67829     paf loss 21.74470     hm loss 3711.93359\n",
      "Iteration:   1340    step:    23932     combined loss: 4057.37098     paf loss 21.27345     hm loss 4036.09753\n",
      "Iteration:   1345    step:    23937     combined loss: 3462.50801     paf loss 24.12044     hm loss 3438.38757\n",
      "Iteration:   1350    step:    23942     combined loss: 3797.01037     paf loss 19.16454     hm loss 3777.84583\n",
      "Iteration:   1355    step:    23947     combined loss: 2886.11944     paf loss 17.90630     hm loss 2868.21313\n",
      "Iteration:   1360    step:    23952     combined loss: 3533.19480     paf loss 24.78184     hm loss 3508.41296\n",
      "Iteration:   1365    step:    23957     combined loss: 3640.21593     paf loss 18.25548     hm loss 3621.96045\n",
      "Iteration:   1370    step:    23962     combined loss: 3286.58936     paf loss 21.70203     hm loss 3264.88733\n",
      "Iteration:   1375    step:    23967     combined loss: 3806.36987     paf loss 22.03112     hm loss 3784.33875\n",
      "Iteration:   1380    step:    23972     combined loss: 5391.12410     paf loss 20.04573     hm loss 5371.07837\n",
      "Iteration:   1385    step:    23977     combined loss: 3814.62105     paf loss 15.92561     hm loss 3798.69543\n",
      "Iteration:   1390    step:    23982     combined loss: 5260.07010     paf loss 33.53958     hm loss 5226.53052\n",
      "Iteration:   1395    step:    23987     combined loss: 3873.78969     paf loss 18.08816     hm loss 3855.70154\n",
      "Iteration:   1400    step:    23992     combined loss: 4470.45495     paf loss 24.11047     hm loss 4446.34448\n",
      "Iteration:   1405    step:    23997     combined loss: 4535.91396     paf loss 26.23061     hm loss 4509.68335\n",
      "Iteration:   1410    step:    24002     combined loss: 5516.42177     paf loss 24.20791     hm loss 5492.21387\n",
      "Iteration:   1415    step:    24007     combined loss: 3253.58925     paf loss 19.67531     hm loss 3233.91394\n",
      "Iteration:   1420    step:    24012     combined loss: 3493.92547     paf loss 19.54168     hm loss 3474.38379\n",
      "Iteration:   1425    step:    24017     combined loss: 2396.31819     paf loss 14.25593     hm loss 2382.06226\n",
      "Iteration:   1430    step:    24022     combined loss: 3860.60345     paf loss 17.51239     hm loss 3843.09106\n",
      "Iteration:   1435    step:    24027     combined loss: 3623.19540     paf loss 17.55136     hm loss 3605.64404\n",
      "Iteration:   1440    step:    24032     combined loss: 4066.45234     paf loss 17.13532     hm loss 4049.31702\n",
      "Iteration:   1445    step:    24037     combined loss: 2769.27248     paf loss 10.37783     hm loss 2758.89465\n",
      "Iteration:   1450    step:    24042     combined loss: 2428.23575     paf loss 13.73136     hm loss 2414.50439\n",
      "Iteration:   1455    step:    24047     combined loss: 2448.12219     paf loss 16.54834     hm loss 2431.57385\n",
      "Iteration:   1460    step:    24052     combined loss: 4553.48413     paf loss 28.14209     hm loss 4525.34204\n",
      "Iteration:   1465    step:    24057     combined loss: 3606.26077     paf loss 22.18350     hm loss 3584.07727\n",
      "Iteration:   1470    step:    24062     combined loss: 2212.56340     paf loss 20.70878     hm loss 2191.85461\n",
      "Iteration:   1475    step:    24067     combined loss: 3880.92814     paf loss 21.25114     hm loss 3859.67700\n",
      "Iteration:   1480    step:    24072     combined loss: 3205.80922     paf loss 23.18959     hm loss 3182.61963\n",
      "Iteration:   1485    step:    24077     combined loss: 4089.37039     paf loss 24.56192     hm loss 4064.80847\n",
      "Iteration:   1490    step:    24082     combined loss: 4430.47421     paf loss 22.63754     hm loss 4407.83667\n",
      "Iteration:   1495    step:    24087     combined loss: 4243.57308     paf loss 27.71224     hm loss 4215.86084\n",
      "Iteration:   1500    step:    24092     combined loss: 3859.52221     paf loss 21.39684     hm loss 3838.12537\n",
      "Iteration:   1505    step:    24097     combined loss: 3061.20065     paf loss 21.07089     hm loss 3040.12976\n",
      "Iteration:   1510    step:    24102     combined loss: 4324.30743     paf loss 24.75982     hm loss 4299.54761\n",
      "Iteration:   1515    step:    24107     combined loss: 4075.50233     paf loss 25.99110     hm loss 4049.51123\n",
      "Iteration:   1520    step:    24112     combined loss: 4236.88784     paf loss 24.87807     hm loss 4212.00977\n",
      "Iteration:   1525    step:    24117     combined loss: 2881.49530     paf loss 19.54815     hm loss 2861.94714\n",
      "Iteration:   1530    step:    24122     combined loss: 3306.74868     paf loss 15.00711     hm loss 3291.74158\n",
      "Iteration:   1535    step:    24127     combined loss: 3162.83220     paf loss 25.47441     hm loss 3137.35779\n",
      "Iteration:   1540    step:    24132     combined loss: 2763.39632     paf loss 17.62044     hm loss 2745.77588\n",
      "Iteration:   1545    step:    24137     combined loss: 4331.74208     paf loss 22.32289     hm loss 4309.41919\n",
      "Iteration:   1550    step:    24142     combined loss: 3593.34106     paf loss 20.44519     hm loss 3572.89587\n",
      "Iteration:   1555    step:    24147     combined loss: 4310.90520     paf loss 22.60100     hm loss 4288.30420\n",
      "Iteration:   1560    step:    24152     combined loss: 3783.77381     paf loss 24.21900     hm loss 3759.55481\n",
      "Iteration:   1565    step:    24157     combined loss: 3771.00825     paf loss 23.07832     hm loss 3747.92993\n",
      "Iteration:   1570    step:    24162     combined loss: 3211.95139     paf loss 18.16525     hm loss 3193.78613\n",
      "Iteration:   1575    step:    24167     combined loss: 2444.48669     paf loss 20.21410     hm loss 2424.27258\n",
      "Iteration:   1580    step:    24172     combined loss: 2992.34990     paf loss 18.30327     hm loss 2974.04663\n",
      "Iteration:   1585    step:    24177     combined loss: 3297.59992     paf loss 19.88263     hm loss 3277.71729\n",
      "Iteration:   1590    step:    24182     combined loss: 3435.59430     paf loss 17.81916     hm loss 3417.77515\n",
      "Iteration:   1595    step:    24187     combined loss: 4093.42234     paf loss 22.60666     hm loss 4070.81567\n",
      "Iteration:   1600    step:    24192     combined loss: 2466.61170     paf loss 14.98695     hm loss 2451.62476\n",
      "Iteration:   1605    step:    24197     combined loss: 2155.96384     paf loss 13.49302     hm loss 2142.47083\n",
      "Iteration:   1610    step:    24202     combined loss: 2278.13149     paf loss 15.71340     hm loss 2262.41809\n",
      "Iteration:   1615    step:    24207     combined loss: 3318.39465     paf loss 21.67212     hm loss 3296.72253\n",
      "Iteration:   1620    step:    24212     combined loss: 3984.76655     paf loss 21.64753     hm loss 3963.11902\n",
      "Iteration:   1625    step:    24217     combined loss: 2609.69862     paf loss 14.93190     hm loss 2594.76672\n",
      "Iteration:   1630    step:    24222     combined loss: 2001.46065     paf loss 17.45797     hm loss 1984.00269\n",
      "Iteration:   1635    step:    24227     combined loss: 3387.88027     paf loss 21.84670     hm loss 3366.03357\n",
      "Iteration:   1640    step:    24232     combined loss: 4743.88210     paf loss 21.69607     hm loss 4722.18604\n",
      "Iteration:   1645    step:    24237     combined loss: 3488.69808     paf loss 18.75265     hm loss 3469.94543\n",
      "Iteration:   1650    step:    24242     combined loss: 2109.36557     paf loss 21.02194     hm loss 2088.34363\n",
      "Iteration:   1655    step:    24247     combined loss: 2996.62741     paf loss 18.05368     hm loss 2978.57373\n",
      "Iteration:   1660    step:    24252     combined loss: 2966.50906     paf loss 23.47549     hm loss 2943.03357\n",
      "Iteration:   1665    step:    24257     combined loss: 3948.55898     paf loss 26.94423     hm loss 3921.61475\n",
      "Iteration:   1670    step:    24262     combined loss: 4059.15135     paf loss 17.82152     hm loss 4041.32983\n",
      "Iteration:   1675    step:    24267     combined loss: 2679.52529     paf loss 17.53871     hm loss 2661.98657\n",
      "Iteration:   1680    step:    24272     combined loss: 3371.79292     paf loss 21.37312     hm loss 3350.41980\n",
      "Iteration:   1685    step:    24277     combined loss: 3034.02515     paf loss 18.54114     hm loss 3015.48401\n",
      "Iteration:   1690    step:    24282     combined loss: 3851.88982     paf loss 20.73955     hm loss 3831.15027\n",
      "Iteration:   1695    step:    24287     combined loss: 2254.79931     paf loss 14.02294     hm loss 2240.77637\n",
      "Iteration:   1700    step:    24292     combined loss: 4022.94883     paf loss 16.67124     hm loss 4006.27759\n",
      "Iteration:   1705    step:    24297     combined loss: 2822.90072     paf loss 16.39730     hm loss 2806.50342\n",
      "Iteration:   1710    step:    24302     combined loss: 3372.74623     paf loss 18.56190     hm loss 3354.18433\n",
      "Iteration:   1715    step:    24307     combined loss: 2225.19016     paf loss 16.75974     hm loss 2208.43042\n",
      "Iteration:   1720    step:    24312     combined loss: 4676.71822     paf loss 26.05904     hm loss 4650.65918\n",
      "Iteration:   1725    step:    24317     combined loss: 3278.78614     paf loss 19.47596     hm loss 3259.31018\n",
      "Iteration:   1730    step:    24322     combined loss: 3486.90960     paf loss 21.96527     hm loss 3464.94434\n",
      "Iteration:   1735    step:    24327     combined loss: 3767.71580     paf loss 19.04172     hm loss 3748.67407\n",
      "Iteration:   1740    step:    24332     combined loss: 4117.43162     paf loss 13.49802     hm loss 4103.93359\n",
      "Iteration:   1745    step:    24337     combined loss: 4225.14822     paf loss 22.31081     hm loss 4202.83740\n",
      "Iteration:   1750    step:    24342     combined loss: 3688.53933     paf loss 18.23501     hm loss 3670.30432\n",
      "Iteration:   1755    step:    24347     combined loss: 4522.23305     paf loss 19.18861     hm loss 4503.04443\n",
      "Iteration:   1760    step:    24352     combined loss: 3534.54765     paf loss 16.55522     hm loss 3517.99243\n",
      "Iteration:   1765    step:    24357     combined loss: 3645.01442     paf loss 19.89601     hm loss 3625.11841\n",
      "Iteration:   1770    step:    24362     combined loss: 3891.95638     paf loss 24.39632     hm loss 3867.56006\n",
      "Iteration:   1775    step:    24367     combined loss: 2797.10199     paf loss 18.80121     hm loss 2778.30078\n",
      "Iteration:   1780    step:    24372     combined loss: 3730.38483     paf loss 24.40948     hm loss 3705.97534\n",
      "Iteration:   1785    step:    24377     combined loss: 3059.02837     paf loss 22.51152     hm loss 3036.51685\n",
      "Iteration:   1790    step:    24382     combined loss: 4487.37306     paf loss 20.13819     hm loss 4467.23486\n",
      "Iteration:   1795    step:    24387     combined loss: 3238.80389     paf loss 23.86395     hm loss 3214.93994\n",
      "Iteration:   1800    step:    24392     combined loss: 4584.20483     paf loss 22.29736     hm loss 4561.90747\n",
      "Iteration:   1805    step:    24397     combined loss: 2271.07288     paf loss 25.95252     hm loss 2245.12036\n",
      "Iteration:   1810    step:    24402     combined loss: 3997.55789     paf loss 22.51151     hm loss 3975.04639\n",
      "Iteration:   1815    step:    24407     combined loss: 4484.82911     paf loss 24.64796     hm loss 4460.18115\n",
      "Iteration:   1820    step:    24412     combined loss: 2851.92849     paf loss 15.36013     hm loss 2836.56836\n",
      "Iteration:   1825    step:    24417     combined loss: 3082.96509     paf loss 20.50610     hm loss 3062.45898\n",
      "Iteration:   1830    step:    24422     combined loss: 2689.64623     paf loss 18.96630     hm loss 2670.67993\n",
      "Iteration:   1835    step:    24427     combined loss: 2814.47612     paf loss 19.80522     hm loss 2794.67090\n",
      "Iteration:   1840    step:    24432     combined loss: 3622.95204     paf loss 25.19606     hm loss 3597.75598\n",
      "Iteration:   1845    step:    24437     combined loss: 3865.46017     paf loss 22.73812     hm loss 3842.72205\n",
      "Iteration:   1850    step:    24442     combined loss: 3798.80292     paf loss 20.87030     hm loss 3777.93262\n",
      "Iteration:   1855    step:    24447     combined loss: 3037.37720     paf loss 18.43922     hm loss 3018.93799\n",
      "Iteration:   1860    step:    24452     combined loss: 3579.27321     paf loss 21.40285     hm loss 3557.87036\n",
      "Iteration:   1865    step:    24457     combined loss: 4390.62674     paf loss 20.64603     hm loss 4369.98071\n",
      "Iteration:   1870    step:    24462     combined loss: 2986.90078     paf loss 14.46401     hm loss 2972.43677\n",
      "Iteration:   1875    step:    24467     combined loss: 2671.14983     paf loss 19.30034     hm loss 2651.84949\n",
      "Iteration:   1880    step:    24472     combined loss: 2230.52405     paf loss 18.58191     hm loss 2211.94214\n",
      "Iteration:   1885    step:    24477     combined loss: 4628.49997     paf loss 25.97433     hm loss 4602.52563\n",
      "Iteration:   1890    step:    24482     combined loss: 3115.51357     paf loss 17.10452     hm loss 3098.40906\n",
      "Iteration:   1895    step:    24487     combined loss: 3354.39810     paf loss 22.86599     hm loss 3331.53210\n",
      "Iteration:   1900    step:    24492     combined loss: 3633.43256     paf loss 31.94904     hm loss 3601.48352\n",
      "Iteration:   1905    step:    24497     combined loss: 3605.48791     paf loss 22.47046     hm loss 3583.01746\n",
      "Iteration:   1910    step:    24502     combined loss: 4215.50171     paf loss 20.69532     hm loss 4194.80640\n",
      "Iteration:   1915    step:    24507     combined loss: 4405.63304     paf loss 21.53953     hm loss 4384.09351\n",
      "Iteration:   1920    step:    24512     combined loss: 5477.78651     paf loss 22.40833     hm loss 5455.37817\n",
      "Iteration:   1925    step:    24517     combined loss: 3457.33176     paf loss 19.10336     hm loss 3438.22839\n",
      "Iteration:   1930    step:    24522     combined loss: 5071.82872     paf loss 24.54210     hm loss 5047.28662\n",
      "Iteration:   1935    step:    24527     combined loss: 2542.09623     paf loss 12.78568     hm loss 2529.31055\n",
      "Iteration:   1940    step:    24532     combined loss: 3100.50452     paf loss 26.07104     hm loss 3074.43347\n",
      "Iteration:   1945    step:    24537     combined loss: 3917.99978     paf loss 22.05703     hm loss 3895.94275\n",
      "Iteration:   1950    step:    24542     combined loss: 4294.31308     paf loss 17.98324     hm loss 4276.32983\n",
      "Iteration:   1955    step:    24547     combined loss: 3246.39224     paf loss 17.31326     hm loss 3229.07898\n",
      "Iteration:   1960    step:    24552     combined loss: 4850.00119     paf loss 21.32785     hm loss 4828.67334\n",
      "Iteration:   1965    step:    24557     combined loss: 2345.83114     paf loss 19.00118     hm loss 2326.82996\n",
      "Iteration:   1970    step:    24562     combined loss: 2186.25086     paf loss 15.60230     hm loss 2170.64856\n",
      "Iteration:   1975    step:    24567     combined loss: 2736.42537     paf loss 18.18990     hm loss 2718.23547\n",
      "Iteration:   1980    step:    24572     combined loss: 3813.12728     paf loss 19.24959     hm loss 3793.87769\n",
      "Iteration:   1985    step:    24577     combined loss: 2663.49243     paf loss 14.93225     hm loss 2648.56018\n",
      "Iteration:   1990    step:    24582     combined loss: 4479.60981     paf loss 22.96894     hm loss 4456.64087\n",
      "Iteration:   1995    step:    24587     combined loss: 2910.80673     paf loss 15.57931     hm loss 2895.22742\n",
      "Iteration:   2000    step:    24592     combined loss: 3263.77792     paf loss 17.79696     hm loss 3245.98096\n",
      "Iteration:   2005    step:    24597     combined loss: 2902.07803     paf loss 17.97891     hm loss 2884.09912\n",
      "Iteration:   2010    step:    24602     combined loss: 3936.23112     paf loss 19.25395     hm loss 3916.97717\n",
      "Iteration:   2015    step:    24607     combined loss: 2555.26388     paf loss 23.34505     hm loss 2531.91882\n",
      "Iteration:   2020    step:    24612     combined loss: 5732.98052     paf loss 23.81426     hm loss 5709.16626\n",
      "Iteration:   2025    step:    24617     combined loss: 4096.51897     paf loss 21.31084     hm loss 4075.20813\n",
      "Iteration:   2030    step:    24622     combined loss: 3919.02822     paf loss 17.38149     hm loss 3901.64673\n",
      "Iteration:   2035    step:    24627     combined loss: 2230.84129     paf loss 15.17649     hm loss 2215.66479\n",
      "Iteration:   2040    step:    24632     combined loss: 2668.17114     paf loss 16.30200     hm loss 2651.86914\n",
      "Iteration:   2045    step:    24637     combined loss: 2521.46155     paf loss 15.52124     hm loss 2505.94031\n",
      "Iteration:   2050    step:    24642     combined loss: 2209.44040     paf loss 14.42502     hm loss 2195.01538\n",
      "Iteration:   2055    step:    24647     combined loss: 2883.61585     paf loss 18.93897     hm loss 2864.67688\n",
      "Iteration:   2060    step:    24652     combined loss: 3517.00148     paf loss 20.83083     hm loss 3496.17065\n",
      "Iteration:   2065    step:    24657     combined loss: 3399.21807     paf loss 20.11200     hm loss 3379.10608\n",
      "Iteration:   2070    step:    24662     combined loss: 4647.76477     paf loss 19.97156     hm loss 4627.79321\n",
      "Iteration:   2075    step:    24667     combined loss: 3438.86772     paf loss 17.94353     hm loss 3420.92419\n",
      "Iteration:   2080    step:    24672     combined loss: 3992.41028     paf loss 24.11230     hm loss 3968.29797\n",
      "Iteration:   2085    step:    24677     combined loss: 4348.74692     paf loss 23.80723     hm loss 4324.93970\n",
      "Iteration:   2090    step:    24682     combined loss: 4201.38070     paf loss 20.20712     hm loss 4181.17358\n",
      "Iteration:   2095    step:    24687     combined loss: 2587.00177     paf loss 14.03680     hm loss 2572.96497\n",
      "Iteration:   2100    step:    24692     combined loss: 3167.25739     paf loss 17.27985     hm loss 3149.97754\n",
      "Iteration:   2105    step:    24697     combined loss: 4101.35224     paf loss 25.77680     hm loss 4075.57544\n",
      "Iteration:   2110    step:    24702     combined loss: 3033.53336     paf loss 20.88200     hm loss 3012.65137\n",
      "Iteration:   2115    step:    24707     combined loss: 3719.18659     paf loss 22.33418     hm loss 3696.85242\n",
      "Iteration:   2120    step:    24712     combined loss: 3196.64301     paf loss 20.83064     hm loss 3175.81238\n",
      "Iteration:   2125    step:    24717     combined loss: 3446.75615     paf loss 25.59124     hm loss 3421.16492\n",
      "Iteration:   2130    step:    24722     combined loss: 3521.94792     paf loss 17.33513     hm loss 3504.61279\n",
      "Iteration:   2135    step:    24727     combined loss: 3216.83612     paf loss 13.90692     hm loss 3202.92920\n",
      "Iteration:   2140    step:    24732     combined loss: 3066.56975     paf loss 18.88067     hm loss 3047.68909\n",
      "Iteration:   2145    step:    24737     combined loss: 3719.54425     paf loss 18.58808     hm loss 3700.95618\n",
      "Iteration:   2150    step:    24742     combined loss: 3164.47367     paf loss 11.74833     hm loss 3152.72534\n",
      "Iteration:   2155    step:    24747     combined loss: 3390.91964     paf loss 21.27254     hm loss 3369.64709\n",
      "Iteration:   2160    step:    24752     combined loss: 2708.00322     paf loss 17.84746     hm loss 2690.15576\n",
      "Iteration:   2165    step:    24757     combined loss: 3142.31410     paf loss 21.02186     hm loss 3121.29224\n",
      "Iteration:   2170    step:    24762     combined loss: 2592.04255     paf loss 16.89241     hm loss 2575.15015\n",
      "Iteration:   2175    step:    24767     combined loss: 4689.14079     paf loss 19.66374     hm loss 4669.47705\n",
      "Iteration:   2180    step:    24772     combined loss: 3225.94187     paf loss 17.87937     hm loss 3208.06250\n",
      "Iteration:   2185    step:    24777     combined loss: 3288.44211     paf loss 17.92490     hm loss 3270.51721\n",
      "Iteration:   2190    step:    24782     combined loss: 5614.42231     paf loss 22.92011     hm loss 5591.50220\n",
      "Iteration:   2195    step:    24787     combined loss: 3505.80250     paf loss 17.90821     hm loss 3487.89429\n",
      "Iteration:   2200    step:    24792     combined loss: 2932.23163     paf loss 16.10028     hm loss 2916.13135\n",
      "Iteration:   2205    step:    24797     combined loss: 4458.86641     paf loss 17.31221     hm loss 4441.55420\n",
      "Iteration:   2210    step:    24802     combined loss: 4362.04871     paf loss 17.16663     hm loss 4344.88208\n",
      "Iteration:   2215    step:    24807     combined loss: 4047.61733     paf loss 15.83047     hm loss 4031.78687\n",
      "Iteration:   2220    step:    24812     combined loss: 3485.86295     paf loss 22.44657     hm loss 3463.41638\n",
      "Iteration:   2225    step:    24817     combined loss: 2649.53305     paf loss 18.22165     hm loss 2631.31140\n",
      "Iteration:   2230    step:    24822     combined loss: 3226.17244     paf loss 22.52180     hm loss 3203.65063\n",
      "Iteration:   2235    step:    24827     combined loss: 2970.65106     paf loss 21.97174     hm loss 2948.67932\n",
      "Iteration:   2240    step:    24832     combined loss: 3229.35831     paf loss 13.67789     hm loss 3215.68042\n",
      "Iteration:   2245    step:    24837     combined loss: 3545.38361     paf loss 16.88728     hm loss 3528.49634\n",
      "Iteration:   2250    step:    24842     combined loss: 2801.55118     paf loss 19.23209     hm loss 2782.31909\n",
      "Iteration:   2255    step:    24847     combined loss: 5594.43569     paf loss 17.62075     hm loss 5576.81494\n",
      "Iteration:   2260    step:    24852     combined loss: 3363.89385     paf loss 17.00774     hm loss 3346.88611\n",
      "Iteration:   2265    step:    24857     combined loss: 3457.29196     paf loss 17.50900     hm loss 3439.78296\n",
      "Iteration:   2270    step:    24862     combined loss: 3595.20856     paf loss 17.27131     hm loss 3577.93726\n",
      "Iteration:   2275    step:    24867     combined loss: 2534.36902     paf loss 19.37268     hm loss 2514.99634\n",
      "Iteration:   2280    step:    24872     combined loss: 3523.05036     paf loss 18.97431     hm loss 3504.07605\n",
      "Iteration:   2285    step:    24877     combined loss: 3385.64814     paf loss 17.63520     hm loss 3368.01294\n",
      "Iteration:   2290    step:    24882     combined loss: 2915.53303     paf loss 24.26057     hm loss 2891.27246\n",
      "Iteration:   2295    step:    24887     combined loss: 2967.70847     paf loss 23.78098     hm loss 2943.92749\n",
      "Iteration:   2300    step:    24892     combined loss: 3235.06984     paf loss 19.71450     hm loss 3215.35535\n",
      "Iteration:   2305    step:    24897     combined loss: 3895.76496     paf loss 26.66828     hm loss 3869.09668\n",
      "Iteration:   2310    step:    24902     combined loss: 3599.37174     paf loss 24.58793     hm loss 3574.78381\n",
      "Iteration:   2315    step:    24907     combined loss: 2581.27456     paf loss 20.67165     hm loss 2560.60291\n",
      "Iteration:   2320    step:    24912     combined loss: 4293.46074     paf loss 14.82475     hm loss 4278.63599\n",
      "Iteration:   2325    step:    24917     combined loss: 5166.28205     paf loss 25.25129     hm loss 5141.03076\n",
      "Iteration:   2330    step:    24922     combined loss: 3442.73971     paf loss 21.89145     hm loss 3420.84827\n",
      "Iteration:   2335    step:    24927     combined loss: 3349.16725     paf loss 17.04433     hm loss 3332.12292\n",
      "Iteration:   2340    step:    24932     combined loss: 3074.99987     paf loss 22.91870     hm loss 3052.08118\n",
      "Iteration:   2345    step:    24937     combined loss: 2617.87582     paf loss 20.56869     hm loss 2597.30713\n",
      "Iteration:   2350    step:    24942     combined loss: 3818.30987     paf loss 27.97491     hm loss 3790.33496\n",
      "Iteration:   2355    step:    24947     combined loss: 2643.66572     paf loss 15.76069     hm loss 2627.90503\n",
      "Iteration:   2360    step:    24952     combined loss: 3753.62351     paf loss 22.30015     hm loss 3731.32336\n",
      "Iteration:   2365    step:    24957     combined loss: 4546.08682     paf loss 17.68325     hm loss 4528.40356\n",
      "Iteration:   2370    step:    24962     combined loss: 3137.77860     paf loss 21.15201     hm loss 3116.62659\n",
      "Iteration:   2375    step:    24967     combined loss: 3238.49794     paf loss 15.84963     hm loss 3222.64832\n",
      "Iteration:   2380    step:    24972     combined loss: 4806.55809     paf loss 17.65355     hm loss 4788.90454\n",
      "Iteration:   2385    step:    24977     combined loss: 3515.76135     paf loss 15.48449     hm loss 3500.27686\n",
      "Iteration:   2390    step:    24982     combined loss: 3705.28936     paf loss 18.79009     hm loss 3686.49927\n",
      "Iteration:   2395    step:    24987     combined loss: 4472.16528     paf loss 20.45825     hm loss 4451.70703\n",
      "Iteration:   2400    step:    24992     combined loss: 3584.15104     paf loss 20.32242     hm loss 3563.82861\n",
      "Iteration:   2405    step:    24997     combined loss: 3292.63358     paf loss 18.53519     hm loss 3274.09839\n",
      "Iteration:   2410    step:    25002     combined loss: 3242.92314     paf loss 17.48015     hm loss 3225.44299\n",
      "Iteration:   2415    step:    25007     combined loss: 3710.61151     paf loss 21.74396     hm loss 3688.86755\n",
      "Iteration:   2420    step:    25012     combined loss: 2634.88728     paf loss 18.57674     hm loss 2616.31055\n",
      "Iteration:   2425    step:    25017     combined loss: 2496.88682     paf loss 16.88463     hm loss 2480.00220\n",
      "Iteration:   2430    step:    25022     combined loss: 3193.88553     paf loss 18.03482     hm loss 3175.85071\n",
      "Iteration:   2435    step:    25027     combined loss: 2976.77643     paf loss 16.32330     hm loss 2960.45312\n",
      "Iteration:   2440    step:    25032     combined loss: 2681.76128     paf loss 21.13469     hm loss 2660.62659\n",
      "Iteration:   2445    step:    25037     combined loss: 4343.56581     paf loss 18.16152     hm loss 4325.40430\n",
      "Iteration:   2450    step:    25042     combined loss: 4331.21236     paf loss 19.67428     hm loss 4311.53809\n",
      "Iteration:   2455    step:    25047     combined loss: 3307.01544     paf loss 16.49359     hm loss 3290.52185\n",
      "Iteration:   2460    step:    25052     combined loss: 2702.09503     paf loss 20.37798     hm loss 2681.71704\n",
      "Iteration:   2465    step:    25057     combined loss: 3848.72762     paf loss 14.91610     hm loss 3833.81152\n",
      "Iteration:   2470    step:    25062     combined loss: 2548.26536     paf loss 18.90256     hm loss 2529.36279\n",
      "Iteration:   2475    step:    25067     combined loss: 3635.08423     paf loss 24.38061     hm loss 3610.70361\n",
      "Iteration:   2480    step:    25072     combined loss: 3308.73467     paf loss 20.47307     hm loss 3288.26160\n",
      "Iteration:   2485    step:    25077     combined loss: 3094.92517     paf loss 18.57446     hm loss 3076.35071\n",
      "Iteration:   2490    step:    25082     combined loss: 2390.81788     paf loss 19.29700     hm loss 2371.52087\n",
      "Iteration:   2495    step:    25087     combined loss: 2644.41633     paf loss 14.92329     hm loss 2629.49304\n",
      "Iteration:   2500    step:    25092     combined loss: 3318.83012     paf loss 18.25493     hm loss 3300.57520\n",
      "Iteration:   2505    step:    25097     combined loss: 3238.19458     paf loss 20.04260     hm loss 3218.15198\n",
      "Iteration:   2510    step:    25102     combined loss: 3406.93469     paf loss 18.19861     hm loss 3388.73608\n",
      "Iteration:   2515    step:    25107     combined loss: 3644.77388     paf loss 16.73543     hm loss 3628.03845\n",
      "Iteration:   2520    step:    25112     combined loss: 3217.24664     paf loss 18.21319     hm loss 3199.03345\n",
      "Iteration:   2525    step:    25117     combined loss: 3409.92485     paf loss 18.72136     hm loss 3391.20349\n",
      "Iteration:   2530    step:    25122     combined loss: 3665.31951     paf loss 26.19402     hm loss 3639.12549\n",
      "Iteration:   2535    step:    25127     combined loss: 3097.27020     paf loss 19.29669     hm loss 3077.97351\n",
      "Iteration:   2540    step:    25132     combined loss: 3017.49838     paf loss 14.91757     hm loss 3002.58081\n",
      "Iteration:   2545    step:    25137     combined loss: 3400.49987     paf loss 20.19384     hm loss 3380.30603\n",
      "Iteration:   2550    step:    25142     combined loss: 2987.96264     paf loss 18.30432     hm loss 2969.65833\n",
      "Iteration:   2555    step:    25147     combined loss: 3452.12616     paf loss 21.86224     hm loss 3430.26392\n",
      "Iteration:   2560    step:    25152     combined loss: 3524.26297     paf loss 17.52835     hm loss 3506.73462\n",
      "Iteration:   2565    step:    25157     combined loss: 3506.57555     paf loss 21.88036     hm loss 3484.69519\n",
      "Iteration:   2570    step:    25162     combined loss: 2281.63017     paf loss 13.00468     hm loss 2268.62549\n",
      "Iteration:   2575    step:    25167     combined loss: 3549.91533     paf loss 18.18168     hm loss 3531.73364\n",
      "Iteration:   2580    step:    25172     combined loss: 3698.29669     paf loss 19.59637     hm loss 3678.70032\n",
      "Iteration:   2585    step:    25177     combined loss: 2459.04332     paf loss 16.18529     hm loss 2442.85803\n",
      "Iteration:   2590    step:    25182     combined loss: 3552.23307     paf loss 19.89726     hm loss 3532.33582\n",
      "Iteration:   2595    step:    25187     combined loss: 5507.94452     paf loss 21.34882     hm loss 5486.59570\n",
      "Iteration:   2600    step:    25192     combined loss: 5171.58833     paf loss 16.88496     hm loss 5154.70337\n",
      "Iteration:   2605    step:    25197     combined loss: 2871.53463     paf loss 20.64108     hm loss 2850.89355\n",
      "Iteration:   2610    step:    25202     combined loss: 3738.33174     paf loss 18.21504     hm loss 3720.11670\n",
      "Iteration:   2615    step:    25207     combined loss: 3092.29882     paf loss 17.11986     hm loss 3075.17896\n",
      "Iteration:   2620    step:    25212     combined loss: 3548.41927     paf loss 20.83675     hm loss 3527.58252\n",
      "Iteration:   2625    step:    25217     combined loss: 3536.44494     paf loss 18.79748     hm loss 3517.64746\n",
      "Iteration:   2630    step:    25222     combined loss: 3087.60817     paf loss 21.27858     hm loss 3066.32959\n",
      "Iteration:   2635    step:    25227     combined loss: 2812.76675     paf loss 24.37148     hm loss 2788.39526\n",
      "Iteration:   2640    step:    25232     combined loss: 6136.12700     paf loss 28.79399     hm loss 6107.33301\n",
      "Iteration:   2645    step:    25237     combined loss: 4068.95896     paf loss 24.21420     hm loss 4044.74475\n",
      "Iteration:   2650    step:    25242     combined loss: 3068.22441     paf loss 17.59636     hm loss 3050.62805\n",
      "Iteration:   2655    step:    25247     combined loss: 2620.08761     paf loss 20.66195     hm loss 2599.42566\n",
      "Iteration:   2660    step:    25252     combined loss: 3801.50622     paf loss 21.57580     hm loss 3779.93042\n",
      "Iteration:   2665    step:    25257     combined loss: 3243.88620     paf loss 19.35128     hm loss 3224.53491\n",
      "Iteration:   2670    step:    25262     combined loss: 4136.02914     paf loss 25.10361     hm loss 4110.92554\n",
      "Iteration:   2675    step:    25267     combined loss: 3187.62435     paf loss 24.53023     hm loss 3163.09412\n",
      "Iteration:   2680    step:    25272     combined loss: 3456.73237     paf loss 21.70881     hm loss 3435.02356\n",
      "Iteration:   2685    step:    25277     combined loss: 4764.43123     paf loss 18.91268     hm loss 4745.51855\n",
      "Iteration:   2690    step:    25282     combined loss: 2836.72032     paf loss 17.10594     hm loss 2819.61438\n",
      "Iteration:   2695    step:    25287     combined loss: 4079.67295     paf loss 20.66856     hm loss 4059.00439\n",
      "Iteration:   2700    step:    25292     combined loss: 2702.60717     paf loss 15.57494     hm loss 2687.03223\n",
      "Iteration:   2705    step:    25297     combined loss: 3241.94243     paf loss 17.87505     hm loss 3224.06738\n",
      "Iteration:   2710    step:    25302     combined loss: 3693.29246     paf loss 15.93724     hm loss 3677.35522\n",
      "Iteration:   2715    step:    25307     combined loss: 2464.69663     paf loss 14.12144     hm loss 2450.57520\n",
      "Iteration:   2720    step:    25312     combined loss: 2828.43794     paf loss 15.22321     hm loss 2813.21472\n",
      "Iteration:   2725    step:    25317     combined loss: 2970.87190     paf loss 21.30232     hm loss 2949.56958\n",
      "Iteration:   2730    step:    25322     combined loss: 3810.15609     paf loss 24.11483     hm loss 3786.04126\n",
      "Iteration:   2735    step:    25327     combined loss: 4353.95328     paf loss 25.37760     hm loss 4328.57568\n",
      "Iteration:   2740    step:    25332     combined loss: 4314.38047     paf loss 20.85215     hm loss 4293.52832\n",
      "Iteration:   2745    step:    25337     combined loss: 2374.94645     paf loss 15.51407     hm loss 2359.43237\n",
      "Iteration:   2750    step:    25342     combined loss: 2795.89796     paf loss 15.23609     hm loss 2780.66187\n",
      "Iteration:   2755    step:    25347     combined loss: 6100.20273     paf loss 32.38510     hm loss 6067.81763\n",
      "Iteration:   2760    step:    25352     combined loss: 3928.92756     paf loss 24.67475     hm loss 3904.25281\n",
      "Iteration:   2765    step:    25357     combined loss: 2991.42028     paf loss 15.65966     hm loss 2975.76062\n",
      "Iteration:   2770    step:    25362     combined loss: 3878.99889     paf loss 14.69627     hm loss 3864.30261\n",
      "Iteration:   2775    step:    25367     combined loss: 3479.16996     paf loss 19.05363     hm loss 3460.11633\n",
      "Iteration:   2780    step:    25372     combined loss: 2119.11028     paf loss 12.39348     hm loss 2106.71680\n",
      "Iteration:   2785    step:    25377     combined loss: 2476.89114     paf loss 19.63125     hm loss 2457.25989\n",
      "Iteration:   2790    step:    25382     combined loss: 2529.09953     paf loss 14.13298     hm loss 2514.96655\n",
      "Iteration:   2795    step:    25387     combined loss: 4198.99944     paf loss 19.75554     hm loss 4179.24390\n",
      "Iteration:   2800    step:    25392     combined loss: 3082.30493     paf loss 17.75086     hm loss 3064.55408\n",
      "Iteration:   2805    step:    25397     combined loss: 3103.67928     paf loss 19.02523     hm loss 3084.65405\n",
      "Iteration:   2810    step:    25402     combined loss: 2360.77388     paf loss 23.87569     hm loss 2336.89819\n",
      "Iteration:   2815    step:    25407     combined loss: 2288.11986     paf loss 17.40929     hm loss 2270.71057\n",
      "Iteration:   2820    step:    25412     combined loss: 4008.69339     paf loss 15.13553     hm loss 3993.55786\n",
      "Train Loss: 3517.5525    PAF Loss:  19.9876    HM Loss:  3497.5649    Acc: NA\n",
      "Val Loss: 3841.1873    PAF Loss:  16.5440    HM Loss:  3824.6433     Acc: NA\n",
      "Epoch 9/9\n",
      "----------\n",
      "Iteration:      0    step:    25416     combined loss: 3065.30288     paf loss 20.15285     hm loss 3045.15002\n",
      "learning rate change: 1.953125e-06 --> 9.765625e-07\n",
      "Iteration:      5    step:    25421     combined loss: 3732.43398     paf loss 19.51003     hm loss 3712.92395\n",
      "Iteration:     10    step:    25426     combined loss: 2475.86463     paf loss 14.23145     hm loss 2461.63318\n",
      "Iteration:     15    step:    25431     combined loss: 4401.03731     paf loss 19.13082     hm loss 4381.90649\n",
      "Iteration:     20    step:    25436     combined loss: 3952.81244     paf loss 22.30011     hm loss 3930.51233\n",
      "Iteration:     25    step:    25441     combined loss: 3388.74484     paf loss 20.43210     hm loss 3368.31274\n",
      "Iteration:     30    step:    25446     combined loss: 4230.14999     paf loss 20.63412     hm loss 4209.51587\n",
      "Iteration:     35    step:    25451     combined loss: 3540.99530     paf loss 22.36138     hm loss 3518.63391\n",
      "Iteration:     40    step:    25456     combined loss: 2940.97250     paf loss 16.44821     hm loss 2924.52429\n",
      "Iteration:     45    step:    25461     combined loss: 3727.63646     paf loss 25.90343     hm loss 3701.73303\n",
      "Iteration:     50    step:    25466     combined loss: 3614.32308     paf loss 21.13497     hm loss 3593.18811\n",
      "Iteration:     55    step:    25471     combined loss: 3647.83877     paf loss 20.62685     hm loss 3627.21191\n",
      "Iteration:     60    step:    25476     combined loss: 2387.58366     paf loss 13.70365     hm loss 2373.88000\n",
      "Iteration:     65    step:    25481     combined loss: 2058.16607     paf loss 13.27081     hm loss 2044.89526\n",
      "Iteration:     70    step:    25486     combined loss: 3972.86399     paf loss 22.09079     hm loss 3950.77319\n",
      "Iteration:     75    step:    25491     combined loss: 4100.37635     paf loss 22.23108     hm loss 4078.14526\n",
      "Iteration:     80    step:    25496     combined loss: 4350.35747     paf loss 21.36186     hm loss 4328.99561\n",
      "Iteration:     85    step:    25501     combined loss: 2835.84442     paf loss 16.66632     hm loss 2819.17810\n",
      "Iteration:     90    step:    25506     combined loss: 3256.20883     paf loss 14.46530     hm loss 3241.74353\n",
      "Iteration:     95    step:    25511     combined loss: 3891.36088     paf loss 16.92131     hm loss 3874.43958\n",
      "Iteration:    100    step:    25516     combined loss: 3466.67866     paf loss 17.70234     hm loss 3448.97632\n",
      "Iteration:    105    step:    25521     combined loss: 3248.71198     paf loss 19.27643     hm loss 3229.43555\n",
      "Iteration:    110    step:    25526     combined loss: 3538.63506     paf loss 22.46282     hm loss 3516.17224\n",
      "Iteration:    115    step:    25531     combined loss: 3338.68870     paf loss 22.66697     hm loss 3316.02173\n",
      "Iteration:    120    step:    25536     combined loss: 3445.88428     paf loss 13.45081     hm loss 3432.43347\n",
      "Iteration:    125    step:    25541     combined loss: 4138.32559     paf loss 20.57852     hm loss 4117.74707\n",
      "Iteration:    130    step:    25546     combined loss: 5387.91191     paf loss 21.92289     hm loss 5365.98901\n",
      "Iteration:    135    step:    25551     combined loss: 3748.16211     paf loss 19.91199     hm loss 3728.25012\n",
      "Iteration:    140    step:    25556     combined loss: 3654.55700     paf loss 22.05200     hm loss 3632.50500\n",
      "Iteration:    145    step:    25561     combined loss: 3544.02141     paf loss 21.47673     hm loss 3522.54468\n",
      "Iteration:    150    step:    25566     combined loss: 4607.62347     paf loss 18.54071     hm loss 4589.08276\n",
      "Iteration:    155    step:    25571     combined loss: 5813.56906     paf loss 24.40475     hm loss 5789.16431\n",
      "Iteration:    160    step:    25576     combined loss: 2665.27899     paf loss 17.39557     hm loss 2647.88342\n",
      "Iteration:    165    step:    25581     combined loss: 3394.42235     paf loss 22.10142     hm loss 3372.32092\n",
      "Iteration:    170    step:    25586     combined loss: 3167.88915     paf loss 17.40624     hm loss 3150.48291\n",
      "Iteration:    175    step:    25591     combined loss: 3576.15138     paf loss 20.79787     hm loss 3555.35352\n",
      "Iteration:    180    step:    25596     combined loss: 3834.98025     paf loss 22.10232     hm loss 3812.87793\n",
      "Iteration:    185    step:    25601     combined loss: 4517.01549     paf loss 25.77403     hm loss 4491.24146\n",
      "Iteration:    190    step:    25606     combined loss: 2528.61960     paf loss 14.03293     hm loss 2514.58667\n",
      "Iteration:    195    step:    25611     combined loss: 2768.30433     paf loss 21.24915     hm loss 2747.05518\n",
      "Iteration:    200    step:    25616     combined loss: 3614.66554     paf loss 20.18935     hm loss 3594.47620\n",
      "Iteration:    205    step:    25621     combined loss: 4626.64693     paf loss 26.94747     hm loss 4599.69946\n",
      "Iteration:    210    step:    25626     combined loss: 4474.04015     paf loss 18.92223     hm loss 4455.11792\n",
      "Iteration:    215    step:    25631     combined loss: 2802.75897     paf loss 16.50079     hm loss 2786.25818\n",
      "Iteration:    220    step:    25636     combined loss: 4169.20336     paf loss 21.22582     hm loss 4147.97754\n",
      "Iteration:    225    step:    25641     combined loss: 2522.72883     paf loss 19.76411     hm loss 2502.96472\n",
      "Iteration:    230    step:    25646     combined loss: 4450.11180     paf loss 19.22703     hm loss 4430.88477\n",
      "Iteration:    235    step:    25651     combined loss: 3625.36313     paf loss 18.00241     hm loss 3607.36072\n",
      "Iteration:    240    step:    25656     combined loss: 3346.94789     paf loss 17.01588     hm loss 3329.93201\n",
      "Iteration:    245    step:    25661     combined loss: 3671.10293     paf loss 24.41238     hm loss 3646.69055\n",
      "Iteration:    250    step:    25666     combined loss: 4161.15544     paf loss 22.14518     hm loss 4139.01025\n",
      "Iteration:    255    step:    25671     combined loss: 3505.70036     paf loss 18.44462     hm loss 3487.25574\n",
      "Iteration:    260    step:    25676     combined loss: 4712.17419     paf loss 22.23888     hm loss 4689.93530\n",
      "Iteration:    265    step:    25681     combined loss: 2833.40890     paf loss 15.15011     hm loss 2818.25879\n",
      "Iteration:    270    step:    25686     combined loss: 5534.62777     paf loss 24.15048     hm loss 5510.47729\n",
      "Iteration:    275    step:    25691     combined loss: 3806.43689     paf loss 20.08264     hm loss 3786.35425\n",
      "Iteration:    280    step:    25696     combined loss: 3471.74131     paf loss 26.09592     hm loss 3445.64539\n",
      "Iteration:    285    step:    25701     combined loss: 4707.06273     paf loss 17.93163     hm loss 4689.13110\n",
      "Iteration:    290    step:    25706     combined loss: 3909.21359     paf loss 30.57308     hm loss 3878.64050\n",
      "Iteration:    295    step:    25711     combined loss: 2998.96843     paf loss 21.27458     hm loss 2977.69385\n",
      "Iteration:    300    step:    25716     combined loss: 3779.06944     paf loss 23.48374     hm loss 3755.58569\n",
      "Iteration:    305    step:    25721     combined loss: 5042.16261     paf loss 20.44117     hm loss 5021.72144\n",
      "Iteration:    310    step:    25726     combined loss: 4205.30849     paf loss 17.81532     hm loss 4187.49316\n",
      "Iteration:    315    step:    25731     combined loss: 2476.56705     paf loss 18.47391     hm loss 2458.09314\n",
      "Iteration:    320    step:    25736     combined loss: 2444.14298     paf loss 14.63028     hm loss 2429.51270\n",
      "Iteration:    325    step:    25741     combined loss: 4063.26985     paf loss 26.76448     hm loss 4036.50537\n",
      "Iteration:    330    step:    25746     combined loss: 3286.61093     paf loss 26.43710     hm loss 3260.17383\n",
      "Iteration:    335    step:    25751     combined loss: 3716.06609     paf loss 27.48528     hm loss 3688.58081\n",
      "Iteration:    340    step:    25756     combined loss: 2474.38699     paf loss 16.19387     hm loss 2458.19312\n",
      "Iteration:    345    step:    25761     combined loss: 2984.51769     paf loss 13.02843     hm loss 2971.48926\n",
      "Iteration:    350    step:    25766     combined loss: 2985.58278     paf loss 20.25515     hm loss 2965.32764\n",
      "Iteration:    355    step:    25771     combined loss: 2763.00889     paf loss 18.82664     hm loss 2744.18225\n",
      "Iteration:    360    step:    25776     combined loss: 3290.27285     paf loss 20.51577     hm loss 3269.75708\n",
      "Iteration:    365    step:    25781     combined loss: 3137.75797     paf loss 21.35636     hm loss 3116.40161\n",
      "Iteration:    370    step:    25786     combined loss: 3338.86812     paf loss 20.08407     hm loss 3318.78406\n",
      "Iteration:    375    step:    25791     combined loss: 3495.39768     paf loss 21.62522     hm loss 3473.77246\n",
      "Iteration:    380    step:    25796     combined loss: 4201.18652     paf loss 16.77954     hm loss 4184.40698\n",
      "Iteration:    385    step:    25801     combined loss: 2666.23089     paf loss 16.43365     hm loss 2649.79724\n",
      "Iteration:    390    step:    25806     combined loss: 3626.86302     paf loss 19.57383     hm loss 3607.28918\n",
      "Iteration:    395    step:    25811     combined loss: 3465.16020     paf loss 19.60539     hm loss 3445.55481\n",
      "Iteration:    400    step:    25816     combined loss: 3183.42460     paf loss 21.19328     hm loss 3162.23132\n",
      "Iteration:    405    step:    25821     combined loss: 3407.32105     paf loss 25.33106     hm loss 3381.98999\n",
      "Iteration:    410    step:    25826     combined loss: 2861.33134     paf loss 21.41044     hm loss 2839.92090\n",
      "Iteration:    415    step:    25831     combined loss: 3034.45955     paf loss 20.36836     hm loss 3014.09119\n",
      "Iteration:    420    step:    25836     combined loss: 2333.77814     paf loss 21.53937     hm loss 2312.23877\n",
      "Iteration:    425    step:    25841     combined loss: 3975.89049     paf loss 23.92247     hm loss 3951.96802\n",
      "Iteration:    430    step:    25846     combined loss: 3443.84700     paf loss 17.26680     hm loss 3426.58020\n",
      "Iteration:    435    step:    25851     combined loss: 2517.91150     paf loss 16.10535     hm loss 2501.80615\n",
      "Iteration:    440    step:    25856     combined loss: 4426.00231     paf loss 20.95714     hm loss 4405.04517\n",
      "Iteration:    445    step:    25861     combined loss: 2891.93272     paf loss 17.54454     hm loss 2874.38818\n",
      "Iteration:    450    step:    25866     combined loss: 3036.93627     paf loss 16.13402     hm loss 3020.80225\n",
      "Iteration:    455    step:    25871     combined loss: 2391.12673     paf loss 18.28005     hm loss 2372.84668\n",
      "Iteration:    460    step:    25876     combined loss: 3461.86048     paf loss 14.95020     hm loss 3446.91028\n",
      "Iteration:    465    step:    25881     combined loss: 2996.25404     paf loss 21.34254     hm loss 2974.91150\n",
      "Iteration:    470    step:    25886     combined loss: 4215.09685     paf loss 15.87420     hm loss 4199.22266\n",
      "Iteration:    475    step:    25891     combined loss: 2860.33148     paf loss 17.80377     hm loss 2842.52771\n",
      "Iteration:    480    step:    25896     combined loss: 3293.94006     paf loss 18.77746     hm loss 3275.16260\n",
      "Iteration:    485    step:    25901     combined loss: 3016.91128     paf loss 22.47464     hm loss 2994.43665\n",
      "Iteration:    490    step:    25906     combined loss: 4044.91850     paf loss 20.95988     hm loss 4023.95862\n",
      "Iteration:    495    step:    25911     combined loss: 4203.38197     paf loss 22.33827     hm loss 4181.04370\n",
      "Iteration:    500    step:    25916     combined loss: 2667.08914     paf loss 16.23294     hm loss 2650.85620\n",
      "Iteration:    505    step:    25921     combined loss: 3619.80432     paf loss 15.45874     hm loss 3604.34558\n",
      "Iteration:    510    step:    25926     combined loss: 3501.98513     paf loss 19.84683     hm loss 3482.13831\n",
      "Iteration:    515    step:    25931     combined loss: 3021.39028     paf loss 14.59157     hm loss 3006.79871\n",
      "Iteration:    520    step:    25936     combined loss: 2839.15417     paf loss 19.43371     hm loss 2819.72046\n",
      "Iteration:    525    step:    25941     combined loss: 2977.61699     paf loss 17.27849     hm loss 2960.33850\n",
      "Iteration:    530    step:    25946     combined loss: 4124.71272     paf loss 24.21772     hm loss 4100.49500\n",
      "Iteration:    535    step:    25951     combined loss: 4055.71098     paf loss 21.85173     hm loss 4033.85925\n",
      "Iteration:    540    step:    25956     combined loss: 6441.52817     paf loss 17.14902     hm loss 6424.37915\n",
      "Iteration:    545    step:    25961     combined loss: 2773.68302     paf loss 15.84843     hm loss 2757.83459\n",
      "Iteration:    550    step:    25966     combined loss: 2457.76060     paf loss 17.48289     hm loss 2440.27771\n",
      "Iteration:    555    step:    25971     combined loss: 3396.02690     paf loss 17.79802     hm loss 3378.22888\n",
      "Iteration:    560    step:    25976     combined loss: 3881.13289     paf loss 21.65499     hm loss 3859.47791\n",
      "Iteration:    565    step:    25981     combined loss: 2779.42045     paf loss 16.77336     hm loss 2762.64709\n",
      "Iteration:    570    step:    25986     combined loss: 3708.86285     paf loss 19.16473     hm loss 3689.69812\n",
      "Iteration:    575    step:    25991     combined loss: 3565.65459     paf loss 16.44255     hm loss 3549.21204\n",
      "Iteration:    580    step:    25996     combined loss: 4574.42117     paf loss 29.26297     hm loss 4545.15820\n",
      "Iteration:    585    step:    26001     combined loss: 2589.30524     paf loss 16.45612     hm loss 2572.84912\n",
      "Iteration:    590    step:    26006     combined loss: 3244.75644     paf loss 25.05319     hm loss 3219.70325\n",
      "Iteration:    595    step:    26011     combined loss: 3888.67762     paf loss 15.37745     hm loss 3873.30017\n",
      "Iteration:    600    step:    26016     combined loss: 4928.14222     paf loss 28.15613     hm loss 4899.98608\n",
      "Iteration:    605    step:    26021     combined loss: 3870.20075     paf loss 19.10017     hm loss 3851.10059\n",
      "Iteration:    610    step:    26026     combined loss: 2841.88829     paf loss 16.97606     hm loss 2824.91223\n",
      "Iteration:    615    step:    26031     combined loss: 4978.73714     paf loss 13.67683     hm loss 4965.06030\n",
      "Iteration:    620    step:    26036     combined loss: 3701.18768     paf loss 20.58685     hm loss 3680.60083\n",
      "Iteration:    625    step:    26041     combined loss: 3156.62063     paf loss 19.97707     hm loss 3136.64355\n",
      "Iteration:    630    step:    26046     combined loss: 3159.82968     paf loss 16.57833     hm loss 3143.25134\n",
      "Iteration:    635    step:    26051     combined loss: 3706.21029     paf loss 27.12960     hm loss 3679.08069\n",
      "Iteration:    640    step:    26056     combined loss: 3691.03781     paf loss 25.65145     hm loss 3665.38635\n",
      "Iteration:    645    step:    26061     combined loss: 3142.90552     paf loss 14.74073     hm loss 3128.16479\n",
      "Iteration:    650    step:    26066     combined loss: 2706.93185     paf loss 14.97054     hm loss 2691.96130\n",
      "Iteration:    655    step:    26071     combined loss: 4732.40783     paf loss 21.64563     hm loss 4710.76221\n",
      "Iteration:    660    step:    26076     combined loss: 2602.87091     paf loss 18.83002     hm loss 2584.04089\n",
      "Iteration:    665    step:    26081     combined loss: 2688.36338     paf loss 15.82798     hm loss 2672.53540\n",
      "Iteration:    670    step:    26086     combined loss: 3526.82140     paf loss 21.72912     hm loss 3505.09229\n",
      "Iteration:    675    step:    26091     combined loss: 2543.13473     paf loss 15.06002     hm loss 2528.07471\n",
      "Iteration:    680    step:    26096     combined loss: 2269.36560     paf loss 16.83216     hm loss 2252.53345\n",
      "Iteration:    685    step:    26101     combined loss: 2827.35173     paf loss 23.33928     hm loss 2804.01245\n",
      "Iteration:    690    step:    26106     combined loss: 2403.91440     paf loss 14.92526     hm loss 2388.98914\n",
      "Iteration:    695    step:    26111     combined loss: 4858.93691     paf loss 25.73110     hm loss 4833.20581\n",
      "Iteration:    700    step:    26116     combined loss: 3390.22746     paf loss 15.89311     hm loss 3374.33435\n",
      "Iteration:    705    step:    26121     combined loss: 3730.05200     paf loss 20.83887     hm loss 3709.21313\n",
      "Iteration:    710    step:    26126     combined loss: 4155.03215     paf loss 22.05827     hm loss 4132.97388\n",
      "Iteration:    715    step:    26131     combined loss: 3354.27956     paf loss 15.88832     hm loss 3338.39124\n",
      "Iteration:    720    step:    26136     combined loss: 3884.36797     paf loss 18.50823     hm loss 3865.85974\n",
      "Iteration:    725    step:    26141     combined loss: 2475.92319     paf loss 13.28257     hm loss 2462.64062\n",
      "Iteration:    730    step:    26146     combined loss: 3718.81552     paf loss 21.48447     hm loss 3697.33105\n",
      "Iteration:    735    step:    26151     combined loss: 3419.37536     paf loss 23.47729     hm loss 3395.89807\n",
      "Iteration:    740    step:    26156     combined loss: 2805.93623     paf loss 17.52729     hm loss 2788.40894\n",
      "Iteration:    745    step:    26161     combined loss: 4155.93947     paf loss 21.69533     hm loss 4134.24414\n",
      "Iteration:    750    step:    26166     combined loss: 4346.61252     paf loss 19.39817     hm loss 4327.21436\n",
      "Iteration:    755    step:    26171     combined loss: 5949.51614     paf loss 27.80447     hm loss 5921.71167\n",
      "Iteration:    760    step:    26176     combined loss: 3049.61323     paf loss 18.56160     hm loss 3031.05164\n",
      "Iteration:    765    step:    26181     combined loss: 3161.13941     paf loss 19.37085     hm loss 3141.76855\n",
      "Iteration:    770    step:    26186     combined loss: 3383.20088     paf loss 20.70942     hm loss 3362.49146\n",
      "Iteration:    775    step:    26191     combined loss: 3276.29877     paf loss 14.90473     hm loss 3261.39404\n",
      "Iteration:    780    step:    26196     combined loss: 4255.00478     paf loss 29.40199     hm loss 4225.60278\n",
      "Iteration:    785    step:    26201     combined loss: 4609.36988     paf loss 17.85743     hm loss 4591.51245\n",
      "Iteration:    790    step:    26206     combined loss: 3305.40228     paf loss 18.52129     hm loss 3286.88098\n",
      "Iteration:    795    step:    26211     combined loss: 2721.50944     paf loss 22.84952     hm loss 2698.65991\n",
      "Iteration:    800    step:    26216     combined loss: 3366.79797     paf loss 20.52075     hm loss 3346.27722\n",
      "Iteration:    805    step:    26221     combined loss: 3664.36890     paf loss 23.55518     hm loss 3640.81372\n",
      "Iteration:    810    step:    26226     combined loss: 2771.68810     paf loss 14.66759     hm loss 2757.02051\n",
      "Iteration:    815    step:    26231     combined loss: 4003.58242     paf loss 18.06753     hm loss 3985.51489\n",
      "Iteration:    820    step:    26236     combined loss: 4581.48833     paf loss 18.28423     hm loss 4563.20410\n",
      "Iteration:    825    step:    26241     combined loss: 4043.30255     paf loss 17.73528     hm loss 4025.56726\n",
      "Iteration:    830    step:    26246     combined loss: 3330.69182     paf loss 19.37797     hm loss 3311.31384\n",
      "Iteration:    835    step:    26251     combined loss: 3851.65828     paf loss 18.80367     hm loss 3832.85461\n",
      "Iteration:    840    step:    26256     combined loss: 2351.28923     paf loss 15.60429     hm loss 2335.68494\n",
      "Iteration:    845    step:    26261     combined loss: 4171.04801     paf loss 21.23136     hm loss 4149.81665\n",
      "Iteration:    850    step:    26266     combined loss: 5088.03354     paf loss 16.57260     hm loss 5071.46094\n",
      "Iteration:    855    step:    26271     combined loss: 2878.01679     paf loss 23.14667     hm loss 2854.87012\n",
      "Iteration:    860    step:    26276     combined loss: 2942.67642     paf loss 14.34513     hm loss 2928.33130\n",
      "Iteration:    865    step:    26281     combined loss: 2594.83541     paf loss 17.48446     hm loss 2577.35095\n",
      "Iteration:    870    step:    26286     combined loss: 2671.92432     paf loss 18.76526     hm loss 2653.15906\n",
      "Iteration:    875    step:    26291     combined loss: 3506.63529     paf loss 18.61247     hm loss 3488.02283\n",
      "Iteration:    880    step:    26296     combined loss: 2957.01187     paf loss 21.82522     hm loss 2935.18665\n",
      "Iteration:    885    step:    26301     combined loss: 4177.55911     paf loss 22.65530     hm loss 4154.90381\n",
      "Iteration:    890    step:    26306     combined loss: 4238.34968     paf loss 18.84870     hm loss 4219.50098\n",
      "Iteration:    895    step:    26311     combined loss: 3668.09752     paf loss 26.01683     hm loss 3642.08069\n",
      "Iteration:    900    step:    26316     combined loss: 3588.70030     paf loss 18.15160     hm loss 3570.54871\n",
      "Iteration:    905    step:    26321     combined loss: 3008.61739     paf loss 17.08272     hm loss 2991.53467\n",
      "Iteration:    910    step:    26326     combined loss: 2580.62958     paf loss 17.04511     hm loss 2563.58447\n",
      "Iteration:    915    step:    26331     combined loss: 4534.64284     paf loss 21.41505     hm loss 4513.22778\n",
      "Iteration:    920    step:    26336     combined loss: 2580.90068     paf loss 22.34209     hm loss 2558.55859\n",
      "Iteration:    925    step:    26341     combined loss: 2429.62780     paf loss 16.47948     hm loss 2413.14832\n",
      "Iteration:    930    step:    26346     combined loss: 2955.97561     paf loss 15.65944     hm loss 2940.31616\n",
      "Iteration:    935    step:    26351     combined loss: 2554.97083     paf loss 16.22559     hm loss 2538.74524\n",
      "Iteration:    940    step:    26356     combined loss: 3184.97095     paf loss 18.71204     hm loss 3166.25891\n",
      "Iteration:    945    step:    26361     combined loss: 3843.57771     paf loss 19.71296     hm loss 3823.86475\n",
      "Iteration:    950    step:    26366     combined loss: 3078.72784     paf loss 19.09271     hm loss 3059.63513\n",
      "Iteration:    955    step:    26371     combined loss: 3085.06019     paf loss 23.39246     hm loss 3061.66772\n",
      "Iteration:    960    step:    26376     combined loss: 2477.01507     paf loss 17.25371     hm loss 2459.76135\n",
      "Iteration:    965    step:    26381     combined loss: 3108.08669     paf loss 19.08169     hm loss 3089.00500\n",
      "Iteration:    970    step:    26386     combined loss: 4842.92747     paf loss 25.28806     hm loss 4817.63940\n",
      "Iteration:    975    step:    26391     combined loss: 3993.53812     paf loss 22.45450     hm loss 3971.08362\n",
      "Iteration:    980    step:    26396     combined loss: 4826.44868     paf loss 20.73383     hm loss 4805.71484\n",
      "Iteration:    985    step:    26401     combined loss: 3589.17676     paf loss 20.05750     hm loss 3569.11926\n",
      "Iteration:    990    step:    26406     combined loss: 3279.03134     paf loss 15.56100     hm loss 3263.47034\n",
      "Iteration:    995    step:    26411     combined loss: 2965.21066     paf loss 19.85006     hm loss 2945.36060\n",
      "Iteration:   1000    step:    26416     combined loss: 2456.03858     paf loss 16.34449     hm loss 2439.69409\n",
      "Iteration:   1005    step:    26421     combined loss: 5303.94326     paf loss 24.73696     hm loss 5279.20630\n",
      "Iteration:   1010    step:    26426     combined loss: 4432.75411     paf loss 22.66988     hm loss 4410.08423\n",
      "Iteration:   1015    step:    26431     combined loss: 3394.46207     paf loss 20.54044     hm loss 3373.92163\n",
      "Iteration:   1020    step:    26436     combined loss: 3080.81564     paf loss 13.97665     hm loss 3066.83899\n",
      "Iteration:   1025    step:    26441     combined loss: 3110.63718     paf loss 26.30954     hm loss 3084.32764\n",
      "Iteration:   1030    step:    26446     combined loss: 4356.53632     paf loss 22.33540     hm loss 4334.20093\n",
      "Iteration:   1035    step:    26451     combined loss: 3768.83337     paf loss 19.81298     hm loss 3749.02039\n",
      "Iteration:   1040    step:    26456     combined loss: 3157.80720     paf loss 27.83711     hm loss 3129.97009\n",
      "Iteration:   1045    step:    26461     combined loss: 3239.91456     paf loss 20.18800     hm loss 3219.72656\n",
      "Iteration:   1050    step:    26466     combined loss: 2894.00439     paf loss 18.25085     hm loss 2875.75354\n",
      "Iteration:   1055    step:    26471     combined loss: 2591.18191     paf loss 13.62771     hm loss 2577.55420\n",
      "Iteration:   1060    step:    26476     combined loss: 3065.03509     paf loss 18.27801     hm loss 3046.75708\n",
      "Iteration:   1065    step:    26481     combined loss: 6029.99496     paf loss 29.70223     hm loss 6000.29272\n",
      "Iteration:   1070    step:    26486     combined loss: 3832.89837     paf loss 25.34539     hm loss 3807.55298\n",
      "Iteration:   1075    step:    26491     combined loss: 3065.79652     paf loss 14.44996     hm loss 3051.34656\n",
      "Iteration:   1080    step:    26496     combined loss: 4576.60426     paf loss 25.94825     hm loss 4550.65601\n",
      "Iteration:   1085    step:    26501     combined loss: 3566.98409     paf loss 15.87215     hm loss 3551.11194\n",
      "Iteration:   1090    step:    26506     combined loss: 2462.27287     paf loss 15.22405     hm loss 2447.04883\n",
      "Iteration:   1095    step:    26511     combined loss: 5343.01120     paf loss 19.77390     hm loss 5323.23730\n",
      "Iteration:   1100    step:    26516     combined loss: 4168.64765     paf loss 26.91230     hm loss 4141.73535\n",
      "Iteration:   1105    step:    26521     combined loss: 3381.65252     paf loss 21.48113     hm loss 3360.17139\n",
      "Iteration:   1110    step:    26526     combined loss: 3777.06547     paf loss 20.51530     hm loss 3756.55017\n",
      "Iteration:   1115    step:    26531     combined loss: 4780.79694     paf loss 25.80451     hm loss 4754.99243\n",
      "Iteration:   1120    step:    26536     combined loss: 4338.18588     paf loss 23.56845     hm loss 4314.61743\n",
      "Iteration:   1125    step:    26541     combined loss: 3677.89604     paf loss 20.76555     hm loss 3657.13049\n",
      "Iteration:   1130    step:    26546     combined loss: 3893.27074     paf loss 18.91295     hm loss 3874.35779\n",
      "Iteration:   1135    step:    26551     combined loss: 3689.62098     paf loss 20.07203     hm loss 3669.54895\n",
      "Iteration:   1140    step:    26556     combined loss: 2915.25968     paf loss 19.47440     hm loss 2895.78528\n",
      "Iteration:   1145    step:    26561     combined loss: 3295.43422     paf loss 27.94521     hm loss 3267.48901\n",
      "Iteration:   1150    step:    26566     combined loss: 3362.86476     paf loss 22.38881     hm loss 3340.47595\n",
      "Iteration:   1155    step:    26571     combined loss: 3384.09328     paf loss 24.47329     hm loss 3359.62000\n",
      "Iteration:   1160    step:    26576     combined loss: 2806.77417     paf loss 23.26270     hm loss 2783.51147\n",
      "Iteration:   1165    step:    26581     combined loss: 3468.37964     paf loss 22.51062     hm loss 3445.86902\n",
      "Iteration:   1170    step:    26586     combined loss: 3082.45347     paf loss 18.42564     hm loss 3064.02783\n",
      "Iteration:   1175    step:    26591     combined loss: 3700.09129     paf loss 16.06969     hm loss 3684.02161\n",
      "Iteration:   1180    step:    26596     combined loss: 3293.67506     paf loss 19.50318     hm loss 3274.17188\n",
      "Iteration:   1185    step:    26601     combined loss: 3766.02094     paf loss 18.29133     hm loss 3747.72961\n",
      "Iteration:   1190    step:    26606     combined loss: 4361.73363     paf loss 22.34276     hm loss 4339.39087\n",
      "Iteration:   1195    step:    26611     combined loss: 3273.47770     paf loss 16.89543     hm loss 3256.58228\n",
      "Iteration:   1200    step:    26616     combined loss: 3110.28400     paf loss 24.06623     hm loss 3086.21777\n",
      "Iteration:   1205    step:    26621     combined loss: 4145.61892     paf loss 20.53152     hm loss 4125.08740\n",
      "Iteration:   1210    step:    26626     combined loss: 5384.29785     paf loss 19.19580     hm loss 5365.10205\n",
      "Iteration:   1215    step:    26631     combined loss: 3500.09551     paf loss 25.46404     hm loss 3474.63147\n",
      "Iteration:   1220    step:    26636     combined loss: 3250.09376     paf loss 19.24305     hm loss 3230.85071\n",
      "Iteration:   1225    step:    26641     combined loss: 2937.94567     paf loss 23.36828     hm loss 2914.57739\n",
      "Iteration:   1230    step:    26646     combined loss: 3922.54757     paf loss 19.09713     hm loss 3903.45044\n",
      "Iteration:   1235    step:    26651     combined loss: 2571.71500     paf loss 16.91190     hm loss 2554.80310\n",
      "Iteration:   1240    step:    26656     combined loss: 2643.95743     paf loss 14.68643     hm loss 2629.27100\n",
      "Iteration:   1245    step:    26661     combined loss: 3897.47951     paf loss 22.60304     hm loss 3874.87646\n",
      "Iteration:   1250    step:    26666     combined loss: 3785.12239     paf loss 20.89168     hm loss 3764.23071\n",
      "Iteration:   1255    step:    26671     combined loss: 3507.52359     paf loss 22.30471     hm loss 3485.21887\n",
      "Iteration:   1260    step:    26676     combined loss: 4107.98377     paf loss 22.53016     hm loss 4085.45361\n",
      "Iteration:   1265    step:    26681     combined loss: 4982.45565     paf loss 25.31967     hm loss 4957.13599\n",
      "Iteration:   1270    step:    26686     combined loss: 3411.47683     paf loss 21.84707     hm loss 3389.62976\n",
      "Iteration:   1275    step:    26691     combined loss: 2921.21591     paf loss 18.51974     hm loss 2902.69617\n",
      "Iteration:   1280    step:    26696     combined loss: 2320.22188     paf loss 16.52156     hm loss 2303.70032\n",
      "Iteration:   1285    step:    26701     combined loss: 3780.34817     paf loss 21.61379     hm loss 3758.73438\n",
      "Iteration:   1290    step:    26706     combined loss: 1973.77041     paf loss 16.59854     hm loss 1957.17188\n",
      "Iteration:   1295    step:    26711     combined loss: 4063.44875     paf loss 20.88613     hm loss 4042.56262\n",
      "Iteration:   1300    step:    26716     combined loss: 3651.78278     paf loss 16.18658     hm loss 3635.59619\n",
      "Iteration:   1305    step:    26721     combined loss: 3548.48175     paf loss 19.40473     hm loss 3529.07703\n",
      "Iteration:   1310    step:    26726     combined loss: 4676.28073     paf loss 17.49191     hm loss 4658.78882\n",
      "Iteration:   1315    step:    26731     combined loss: 4821.53829     paf loss 24.88204     hm loss 4796.65625\n",
      "Iteration:   1320    step:    26736     combined loss: 4031.03876     paf loss 24.25654     hm loss 4006.78223\n",
      "Iteration:   1325    step:    26741     combined loss: 2908.14054     paf loss 23.00601     hm loss 2885.13452\n",
      "Iteration:   1330    step:    26746     combined loss: 3772.03930     paf loss 17.61376     hm loss 3754.42554\n",
      "Iteration:   1335    step:    26751     combined loss: 3891.99198     paf loss 20.45280     hm loss 3871.53918\n",
      "Iteration:   1340    step:    26756     combined loss: 5136.69842     paf loss 23.16766     hm loss 5113.53076\n",
      "Iteration:   1345    step:    26761     combined loss: 4270.26639     paf loss 21.57351     hm loss 4248.69287\n",
      "Iteration:   1350    step:    26766     combined loss: 3713.37734     paf loss 21.48037     hm loss 3691.89697\n",
      "Iteration:   1355    step:    26771     combined loss: 3637.98081     paf loss 27.36325     hm loss 3610.61755\n",
      "Iteration:   1360    step:    26776     combined loss: 2971.91129     paf loss 17.39970     hm loss 2954.51160\n",
      "Iteration:   1365    step:    26781     combined loss: 3817.73388     paf loss 21.52099     hm loss 3796.21289\n",
      "Iteration:   1370    step:    26786     combined loss: 2782.92853     paf loss 18.60554     hm loss 2764.32300\n",
      "Iteration:   1375    step:    26791     combined loss: 3355.47585     paf loss 14.72084     hm loss 3340.75500\n",
      "Iteration:   1380    step:    26796     combined loss: 4253.07002     paf loss 22.98188     hm loss 4230.08813\n",
      "Iteration:   1385    step:    26801     combined loss: 3771.68383     paf loss 22.46739     hm loss 3749.21643\n",
      "Iteration:   1390    step:    26806     combined loss: 2945.39024     paf loss 15.30845     hm loss 2930.08179\n",
      "Iteration:   1395    step:    26811     combined loss: 4924.33332     paf loss 23.74372     hm loss 4900.58960\n",
      "Iteration:   1400    step:    26816     combined loss: 3681.20193     paf loss 17.65737     hm loss 3663.54456\n",
      "Iteration:   1405    step:    26821     combined loss: 4431.96433     paf loss 21.57859     hm loss 4410.38574\n",
      "Iteration:   1410    step:    26826     combined loss: 3358.17868     paf loss 15.37985     hm loss 3342.79883\n",
      "Iteration:   1415    step:    26831     combined loss: 3951.41957     paf loss 17.86988     hm loss 3933.54968\n",
      "Iteration:   1420    step:    26836     combined loss: 2852.79250     paf loss 20.87331     hm loss 2831.91919\n",
      "Iteration:   1425    step:    26841     combined loss: 4957.64263     paf loss 26.30474     hm loss 4931.33789\n",
      "Iteration:   1430    step:    26846     combined loss: 2352.89036     paf loss 14.17222     hm loss 2338.71814\n",
      "Iteration:   1435    step:    26851     combined loss: 3315.11918     paf loss 15.19621     hm loss 3299.92297\n",
      "Iteration:   1440    step:    26856     combined loss: 3837.22969     paf loss 18.61043     hm loss 3818.61926\n",
      "Iteration:   1445    step:    26861     combined loss: 3024.85676     paf loss 16.35750     hm loss 3008.49927\n",
      "Iteration:   1450    step:    26866     combined loss: 3710.80296     paf loss 22.47947     hm loss 3688.32349\n",
      "Iteration:   1455    step:    26871     combined loss: 3370.71441     paf loss 16.50885     hm loss 3354.20557\n",
      "Iteration:   1460    step:    26876     combined loss: 3189.08283     paf loss 20.48627     hm loss 3168.59656\n",
      "Iteration:   1465    step:    26881     combined loss: 2848.82373     paf loss 24.21899     hm loss 2824.60474\n",
      "Iteration:   1470    step:    26886     combined loss: 3863.61651     paf loss 21.35430     hm loss 3842.26221\n",
      "Iteration:   1475    step:    26891     combined loss: 3038.04767     paf loss 19.02838     hm loss 3019.01929\n",
      "Iteration:   1480    step:    26896     combined loss: 2464.74765     paf loss 17.00400     hm loss 2447.74365\n",
      "Iteration:   1485    step:    26901     combined loss: 3472.70279     paf loss 16.86600     hm loss 3455.83679\n",
      "Iteration:   1490    step:    26906     combined loss: 2434.71022     paf loss 17.27516     hm loss 2417.43506\n",
      "Iteration:   1495    step:    26911     combined loss: 2230.80655     paf loss 15.84513     hm loss 2214.96143\n",
      "Iteration:   1500    step:    26916     combined loss: 4693.53859     paf loss 26.97096     hm loss 4666.56763\n",
      "Iteration:   1505    step:    26921     combined loss: 2916.62522     paf loss 13.15208     hm loss 2903.47314\n",
      "Iteration:   1510    step:    26926     combined loss: 4113.49288     paf loss 21.13363     hm loss 4092.35925\n",
      "Iteration:   1515    step:    26931     combined loss: 2147.93965     paf loss 15.13264     hm loss 2132.80701\n",
      "Iteration:   1520    step:    26936     combined loss: 3186.40868     paf loss 23.18358     hm loss 3163.22510\n",
      "Iteration:   1525    step:    26941     combined loss: 4116.14833     paf loss 23.79518     hm loss 4092.35315\n",
      "Iteration:   1530    step:    26946     combined loss: 3229.65467     paf loss 19.31214     hm loss 3210.34253\n",
      "Iteration:   1535    step:    26951     combined loss: 2987.14026     paf loss 21.59985     hm loss 2965.54041\n",
      "Iteration:   1540    step:    26956     combined loss: 3904.43085     paf loss 16.77728     hm loss 3887.65356\n",
      "Iteration:   1545    step:    26961     combined loss: 2226.00952     paf loss 20.24329     hm loss 2205.76624\n",
      "Iteration:   1550    step:    26966     combined loss: 3019.82083     paf loss 17.06155     hm loss 3002.75928\n",
      "Iteration:   1555    step:    26971     combined loss: 3807.24988     paf loss 16.69409     hm loss 3790.55579\n",
      "Iteration:   1560    step:    26976     combined loss: 2802.22611     paf loss 14.59904     hm loss 2787.62708\n",
      "Iteration:   1565    step:    26981     combined loss: 3641.37936     paf loss 18.15292     hm loss 3623.22644\n",
      "Iteration:   1570    step:    26986     combined loss: 4083.64427     paf loss 23.49913     hm loss 4060.14514\n",
      "Iteration:   1575    step:    26991     combined loss: 2419.26539     paf loss 14.82704     hm loss 2404.43835\n",
      "Iteration:   1580    step:    26996     combined loss: 3530.56246     paf loss 18.62716     hm loss 3511.93530\n",
      "Iteration:   1585    step:    27001     combined loss: 3507.01659     paf loss 15.78002     hm loss 3491.23657\n",
      "Iteration:   1590    step:    27006     combined loss: 2734.85433     paf loss 18.70052     hm loss 2716.15381\n",
      "Iteration:   1595    step:    27011     combined loss: 2886.76045     paf loss 16.91402     hm loss 2869.84644\n",
      "Iteration:   1600    step:    27016     combined loss: 2352.97453     paf loss 14.47416     hm loss 2338.50037\n",
      "Iteration:   1605    step:    27021     combined loss: 4394.11816     paf loss 28.33569     hm loss 4365.78247\n",
      "Iteration:   1610    step:    27026     combined loss: 4070.13728     paf loss 25.74104     hm loss 4044.39624\n",
      "Iteration:   1615    step:    27031     combined loss: 3700.95606     paf loss 20.30847     hm loss 3680.64758\n",
      "Iteration:   1620    step:    27036     combined loss: 2986.94092     paf loss 16.08203     hm loss 2970.85889\n",
      "Iteration:   1625    step:    27041     combined loss: 4297.41729     paf loss 23.68731     hm loss 4273.72998\n",
      "Iteration:   1630    step:    27046     combined loss: 3730.73897     paf loss 27.62825     hm loss 3703.11072\n",
      "Iteration:   1635    step:    27051     combined loss: 4075.19128     paf loss 18.06200     hm loss 4057.12927\n",
      "Iteration:   1640    step:    27056     combined loss: 4148.35988     paf loss 20.26393     hm loss 4128.09595\n",
      "Iteration:   1645    step:    27061     combined loss: 2564.57249     paf loss 20.12791     hm loss 2544.44458\n",
      "Iteration:   1650    step:    27066     combined loss: 3814.75722     paf loss 19.73049     hm loss 3795.02673\n",
      "Iteration:   1655    step:    27071     combined loss: 2532.53863     paf loss 20.45892     hm loss 2512.07971\n",
      "Iteration:   1660    step:    27076     combined loss: 2825.74523     paf loss 18.81798     hm loss 2806.92725\n",
      "Iteration:   1665    step:    27081     combined loss: 4728.72503     paf loss 23.60369     hm loss 4705.12134\n",
      "Iteration:   1670    step:    27086     combined loss: 3537.48777     paf loss 17.73643     hm loss 3519.75134\n",
      "Iteration:   1675    step:    27091     combined loss: 3069.54312     paf loss 22.99576     hm loss 3046.54736\n",
      "Iteration:   1680    step:    27096     combined loss: 3414.37650     paf loss 16.14188     hm loss 3398.23462\n",
      "Iteration:   1685    step:    27101     combined loss: 2334.28306     paf loss 17.83555     hm loss 2316.44751\n",
      "Iteration:   1690    step:    27106     combined loss: 3770.01472     paf loss 21.49152     hm loss 3748.52319\n",
      "Iteration:   1695    step:    27111     combined loss: 3261.76744     paf loss 14.61900     hm loss 3247.14844\n",
      "Iteration:   1700    step:    27116     combined loss: 2885.16719     paf loss 22.25521     hm loss 2862.91199\n",
      "Iteration:   1705    step:    27121     combined loss: 5060.49015     paf loss 19.15372     hm loss 5041.33643\n",
      "Iteration:   1710    step:    27126     combined loss: 3187.01473     paf loss 21.81331     hm loss 3165.20142\n",
      "Iteration:   1715    step:    27131     combined loss: 3444.43579     paf loss 15.32116     hm loss 3429.11462\n",
      "Iteration:   1720    step:    27136     combined loss: 3307.94557     paf loss 21.79335     hm loss 3286.15222\n",
      "Iteration:   1725    step:    27141     combined loss: 4411.98237     paf loss 22.22846     hm loss 4389.75391\n",
      "Iteration:   1730    step:    27146     combined loss: 4040.32178     paf loss 23.29675     hm loss 4017.02502\n",
      "Iteration:   1735    step:    27151     combined loss: 3537.12882     paf loss 16.99368     hm loss 3520.13513\n",
      "Iteration:   1740    step:    27156     combined loss: 2287.14267     paf loss 14.78805     hm loss 2272.35461\n",
      "Iteration:   1745    step:    27161     combined loss: 3795.24876     paf loss 20.42503     hm loss 3774.82373\n",
      "Iteration:   1750    step:    27166     combined loss: 3287.59249     paf loss 14.42599     hm loss 3273.16650\n",
      "Iteration:   1755    step:    27171     combined loss: 3724.89927     paf loss 21.20872     hm loss 3703.69055\n",
      "Iteration:   1760    step:    27176     combined loss: 3359.05876     paf loss 16.46233     hm loss 3342.59644\n",
      "Iteration:   1765    step:    27181     combined loss: 2649.31478     paf loss 17.14486     hm loss 2632.16992\n",
      "Iteration:   1770    step:    27186     combined loss: 3236.27379     paf loss 20.45995     hm loss 3215.81384\n",
      "Iteration:   1775    step:    27191     combined loss: 2853.46451     paf loss 20.32547     hm loss 2833.13904\n",
      "Iteration:   1780    step:    27196     combined loss: 4113.97491     paf loss 19.58087     hm loss 4094.39404\n",
      "Iteration:   1785    step:    27201     combined loss: 2319.57639     paf loss 16.85337     hm loss 2302.72302\n",
      "Iteration:   1790    step:    27206     combined loss: 3811.94546     paf loss 22.36124     hm loss 3789.58423\n",
      "Iteration:   1795    step:    27211     combined loss: 3947.41790     paf loss 21.81597     hm loss 3925.60193\n",
      "Iteration:   1800    step:    27216     combined loss: 3037.16321     paf loss 19.02783     hm loss 3018.13538\n",
      "Iteration:   1805    step:    27221     combined loss: 2924.71450     paf loss 19.15676     hm loss 2905.55774\n",
      "Iteration:   1810    step:    27226     combined loss: 2036.46993     paf loss 11.24618     hm loss 2025.22375\n",
      "Iteration:   1815    step:    27231     combined loss: 2866.54507     paf loss 16.09353     hm loss 2850.45154\n",
      "Iteration:   1820    step:    27236     combined loss: 2385.78593     paf loss 18.21122     hm loss 2367.57471\n",
      "Iteration:   1825    step:    27241     combined loss: 3334.93010     paf loss 22.28557     hm loss 3312.64453\n",
      "Iteration:   1830    step:    27246     combined loss: 2928.33964     paf loss 20.19719     hm loss 2908.14246\n",
      "Iteration:   1835    step:    27251     combined loss: 4920.98738     paf loss 20.93465     hm loss 4900.05273\n",
      "Iteration:   1840    step:    27256     combined loss: 3724.70915     paf loss 24.51823     hm loss 3700.19092\n",
      "Iteration:   1845    step:    27261     combined loss: 3170.42991     paf loss 20.88536     hm loss 3149.54456\n",
      "Iteration:   1850    step:    27266     combined loss: 4906.65323     paf loss 27.83341     hm loss 4878.81982\n",
      "Iteration:   1855    step:    27271     combined loss: 4399.77419     paf loss 24.27004     hm loss 4375.50415\n",
      "Iteration:   1860    step:    27276     combined loss: 2355.77773     paf loss 17.27394     hm loss 2338.50378\n",
      "Iteration:   1865    step:    27281     combined loss: 2724.77297     paf loss 17.00295     hm loss 2707.77002\n",
      "Iteration:   1870    step:    27286     combined loss: 2777.70948     paf loss 17.36341     hm loss 2760.34607\n",
      "Iteration:   1875    step:    27291     combined loss: 3538.71323     paf loss 22.86911     hm loss 3515.84412\n",
      "Iteration:   1880    step:    27296     combined loss: 2950.65969     paf loss 20.78469     hm loss 2929.87500\n",
      "Iteration:   1885    step:    27301     combined loss: 2942.78734     paf loss 20.39830     hm loss 2922.38904\n",
      "Iteration:   1890    step:    27306     combined loss: 2855.44883     paf loss 19.73472     hm loss 2835.71411\n",
      "Iteration:   1895    step:    27311     combined loss: 2169.56008     paf loss 18.09694     hm loss 2151.46313\n",
      "Iteration:   1900    step:    27316     combined loss: 2989.99283     paf loss 18.28287     hm loss 2971.70996\n",
      "Iteration:   1905    step:    27321     combined loss: 3803.33446     paf loss 17.71886     hm loss 3785.61560\n",
      "Iteration:   1910    step:    27326     combined loss: 3449.81604     paf loss 18.41199     hm loss 3431.40405\n",
      "Iteration:   1915    step:    27331     combined loss: 3365.88475     paf loss 15.97045     hm loss 3349.91431\n",
      "Iteration:   1920    step:    27336     combined loss: 2594.42123     paf loss 13.77694     hm loss 2580.64429\n",
      "Iteration:   1925    step:    27341     combined loss: 3178.02938     paf loss 26.28402     hm loss 3151.74536\n",
      "Iteration:   1930    step:    27346     combined loss: 3734.03363     paf loss 17.38019     hm loss 3716.65344\n",
      "Iteration:   1935    step:    27351     combined loss: 2421.33992     paf loss 18.60188     hm loss 2402.73804\n",
      "Iteration:   1940    step:    27356     combined loss: 1897.89224     paf loss 18.86795     hm loss 1879.02429\n",
      "Iteration:   1945    step:    27361     combined loss: 2289.12943     paf loss 15.52579     hm loss 2273.60364\n",
      "Iteration:   1950    step:    27366     combined loss: 4130.87348     paf loss 22.05097     hm loss 4108.82251\n",
      "Iteration:   1955    step:    27371     combined loss: 4123.18258     paf loss 26.14144     hm loss 4097.04114\n",
      "Iteration:   1960    step:    27376     combined loss: 3661.64722     paf loss 22.30994     hm loss 3639.33728\n",
      "Iteration:   1965    step:    27381     combined loss: 2293.35716     paf loss 15.74510     hm loss 2277.61206\n",
      "Iteration:   1970    step:    27386     combined loss: 4279.19085     paf loss 21.43182     hm loss 4257.75903\n",
      "Iteration:   1975    step:    27391     combined loss: 3428.78011     paf loss 18.61409     hm loss 3410.16602\n",
      "Iteration:   1980    step:    27396     combined loss: 2926.76113     paf loss 15.12197     hm loss 2911.63916\n",
      "Iteration:   1985    step:    27401     combined loss: 3771.76508     paf loss 18.47431     hm loss 3753.29077\n",
      "Iteration:   1990    step:    27406     combined loss: 2371.79736     paf loss 13.25744     hm loss 2358.53992\n",
      "Iteration:   1995    step:    27411     combined loss: 2981.64139     paf loss 21.62735     hm loss 2960.01404\n",
      "Iteration:   2000    step:    27416     combined loss: 4406.59063     paf loss 27.03350     hm loss 4379.55713\n",
      "Iteration:   2005    step:    27421     combined loss: 3736.23799     paf loss 15.89278     hm loss 3720.34521\n",
      "Iteration:   2010    step:    27426     combined loss: 4060.82508     paf loss 20.42762     hm loss 4040.39746\n",
      "Iteration:   2015    step:    27431     combined loss: 2625.38306     paf loss 16.23584     hm loss 2609.14722\n",
      "Iteration:   2020    step:    27436     combined loss: 4039.21811     paf loss 18.33237     hm loss 4020.88574\n",
      "Iteration:   2025    step:    27441     combined loss: 4021.33438     paf loss 18.41153     hm loss 4002.92285\n",
      "Iteration:   2030    step:    27446     combined loss: 5360.86017     paf loss 16.50251     hm loss 5344.35767\n",
      "Iteration:   2035    step:    27451     combined loss: 3578.42071     paf loss 20.21453     hm loss 3558.20618\n",
      "Iteration:   2040    step:    27456     combined loss: 2706.39190     paf loss 16.62432     hm loss 2689.76758\n",
      "Iteration:   2045    step:    27461     combined loss: 3866.92531     paf loss 26.16616     hm loss 3840.75916\n",
      "Iteration:   2050    step:    27466     combined loss: 3064.14197     paf loss 15.06226     hm loss 3049.07971\n",
      "Iteration:   2055    step:    27471     combined loss: 3215.57011     paf loss 16.11454     hm loss 3199.45557\n",
      "Iteration:   2060    step:    27476     combined loss: 3651.92205     paf loss 19.87114     hm loss 3632.05090\n",
      "Iteration:   2065    step:    27481     combined loss: 4352.60367     paf loss 19.70011     hm loss 4332.90356\n",
      "Iteration:   2070    step:    27486     combined loss: 2662.61687     paf loss 17.76079     hm loss 2644.85608\n",
      "Iteration:   2075    step:    27491     combined loss: 2812.88948     paf loss 17.71138     hm loss 2795.17810\n",
      "Iteration:   2080    step:    27496     combined loss: 3168.00938     paf loss 21.12644     hm loss 3146.88293\n",
      "Iteration:   2085    step:    27501     combined loss: 3098.44123     paf loss 20.03877     hm loss 3078.40247\n",
      "Iteration:   2090    step:    27506     combined loss: 3572.11357     paf loss 18.62846     hm loss 3553.48511\n",
      "Iteration:   2095    step:    27511     combined loss: 4426.08282     paf loss 21.46417     hm loss 4404.61865\n",
      "Iteration:   2100    step:    27516     combined loss: 3905.79647     paf loss 22.44259     hm loss 3883.35388\n",
      "Iteration:   2105    step:    27521     combined loss: 4295.30278     paf loss 21.68657     hm loss 4273.61621\n",
      "Iteration:   2110    step:    27526     combined loss: 3391.28818     paf loss 19.89865     hm loss 3371.38953\n",
      "Iteration:   2115    step:    27531     combined loss: 3932.76605     paf loss 18.82587     hm loss 3913.94019\n",
      "Iteration:   2120    step:    27536     combined loss: 3598.18428     paf loss 21.06343     hm loss 3577.12085\n",
      "Iteration:   2125    step:    27541     combined loss: 3700.88566     paf loss 18.89653     hm loss 3681.98914\n",
      "Iteration:   2130    step:    27546     combined loss: 2455.69419     paf loss 16.72165     hm loss 2438.97253\n",
      "Iteration:   2135    step:    27551     combined loss: 2504.13334     paf loss 18.01054     hm loss 2486.12280\n",
      "Iteration:   2140    step:    27556     combined loss: 3900.03924     paf loss 16.19976     hm loss 3883.83948\n",
      "Iteration:   2145    step:    27561     combined loss: 2903.09224     paf loss 18.56160     hm loss 2884.53064\n",
      "Iteration:   2150    step:    27566     combined loss: 3885.90620     paf loss 18.54487     hm loss 3867.36133\n",
      "Iteration:   2155    step:    27571     combined loss: 3529.01557     paf loss 19.55048     hm loss 3509.46509\n",
      "Iteration:   2160    step:    27576     combined loss: 2827.78580     paf loss 17.87442     hm loss 2809.91138\n",
      "Iteration:   2165    step:    27581     combined loss: 4673.84151     paf loss 27.20039     hm loss 4646.64111\n",
      "Iteration:   2170    step:    27586     combined loss: 2480.88814     paf loss 14.94429     hm loss 2465.94385\n",
      "Iteration:   2175    step:    27591     combined loss: 2830.52750     paf loss 21.22709     hm loss 2809.30042\n",
      "Iteration:   2180    step:    27596     combined loss: 2075.35267     paf loss 14.53126     hm loss 2060.82141\n",
      "Iteration:   2185    step:    27601     combined loss: 2874.17511     paf loss 16.34833     hm loss 2857.82678\n",
      "Iteration:   2190    step:    27606     combined loss: 3734.22842     paf loss 20.67654     hm loss 3713.55188\n",
      "Iteration:   2195    step:    27611     combined loss: 4446.10279     paf loss 24.93092     hm loss 4421.17188\n",
      "Iteration:   2200    step:    27616     combined loss: 3988.94770     paf loss 22.46589     hm loss 3966.48181\n",
      "Iteration:   2205    step:    27621     combined loss: 2769.92640     paf loss 17.76539     hm loss 2752.16101\n",
      "Iteration:   2210    step:    27626     combined loss: 4285.13960     paf loss 20.01875     hm loss 4265.12085\n",
      "Iteration:   2215    step:    27631     combined loss: 2888.85628     paf loss 20.94185     hm loss 2867.91443\n",
      "Iteration:   2220    step:    27636     combined loss: 2942.74102     paf loss 17.24163     hm loss 2925.49939\n",
      "Iteration:   2225    step:    27641     combined loss: 3662.17382     paf loss 26.41491     hm loss 3635.75891\n",
      "Iteration:   2230    step:    27646     combined loss: 3394.91275     paf loss 23.25992     hm loss 3371.65283\n",
      "Iteration:   2235    step:    27651     combined loss: 2733.36289     paf loss 15.88278     hm loss 2717.48010\n",
      "Iteration:   2240    step:    27656     combined loss: 3573.78934     paf loss 22.63908     hm loss 3551.15027\n",
      "Iteration:   2245    step:    27661     combined loss: 4423.78136     paf loss 27.54650     hm loss 4396.23486\n",
      "Iteration:   2250    step:    27666     combined loss: 5130.50080     paf loss 17.86653     hm loss 5112.63428\n",
      "Iteration:   2255    step:    27671     combined loss: 4361.99401     paf loss 13.96301     hm loss 4348.03101\n",
      "Iteration:   2260    step:    27676     combined loss: 2644.78900     paf loss 19.10773     hm loss 2625.68127\n",
      "Iteration:   2265    step:    27681     combined loss: 4694.64011     paf loss 25.91868     hm loss 4668.72144\n",
      "Iteration:   2270    step:    27686     combined loss: 4312.24614     paf loss 24.47270     hm loss 4287.77344\n",
      "Iteration:   2275    step:    27691     combined loss: 2492.96034     paf loss 24.55153     hm loss 2468.40881\n",
      "Iteration:   2280    step:    27696     combined loss: 3983.11536     paf loss 22.80396     hm loss 3960.31140\n",
      "Iteration:   2285    step:    27701     combined loss: 3896.28432     paf loss 22.54579     hm loss 3873.73853\n",
      "Iteration:   2290    step:    27706     combined loss: 3100.13855     paf loss 18.52246     hm loss 3081.61609\n",
      "Iteration:   2295    step:    27711     combined loss: 3951.17100     paf loss 15.91379     hm loss 3935.25720\n",
      "Iteration:   2300    step:    27716     combined loss: 3817.14328     paf loss 18.69723     hm loss 3798.44604\n",
      "Iteration:   2305    step:    27721     combined loss: 2972.46838     paf loss 17.45386     hm loss 2955.01453\n",
      "Iteration:   2310    step:    27726     combined loss: 2834.21138     paf loss 19.97579     hm loss 2814.23560\n",
      "Iteration:   2315    step:    27731     combined loss: 3310.93914     paf loss 15.33978     hm loss 3295.59937\n",
      "Iteration:   2320    step:    27736     combined loss: 3776.02001     paf loss 23.87072     hm loss 3752.14929\n",
      "Iteration:   2325    step:    27741     combined loss: 3078.44199     paf loss 19.83676     hm loss 3058.60522\n",
      "Iteration:   2330    step:    27746     combined loss: 3615.15569     paf loss 25.07342     hm loss 3590.08228\n",
      "Iteration:   2335    step:    27751     combined loss: 2857.00323     paf loss 13.89667     hm loss 2843.10657\n",
      "Iteration:   2340    step:    27756     combined loss: 4046.47110     paf loss 15.79398     hm loss 4030.67712\n",
      "Iteration:   2345    step:    27761     combined loss: 3253.55732     paf loss 18.71210     hm loss 3234.84521\n",
      "Iteration:   2350    step:    27766     combined loss: 4742.06647     paf loss 21.46808     hm loss 4720.59839\n",
      "Iteration:   2355    step:    27771     combined loss: 3943.33368     paf loss 21.96844     hm loss 3921.36523\n",
      "Iteration:   2360    step:    27776     combined loss: 2837.45983     paf loss 23.43614     hm loss 2814.02368\n",
      "Iteration:   2365    step:    27781     combined loss: 3191.29967     paf loss 17.39318     hm loss 3173.90649\n",
      "Iteration:   2370    step:    27786     combined loss: 2913.06576     paf loss 18.39914     hm loss 2894.66663\n",
      "Iteration:   2375    step:    27791     combined loss: 3987.11732     paf loss 18.36195     hm loss 3968.75537\n",
      "Iteration:   2380    step:    27796     combined loss: 4032.42382     paf loss 25.43883     hm loss 4006.98499\n",
      "Iteration:   2385    step:    27801     combined loss: 3738.75570     paf loss 22.58566     hm loss 3716.17004\n",
      "Iteration:   2390    step:    27806     combined loss: 2284.57595     paf loss 18.49184     hm loss 2266.08411\n",
      "Iteration:   2395    step:    27811     combined loss: 2407.74200     paf loss 19.18047     hm loss 2388.56152\n",
      "Iteration:   2400    step:    27816     combined loss: 2927.30627     paf loss 12.59631     hm loss 2914.70996\n",
      "Iteration:   2405    step:    27821     combined loss: 5132.39457     paf loss 15.87357     hm loss 5116.52100\n",
      "Iteration:   2410    step:    27826     combined loss: 2637.86871     paf loss 16.77875     hm loss 2621.08997\n",
      "Iteration:   2415    step:    27831     combined loss: 4252.67940     paf loss 20.60201     hm loss 4232.07739\n",
      "Iteration:   2420    step:    27836     combined loss: 2497.41885     paf loss 16.90969     hm loss 2480.50916\n",
      "Iteration:   2425    step:    27841     combined loss: 5029.91080     paf loss 25.22037     hm loss 5004.69043\n",
      "Iteration:   2430    step:    27846     combined loss: 2494.39927     paf loss 17.85154     hm loss 2476.54773\n",
      "Iteration:   2435    step:    27851     combined loss: 4290.42340     paf loss 27.30474     hm loss 4263.11865\n",
      "Iteration:   2440    step:    27856     combined loss: 3668.31948     paf loss 20.71791     hm loss 3647.60156\n",
      "Iteration:   2445    step:    27861     combined loss: 2207.21863     paf loss 12.06872     hm loss 2195.14990\n",
      "Iteration:   2450    step:    27866     combined loss: 3266.82000     paf loss 20.77093     hm loss 3246.04907\n",
      "Iteration:   2455    step:    27871     combined loss: 3698.37220     paf loss 22.06532     hm loss 3676.30688\n",
      "Iteration:   2460    step:    27876     combined loss: 2327.52992     paf loss 14.93763     hm loss 2312.59229\n",
      "Iteration:   2465    step:    27881     combined loss: 2994.98048     paf loss 22.01063     hm loss 2972.96985\n",
      "Iteration:   2470    step:    27886     combined loss: 3308.55659     paf loss 15.25788     hm loss 3293.29871\n",
      "Iteration:   2475    step:    27891     combined loss: 2786.37590     paf loss 20.45684     hm loss 2765.91907\n",
      "Iteration:   2480    step:    27896     combined loss: 2770.78771     paf loss 18.41186     hm loss 2752.37585\n",
      "Iteration:   2485    step:    27901     combined loss: 5024.12253     paf loss 16.47532     hm loss 5007.64722\n",
      "Iteration:   2490    step:    27906     combined loss: 3486.52696     paf loss 21.05650     hm loss 3465.47046\n",
      "Iteration:   2495    step:    27911     combined loss: 2946.38366     paf loss 19.01476     hm loss 2927.36890\n",
      "Iteration:   2500    step:    27916     combined loss: 3018.72113     paf loss 18.11225     hm loss 3000.60889\n",
      "Iteration:   2505    step:    27921     combined loss: 3050.55857     paf loss 23.99424     hm loss 3026.56433\n",
      "Iteration:   2510    step:    27926     combined loss: 4317.25639     paf loss 23.08403     hm loss 4294.17236\n",
      "Iteration:   2515    step:    27931     combined loss: 3927.79553     paf loss 24.99133     hm loss 3902.80420\n",
      "Iteration:   2520    step:    27936     combined loss: 3462.87358     paf loss 22.65934     hm loss 3440.21423\n",
      "Iteration:   2525    step:    27941     combined loss: 3583.01782     paf loss 23.45068     hm loss 3559.56714\n",
      "Iteration:   2530    step:    27946     combined loss: 3014.71629     paf loss 21.57957     hm loss 2993.13672\n",
      "Iteration:   2535    step:    27951     combined loss: 2738.30864     paf loss 14.32609     hm loss 2723.98254\n",
      "Iteration:   2540    step:    27956     combined loss: 3209.01275     paf loss 17.70586     hm loss 3191.30688\n",
      "Iteration:   2545    step:    27961     combined loss: 2587.36161     paf loss 19.72197     hm loss 2567.63965\n",
      "Iteration:   2550    step:    27966     combined loss: 4616.44902     paf loss 25.37724     hm loss 4591.07178\n",
      "Iteration:   2555    step:    27971     combined loss: 2827.40036     paf loss 18.13926     hm loss 2809.26111\n",
      "Iteration:   2560    step:    27976     combined loss: 2732.21162     paf loss 15.88130     hm loss 2716.33032\n",
      "Iteration:   2565    step:    27981     combined loss: 3822.78964     paf loss 16.77951     hm loss 3806.01013\n",
      "Iteration:   2570    step:    27986     combined loss: 3156.17531     paf loss 23.59547     hm loss 3132.57983\n",
      "Iteration:   2575    step:    27991     combined loss: 2737.76930     paf loss 20.02589     hm loss 2717.74341\n",
      "Iteration:   2580    step:    27996     combined loss: 2940.24361     paf loss 19.87105     hm loss 2920.37256\n",
      "Iteration:   2585    step:    28001     combined loss: 3003.51976     paf loss 17.75426     hm loss 2985.76550\n",
      "Iteration:   2590    step:    28006     combined loss: 4692.78970     paf loss 28.19961     hm loss 4664.59009\n",
      "Iteration:   2595    step:    28011     combined loss: 3257.44710     paf loss 15.25886     hm loss 3242.18823\n",
      "Iteration:   2600    step:    28016     combined loss: 3849.38886     paf loss 18.13141     hm loss 3831.25745\n",
      "Iteration:   2605    step:    28021     combined loss: 3695.19615     paf loss 24.64622     hm loss 3670.54993\n",
      "Iteration:   2610    step:    28026     combined loss: 4679.06973     paf loss 28.76235     hm loss 4650.30737\n",
      "Iteration:   2615    step:    28031     combined loss: 4688.34752     paf loss 19.85436     hm loss 4668.49316\n",
      "Iteration:   2620    step:    28036     combined loss: 4659.34605     paf loss 25.01427     hm loss 4634.33179\n",
      "Iteration:   2625    step:    28041     combined loss: 2480.69409     paf loss 24.14965     hm loss 2456.54443\n",
      "Iteration:   2630    step:    28046     combined loss: 2355.51047     paf loss 16.89951     hm loss 2338.61096\n",
      "Iteration:   2635    step:    28051     combined loss: 3423.62435     paf loss 20.29354     hm loss 3403.33081\n",
      "Iteration:   2640    step:    28056     combined loss: 2938.56717     paf loss 18.26944     hm loss 2920.29773\n",
      "Iteration:   2645    step:    28061     combined loss: 5336.82373     paf loss 24.94287     hm loss 5311.88086\n",
      "Iteration:   2650    step:    28066     combined loss: 3150.41480     paf loss 21.86781     hm loss 3128.54700\n",
      "Iteration:   2655    step:    28071     combined loss: 3904.69815     paf loss 23.10172     hm loss 3881.59644\n",
      "Iteration:   2660    step:    28076     combined loss: 3749.24915     paf loss 22.41077     hm loss 3726.83838\n",
      "Iteration:   2665    step:    28081     combined loss: 3962.19916     paf loss 25.84845     hm loss 3936.35071\n",
      "Iteration:   2670    step:    28086     combined loss: 3829.45690     paf loss 17.27539     hm loss 3812.18152\n",
      "Iteration:   2675    step:    28091     combined loss: 2922.94619     paf loss 22.05923     hm loss 2900.88696\n",
      "Iteration:   2680    step:    28096     combined loss: 4210.01786     paf loss 19.26005     hm loss 4190.75781\n",
      "Iteration:   2685    step:    28101     combined loss: 3648.43813     paf loss 23.22536     hm loss 3625.21277\n",
      "Iteration:   2690    step:    28106     combined loss: 3382.85826     paf loss 18.34679     hm loss 3364.51147\n",
      "Iteration:   2695    step:    28111     combined loss: 3756.09513     paf loss 14.44156     hm loss 3741.65356\n",
      "Iteration:   2700    step:    28116     combined loss: 2042.24250     paf loss 21.72663     hm loss 2020.51587\n",
      "Iteration:   2705    step:    28121     combined loss: 4325.73322     paf loss 19.50788     hm loss 4306.22534\n",
      "Iteration:   2710    step:    28126     combined loss: 3792.70046     paf loss 21.90871     hm loss 3770.79175\n",
      "Iteration:   2715    step:    28131     combined loss: 4969.82194     paf loss 21.30436     hm loss 4948.51758\n",
      "Iteration:   2720    step:    28136     combined loss: 2751.97626     paf loss 18.71088     hm loss 2733.26538\n",
      "Iteration:   2725    step:    28141     combined loss: 3011.69535     paf loss 22.12125     hm loss 2989.57410\n",
      "Iteration:   2730    step:    28146     combined loss: 4239.18027     paf loss 15.59238     hm loss 4223.58789\n",
      "Iteration:   2735    step:    28151     combined loss: 3548.02190     paf loss 20.81889     hm loss 3527.20300\n",
      "Iteration:   2740    step:    28156     combined loss: 3597.00884     paf loss 20.93816     hm loss 3576.07068\n",
      "Iteration:   2745    step:    28161     combined loss: 3980.97851     paf loss 23.42260     hm loss 3957.55591\n",
      "Iteration:   2750    step:    28166     combined loss: 2778.55387     paf loss 19.93681     hm loss 2758.61707\n",
      "Iteration:   2755    step:    28171     combined loss: 3788.88666     paf loss 19.83137     hm loss 3769.05530\n",
      "Iteration:   2760    step:    28176     combined loss: 4300.49224     paf loss 19.65460     hm loss 4280.83765\n",
      "Iteration:   2765    step:    28181     combined loss: 3721.44531     paf loss 22.99743     hm loss 3698.44788\n",
      "Iteration:   2770    step:    28186     combined loss: 2980.57145     paf loss 21.77299     hm loss 2958.79846\n",
      "Iteration:   2775    step:    28191     combined loss: 4416.33262     paf loss 21.42881     hm loss 4394.90381\n",
      "Iteration:   2780    step:    28196     combined loss: 3646.27106     paf loss 25.88581     hm loss 3620.38525\n",
      "Iteration:   2785    step:    28201     combined loss: 2755.67219     paf loss 16.96333     hm loss 2738.70886\n",
      "Iteration:   2790    step:    28206     combined loss: 3199.55415     paf loss 21.80341     hm loss 3177.75073\n",
      "Iteration:   2795    step:    28211     combined loss: 2797.15873     paf loss 15.43339     hm loss 2781.72534\n",
      "Iteration:   2800    step:    28216     combined loss: 1927.01334     paf loss 17.65849     hm loss 1909.35486\n",
      "Iteration:   2805    step:    28221     combined loss: 4643.91317     paf loss 18.11776     hm loss 4625.79541\n",
      "Iteration:   2810    step:    28226     combined loss: 2784.83752     paf loss 15.78296     hm loss 2769.05457\n",
      "Iteration:   2815    step:    28231     combined loss: 3737.90794     paf loss 19.31310     hm loss 3718.59485\n",
      "Iteration:   2820    step:    28236     combined loss: 3343.70290     paf loss 18.00844     hm loss 3325.69446\n",
      "Train Loss: 3519.5993    PAF Loss:  19.9915    HM Loss:  3499.6078    Acc: NA\n",
      "Val Loss: 3834.2919    PAF Loss:  16.4992    HM Loss:  3817.7928     Acc: NA\n",
      "Training complete in 761m 26s\n",
      "Min train loss: 3834.291937\n"
     ]
    }
   ],
   "source": [
    "train_pkmodel(model, optimizer, train_loader, valid_loader, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.array(epoch_stats['train'])[:,0],'-', label='train Loss')\n",
    "plt.plot(np.array(epoch_stats['train'])[:,1],'-', label='train PAF Loss')\n",
    "plt.plot(np.array(epoch_stats['train'])[:,2],'-', label='train HM Loss')\n",
    "plt.plot(np.array(epoch_stats['valid'])[:,0],'-', label='valid Loss')\n",
    "plt.plot(np.array(epoch_stats['valid'])[:,1],'-', label='valid PAF Loss')\n",
    "plt.plot(np.array(epoch_stats['valid'])[:,2],'-', label='valid HM Loss')\n",
    "\n",
    "plt.title('epoch loss')\n",
    "plt.legend();\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(np.array(iter_stats['train'])[:,0],'r-', label='total loss')\n",
    "plt.plot(np.array(iter_stats['train'])[:,1],'b-', label='PAF loss')\n",
    "plt.plot(np.array(iter_stats['train'])[:,2],'g-', label='HM loss')\n",
    "\n",
    "plt.title('iter loss');\n",
    "plt.legend();\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "json.dump(epoch_stats, open(f'{TRIAL}_{EPOCH}epochs_e.json','w'))\n",
    "json.dump(iter_stats, open(f'{TRIAL}_{EPOCH}epochs_i.json','w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-07T17:14:23.084428",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
